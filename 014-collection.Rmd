# Data collection {#collection}

::: {.box .learning_goals}
* Review best practices for online and in person data collection
* Implement data integrity checks, manipulation checks, and pilot testing
:::

You have selected your measure and manipulation and planned your sample. Your preregistration is set. You have gone through your recruitment and ethics. Now it's time to think about the nuts and bolts of collecting data. 

While the details of data collection may vary from context to context and sample to sample, this chapter will highlight some general best practices for the data collection process. We organize these practices around two goals. The first section is participant-centric: we review some concerns regarding how to provide a positive experience for participants in both in-person and online experiments. Then in the second section, we discuss the data collection process from the perspective of the experimenter, covering some best practices 


::: {.box .case_study}
(TITLE) The rise of online data collection

Since the rise of experimental psychology laboratories in university settings during the period after World War 2 [@benjamin-jr2000], experiments have typically been conducted by recruiting participants from what has been referred to as the "subject pool." This term denotes a group of people who can be recruited for experiments, typically students from introductory psychology courses [@sieber1989] recruited via the requirement that students complete a certain quantity of experiments as part of their course work.^[At various times, students have raised ethical concerns about these requirements as being coercive of participationin precisely the way that should be off limits for psychology experiments (see Chapter \@ref(ethics)). As a result, most programs now provide some more or less onerous alternative to participation.] The ready availability of this convenience population led inevitably to the massive over-representation of US undergraduates in published psychology research, leading to persistent critiques of this practice for the generalizability of research [@sears1986;@henrich2012].

Yet in the period 2005--2015, there has been a revolution in data collection from convenience populations. Instead of focusing on university undergraduates, increasingly, published psychology work uses convenience samples of online workers recruited from crowdsourcing sites like Amazon Mechanical Turk (AMT). Originally designed to distribute micropayments to workers for business purposes like retyping reciepts, these services have become marketplaces to connect researchers with research participants who are willing to complete surveys and experimental tasks for small payments. As of 2015, more than a third of studies in top social and personality psychology journals were conducted on crowdsourcing platforms (another third were still conducted with college undergraduates) [@anderson2019] and this proportion is likely continuing to grow.

Online data collection 

[@mason2016;@buhrmester2016]


Crump et al. (2013) show through a set of beautiful experiments designed in the web browser how online data collection can replicate effects initially found in the lab.
:::

## The participant's perspective

### Data collection in person




### Data collection online

Online data collection is increasingly ubiquitous in the behavioral sciences. Further, the web browser – alongside survey software like Qualtrics – can be a major aid to transparency in sharing experimental materials.

- Validating the process of collecting data online. We briefly review studies suggesting that for general data collection across many paradigms, online data collection is valid. 
- When is online not enough? We describe cases where in-person data collection is necessary, highlighting psychophysical and physiological measurement and social interaction as two common classes of experiments that still cannot be done effectively online. 



Class MTurk Guidelines
Intro
In this class we will be running our replication projects on Amazon's Mechanical Turk (AMT/mTurk). mTurk is a platform on which Workers (or "Turkers") can complete Human Intelligence Tasks (HITs) for monetary compensation. HITs are put up by Requesters. mTurk originally was set up to do large-scale tasks that require human intelligence (e.g. labeling photos, finding telephone numbers, etc), but has recently been used by social scientists to conduct (large-scale) online experiments.

In this class, all replication projects will be launched using a common class account. This saves you the hassle of applying for a personal Requester account (which lately has become not-as-straightforward...), and also simplifies funding logistics. We will be giving more detailed instructions in class on how to access the class account and launch HITs etc.

General notes to keep in mind:

Many Turkers multi-task and do tens of HITs for many hours a day, so design your study with this in mind. Include appropriate attention checks, manipulation checks, and exclusion criteria (depending on the original study design as well).
Amazon worker IDs are actually tied to their (public) Amazon account and thus constitute identifiable information. Keep Turk IDs and all other identifiable information private. Anonymize all data before pushing to your git project repo, especially if your repo is public. There are useful tools for quickly automatically anonymizing IDs so reach out to the TAs if you want help with this!
Do all analyses on anonymized data. (This is to prevent cases where others are unable to reproduce your analyses because it might rely somehow on identifiable information. If you start with anonymized data, your analyses would never use any of this information.)
Resources
Beginner:
A gentle guide to MTurk (Links to an external site.)
Guide to Mturk basics (Links to an external site.)
MTurkers are people (Links to an external site.) (& the problem with common paradigms on MTurk)
Survey within MTurk platform (Links to an external site.)
Qualtrics Link (Links to an external site.)
Intermediate:
Turker Nation (Links to an external site.): Discussion board for Turkers
Creating a launch page to external site (Links to an external site.): advantage is that your study is embedded within MTurk, then opens a full page window for your task when required. Note launcher.html and task.html are separate.
Getting around the ridiculous extra 20% fee for 9+ participants (Links to an external site.)
MTurk fee structure  (Links to an external site.) (Links to an external site.)
IRB
We have approval under protocol #IRB-23274Links to an external site.: “Reproducibility of psychological science and instruction.”

Before running your study, you will need to complete a short CITI human subjects training if you have not already. (See assignment).  

Please include the following text on the first page / consent form of your study:

By answering the following questions, you are participating in a study being performed by cognitive scientists in the Stanford Department of Psychology. If you have questions about this research, please contact us at stanfordpsych251@gmail.com. You must be at least 18 years old to participate. Your participation in this research is voluntary. You may decline to answer any or all of the following questions. You may decline further participation, at any time, without adverse consequences. Your anonymity is assured; the researchers who have requested your participation will not receive any personal information about you. We have recently been made aware that your public Amazon.com profile can be accessed via your worker ID if you do not choose to opt out. If you would like to opt out of this feature, you may follow instructions available here (Links to an external site.).

Please include a short debriefing in your experiment, thanking the participant, explaining in 2-4 lines what your study was about, and asking them not to share this information with other potential participants.

Additional notes on MTurk
Communicating with participants

When you are running either Pilot B or your actual study, please keep this gmail window open and monitor traffic on it.
If you get complaints about your study, please address them courteously and quickly (ideally, within a few hours). Turkers can be very helpful if you are responsive. Always assure them that they will be paid for their work.
Payment policies

When in doubt as to technical issues, pay the Turker. The TAs can help you bonus those who had technical issues with the experiment or completion code!
If a Turker seems especially difficult, then please bring their complaint to the attention of the course team. Turkers can potentially complain to IRB so err on the side of doing this more frequently if you have issues.

::: {.box .ethical_considerations}
(TITLE) Best practices for online research

The rise of Amazon Mechanical Turk was a

* Fair payment. 
* Good user experience for participants
* Clear communication 
:::

## Ensuring high quality data

In the second section of this chapter, we review a few key practices for 


::: {.box .accident_report}
(TITLE) Does data quality vary throughout the semester? 

Every lab that collects empirical data repeatedly using the same population builds up lore about how that population varies. One infant development lab famously repainted their walls a particularly bright shade of blue and claimed that their studies did not yield significant findings (even replicating highly robust paradigms) until they went back to a more neutral color. ...

The ManyLabs studies were a series of large-scale, collaborative studies that involved the same experimental protocol being run at a variety of different sites. 
:::


### Run pilot studies

A **pilot study** is a small study conducted before you collect your main sample. Smooth and successful data collection is typically difficult without piloting, at least the first time you do an experiment of a given type. Fundamentally, experiments induce a particular experience in their participants, and careful attention to the nature of that experience^[Even if the experience is somewhat tedious, like searching for a T amongst Ls for hundreds of trials!] requires iterative development. 

Pilot studies cannot tell you about expected effect size (as we discussed in Chapter \@ref(sampling)). They also cannot tell you about the significance of your main result. What they *can* do is tell you about whether your paradigm works. They can reveal:
* if your code crashes under certain circumstances
* if your instructions confuse a substantial portion of your participants
* if you have a very high dropout rate
* if your data collection procedure fails to log variables of interest
* if participants are disgruntled by the end of the experiment

We recommend that all experimenters do -- at the very minimum -- two pilot studies before they launch their experiment. 

The first pilot study, **pilot A**^[Good name, right?], is a test with non-naive participants. Your parents can do this experiment, or in a pinch you can run yourself a bunch of times (though this isn't preferable because you're likely to miss a lot of aspects of the experience that you are habituated to, especially if you've been debugging the software). The goal of pilot A is to ensure that your experiment is comprehensible, that participants can complete it, and that the data are logged appropriately. This last goal means that you must *analyze* the data from pilot A, at least to the point of checking that the relevant data about each trial is logged.^[At a minimum, for each trial you need to know a subject ID, a trial ID, the state of any manipulation (condition, trial type, etc.), and the value for the measure.] 

The second pilot study, **pilot B**, consists of a test of a small set of naive participants. Pilot size will depend on the costliness of running the experiment (in time, money, and opportunity cost) as well as your worries about the paradigm. If we're talking about a short online survey experiment, then running a pilot of 10--20 people is reasonable. A more extensive laboratory study might be better served by piloting just two or three people. The goal of this second study is to understand properties of the participant experience: for example, were they confused? Did they withdraw before the study finished? You won't have the numbers to make robust statistical inferences about these questions, but even a small number of pilots can tell you that your dropout rate is likely too high: if 5 of 10 pilot participants withdraw you may need to reconsider aspects of your design. It's critical for pilot B  that you debrief more extensively with your participants. This debriefing often takes the form of an interview questionnaire after the study is over ("what did you think the study was about?" and "is there any way we could improve the experience of being in the study?" can be helpful questions). 

Piloting is often an iterative process. We frequently launch studies for a pilot B, then recognise from the data or from participant feedback that they can be improved. We make tweaks and pilot again. Be careful not to overfit to small differences in pilot data -- the samples are small and so inferences will not be robust. The process should be more like workshopping a manuscript to remove typos and make it read better than doing a study.

In the case of especially expensive experiments, it can be a dilemma whether to run a larger pilot to identify difficulties since such a pilot will be costly. In these cases, one possibility can be to preregister a contingent strategy. For example, in a planned sample of 100 participants, you could preregister running 20 as a pilot sample with the stipulation that you will look only at their dropout rate and not at any condition differences in the target measure. Then the registration could state that if the dropout rate is lower than 25%, you will collect the next 80 participants and analyze the whole dataset including the initial pilot. This sort of registration can help you split the difference between cautious piloting and conservation of rare or costly data. 

### Keep consistent data collection records

Important to put checks in place on your data collection pipeline early.



### Measure participant compliance

Data collection in the field: An opinionated discussion of common pitfalls of field experiments in psychology.

- Blinding and randomization. Fieldwork makes it harder to maintain these critical principles of experimental design, potentially leading to bias. 
- Reasoning about and combatting selection bias.

https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00998/full

## Chapter summary: Data collection

In this brief chapter, we reviewed the process of data collection from two perspectives. From the particpant's perspective, we emphasized fair payment, a good "user experience", nad clear communication as the three key factors ensuring that they 

