# Meta-analysis {#meta}


<!-- NC: Need to upldate .bib at end -->

```{r meta-initialization}
#--------------------------------------------------------------------------------
# Paluck et al. meta-analysis
# Note: I cannot perfectly reproduce their results
#--------------------------------------------------------------------------------
library("tidyverse")
library("metafor")
library("MetaUtility")
library("PublicationBias")

# rounding digits
digits = 2

# helper fn taken from https://osf.io/ue36m/
sim_data2 = function(p,
                     keep.all.studies = FALSE) {

  N = p$k * p$per.cluster

  # generate cluster random intercepts
  # called "zeta" in paper
  # these are normal even when true effect dist is exponential
  gam1 = rnorm( n = p$k, mean = 0, sd = sqrt( p$V.gam ) )
  gam1i = rep( gam1, each = p$per.cluster )

  # generate individual-study random intrcepts
  # these are called "gamma" in the paper
  # these are either normal or exponential
  if ( p$true.dist == "norm" ) gam2i = rnorm( n = N, mean = 0, sd = sqrt( p$V - p$V.gam ) )

    if ( p$true.dist == "exp" ) {
      true.effect.var = p$V - p$V.gam
      # set var using properties of exponential
      gam2i = rexp( n = N, rate = true.effect.var^(-1/2) )
      # shift to have mean of 0
      # use fact that var = mean^2 in exponential
      gam2i = gam2i - true.effect.var^(1/2)
    }


  # individual study SEs
  sei = runif( n = N, min = p$sei.min, max = p$sei.max )

  # individual study means
  if ( p$SE.corr == TRUE ) {
    beta = 6
    mui = p$mu + beta * sei + gam1i + gam2i

    # recenter them to have desired mean of p$mu
    # because E[sei}]
    mui = mui - beta * mean(sei)

    cor(sei, mui)
  } else {
    mui = p$mu + gam1i + gam2i
  }

  # individual study point estimates
  yi = rnorm( n = N, mean = mui, sd = sei )

  # NEW: make cluster variable
  # this way, if we generate (say) 5 studies per "cluster", but V.gam = 0 in order
  #  to effectively generate without clustering, the cluster variable we ultimately
  #  pass to robumeta correctly reflects the lack of clustering
  if ( p$V.gam == 0 ) cluster = 1:N else cluster = rep(1:p$k, each = p$per.cluster)
  
  # suppress warnings from cor if SD of mui is 0 (fixed-effects model)
  d = suppressWarnings( data.frame( cluster = cluster,
                                    Study.name = 1:N,
                                    mui,
                                    yi = yi,
                                    sei = sei,
                                    vi = sei^2,
                                    pval = 2 * ( 1 - pnorm( abs(yi) / sei ) ),
                                    SE.corr.emp = cor(sei, mui) ) )  # empirical correlation
  
  # 1-tailed publication bias
  signif = d$pval < 0.05 & d$yi > 0
  publish = rep( 1, nrow(d) )
  publish[ signif == FALSE ] = rbinom( n = sum(signif == FALSE), size = 1, prob = 1/p$eta )

  d$weight = 1
  d$weight[ signif == 0 ] = p$eta

  # optionally, also select for small SE
  # Fi as defined in Supplement
  # tertiles of empirical SEs prior to selection
  Fi = rep( 1, nrow(d) )  # so that it has no effect if we're not selecting based on SE
  if ( p$select.SE == TRUE ) {
    terts = p$sei.min + (p$sei.max - p$sei.min) * c(1/3, 2/3)
    # prob of inclusion based on SE
    P.Fi = rep(NA, nrow(d))
    P.Fi[ d$sei < terts[1] ] = 1
    #Fi[ d$sei >= terts[1] ] = 0
    P.Fi[ d$sei >= terts[1] & d$sei < terts[2] ] = .75
    P.Fi[ d$sei >= terts[2] ] = .5
    Fi = rbinom( n = nrow(d),
                 size = 1,
                 prob = P.Fi )
  }

  if ( keep.all.studies == FALSE ) d = d[ publish == 1 & Fi == 1, ]
  if ( keep.all.studies == TRUE ){
    # just add the indicators
    d$publish = publish
    d$Fi = Fi
  }

  # record the mean empirical SE (after publication bias) as sanity check on SE selection
  # but avoid errors if d now has 0 studies
  if ( nrow(d) > 0 ) {
    d$SE.mean.emp = mean(d$sei)
    d$P.select.SE.emp = mean(Fi)
    d$P.publish.emp = mean(publish)
    return(d)
  } else {
    # still return the empty dataset for use in doParallel
    return(d)
  }
}
```

```{r meta-paluck}  
DF <- read.csv("data/Contact_quant_outcomes.csv")


# remove supplemental studies (see Paluck et al. Supplement Section 1)
DF <- DF[1:27, ]

# calculate total n and determine whether d estimate was significant
DF <- DF %>% 
  
  rowwise() %>% 
  
  # calculate total n
  mutate(n_total = n_t + n_c) %>% 
  
  # determine whether d estimate was significant
    ## calculate lower and upper bounds
    mutate(lb = d - (1.96 * sqrt(var_d)),
           ub = d + (1.96 * sqrt(var_d))) %>% 
    
    ## determine significance
    mutate(sig = if_else(
      condition = lb > 0,
      true = 1,
      false = 0)) %>% 
  
  ungroup() %>% 
  
  # MM likes it ordered by variance for forest plot prettiness
  arrange(var_d)

# fixed effect meta
fe.m <- rma(yi = d,
            vi = var_d,
            data = DF,
            method = "FE")

# random effect meta
re.m <- rma(yi = d,
            vi = var_d,
            data = DF,
            method = "REML",
            knha = TRUE,
            # for forest plot joy:
            slab = DF$name_short)

# package Paluck used
#re.m <- metaplus(yi = d, sei = se_d, random = "normal", 
#                 data = DF)

# estimated heterogeneity with inference
t2.CI = tau_CI(meta = re.m)
tauStats = round( c( sqrt(re.m$tau2), t2.CI[1], t2.CI[2] ), digits )

```

::: learning-goals
🍎 Learning goals:

- Understand the scientific utility of synthesizing evidence across studies
- Understand pitfalls of intuitive approaches to evidence synthesis and how formal approaches to meta-analysis circumvent these pitfalls
- Conduct a simple fixed- or random-effects meta analysis
- Realize that within- and across-study biases can propagate to meta-analysis estimates
:::

::: case-study
🔬 Case study: Hotel towel reuse (Scheibehenne, Jamil, and Wagenmakers 2016). A simple example of aggregating noisy evidence from replications.

In a widely-cited study on the power of describing social norms, Goldstein, Cialdini, and Griskevicius (2008) examined the effect of a *social norm messaging* on hotel towel reuse. Across two studies, they found that guests who received a social norm message indicating that most other guests reused their towels were more likely to reuse their towels than guests in a control condition (Study 1 $p = .05$, Study 2 $p = .03$). In subsequent researcher, however, five studies consistently failed to indicate that the social norm message increased hotel towel reuse (all $p > .05$).

At first glance, you might conclude that social norm messages do not reliably impact towel reuse. You might even go a step further and try to think of explanations for why one team found the effect, but the other did not. Maybe the messages change behavior when guest receive nice fluffy towels, but doesn't work when guests receive cheap, low-quality towels. However, this is a case where binary thinking can mislead us. When @scheibehenne2016 combined the evidence via a *meta-analysis*, they found that the results across these studies were, for the most part, quite consistent. Participants who received a social norm message were slightly more likely to reuse their towels in four out of five of the studies--but the difference was not large enough in most studies to yield a significant *p*-value. When combined, however, meta-analysis indicated that social norm messages did *on average* increase hotel towel reuse.
:::

There are many ways that researchers perform reviews of the scientific literature (see Grant and Booth, 2009). When students hear the term "review", they often have flashbacks to times where they threw some search term into Google Scholar, downloaded a bunch of articles that looked interesting, spent a few days reading those articles, and then wrote a summary of what has been learned from these studies. There is not necessarily anything wrong with this type of review--but meta-analysis goes one step further by using statistical techniques to combine results across multiple studies.

One of the central parts of the meta-analysis is extracting *effect sizes* from studies (remember effect sizes from Chapter \@ref(replication)?) and then combining these effect sizes in a single set of analyses. By combining information from multiple studies, the meta-analyst can make a more precise estimate of the size of an effect. The researcher can also look at the extent to which this effect varies across studies. They may even them perform analyses indicating whether studies with certain characteristics systematically produce different results.

Think about how meta-analysis might give you a different impression of an area of study than a literature review. If you would have performed a literature review on studies examining the effect of social norm messaging on hotel towel re-use, you would have likely come away with the impression that there is an unreliable effect at best and no effect at worst. However, when statistically combining the results of these studies, a meta-analyst is able to see that the findings are more consistent than they appear at first glance--and that, when considering all the available evidence, it actually appears that social norm messaging does decrease hotel towel reuse. Considering that, for example, there were 1.7 billion overnight hotel bookings in the European Union alone in 2013 (Eurostat, 2015), the difference in the types of the conclusions formed by a literature review and a meta-analysis matter quite a bit!

The towel example is a good starting point but there is much more we can do with meta-analysis. So we'll now turn to a different running example. For more than a century, social scientists have sought to understand and mitigate prejudice. One influential idea, the *contact hypothesis*, posits that prejudice can be reduced when majority and minority groups come together in the pursuit of common goals (Allport, 1954, p. 281). In a recent meta-analysis, @paluck2019contact examined whether randomly-assigned contact interventions had long-term effects on prejudice-related outcomes. We use their meta-analysis throughout this chapter to highlight how the meta-analytic tools they have used provide insights about this important question.

## The basics of evidence synthesis

### How *not* to synthesize evidence

When considering evidence across multiple studies, most of us intuitively count how many studies supported versus did not support the hypothesis under investigation. This usually amounts to counting the number of studies with "significant" $p$-values, since (for better or for worse) "significance" is largely what drives the take-home conclusions researchers report [@mcshane2017statistical; @nelson1986interpretation]. This is often called *vote-counting* [@borenstein2021introduction]. For example, in the @paluck2019contact meta-analysis, all studies yielded positive effect sizes, but only approximately `r sum(DF$sig)` of `r nrow(DF)` were significant. So, based on this vote-count, we would have the impression that most studies do not support the contact hypothesis.[^1]

[^1]: Many narrative reviews take essentially this vote-counting approach, albeit often not explicitly. If you see this kind of practice, your first instinct should be to say "why is there no quantitative meta-analysis?"

Despite its intuitive appeal, vote-counting can be very misleading because this approach characterizes evidence solely in terms of dichotomized $p$-values, while entirely ignoring effect sizes. In Chapter \@ref(replication), we saw how this fetishism of statistical significance can mislead us when we consider individual studies. These problems propagate to evidence syntheses if we simply vote-count statistical significance across studies as well. For example, it could be the case that relatively few studies are significant (for example, in a literature of typically small studies), yet if the studies typically have large point estimates, then the consensus of evidence could actually be quite strong. Or inversely, it could be that nearly all studies are significant, but if they typically have small point estimates, then the consensus of evidence might still be rather weak. In these cases, vote-counting could lead us badly astray [@borenstein2021introduction].

To avoid these pitfalls, a principled evidence synthesis needs to account for effect sizes themselves. How about if we just average the studies' estimates? For example, in Paluck et al.'s meta-analysis, the mean of the studies' estimates is `r round( mean(DF$d), digits )` This approach is perhaps a step in the right direction, but still has some important limitations. In particular, this approach gives equal weight to each study, such that a small study (like Clunies-Ross and O'Meara, 1989, with a sample size of `r DF[1, ]$n_total`) contributes as much to the mean effect size as a large study (like Boisjoly et al., 2006, with a sample size of `r DF[27, ]$n_total`). That doesn't make sense: larger studies should clearly carry more weight in the analysis. This brings us to our first principled approach to meta-analysis, namely fixed-effects meta-analysis.



### Combining results across studies using fixed-effects meta-analysis

As we've seen throughout this book, visualizing data before and after analysis helps benchmark and sanity-check our intuitions about the formal statistical results. In a meta-analysis, a common way to do so is the **forest plot**, which depicts individual studies' estimates and confidence intervals. (You can ignore for now the column of percentages and the final line, "RE Model"; we will return to these later.) In this plot, the larger squares correspond to more precise studies; notice how much narrower their confidence intervals are than the confidence intervals of less precise studies.


```{r meta-forest, fig.cap="Forest plot for Paluck et al. meta-analysis. Studies are ordered from smallest to largest standard error."}
metafor::forest(re.m,
       showweights = TRUE,
       header = TRUE,
       xlab = "Standardized mean difference")
```

Fixed-effects meta-analysis is a simple way to average studies' estimates while giving more weight to larger, more precise studies. Specifically, fixed-effects meta-analysis is a weighted average in which each study's estimate is weighted by its inverse-variance (i.e., the inverse of its squared standard error). This makes sense because larger, more precise studies have smaller variances, so they should get more weight in the analysis. The fixed-effects pooled estimate is:

$$\widehat{\mu} = \frac{ \sum_{i=1}^k w_i \widehat{\theta}_i}{\sum_{i=1}^k w_i}$$ where $k$ is the number of studies, $\widehat{\theta}_i$ is the point estimate of the $i^{th}$ study, and $w_i = 1/\widehat{\sigma}^2_i$ is study $i$'s weight in the analysis (i.e., the inverse of its variance).[^9]

In Paluck et al.'s meta-analysis, we would calculate the fixed-effects estimate, $\widehat{\mu}$, as:

<!-- hard-coded from DF$d[1:4] and DF$se_d[1:4] -->
$$\widehat{\mu} = \frac{ \frac{0.03}{0.08^2} + \frac{0.30}{0.08^2} + \frac{0.00}{0.10^2} + \frac{0.00}{0.11^2} + \cdots }{ \frac{1}{0.08^2} + \frac{1}{0.08^2} + \frac{1}{0.10^2} + \frac{1}{0.11^2} + \cdots }$$


We thus estimate that the effect size in these studies is a standardized mean difference of $\widehat{\mu}$ = `r round(as.numeric(fe.m$b), 2)`; 95% confidence interval \[`r paste0(round(fe.m$ci.lb, 2), ", ", round(fe.m$ci.ub, 2))`\]; $p=$ \< .001. That is, we estimated that intergroup contact was associated with a decrease in prejudice of `r round(as.numeric(fe.m$b), 2)` standard deviations.


[^9]: The standard error of the fixed-effects $\Mhat$ itself is simply $\sum_{i=1}^k w_i \widehat{\theta}_i}{\sum_{i=1}^k w_i}$. This can be used to construct a confidence interval or $p$-value.



### Limitations of fixed-effects meta-analysis

Fixed-effects meta-analysis does have important limitations. To see this intuitively, imagine we actually had the full dataset of individual observations from each study. (This is what meta-analysts dream of, but it's rarely feasible in practice.) Fixed-effects meta-analysis is like simply combining all the studies' datasets and then calculating the effect size--completly ignoring the fact that observations came from different studies.

But what if the studies differ from one another? For example, in Paluck et al.'s meta-analysis, many design elements differed in ways that could make the interventions more or less effective in different studies. Some studies recruited adult participants while others recruited children, some studies considered contact between participants of different races while others considered contact between participants of different ages, and so on. This means that the contact effect could actually differ across studies: an idea that is called **heterogeneity**.

Does this presence of heterogeneity remind you of anything from when we analyzed repeated-measures data in Chapter \@ref(models) on models? Recall that, with repeated-measures data, we had dealt with the possibility of heterogeneity across participants by introducing participant-level random intercepts to our regression model. Turns out that we can do a similar thing in meta-analysis.

### Random-effects meta-analysis

Whereas fixed-effects meta-analysis essentially postulates that all studies in the meta-analysis have the same population effect size, $\mu$, random-effects meta-analysis instead postulates that studies' population effects come from a normal distribution with mean $\mu$ and standard deviation $\tau$.[^2] The larger the standard deviation, $\tau$, the more heterogeneous the effects are across studies. A random-effects model then estimates both $\mu$ and $\tau$, for example by maximum likelihood (@dersimonian1986meta; @brockwell2001comparison).[^10]

[^2]: Technically, other specifications of random-effects meta-analysis are possible. For example, robust variance estimation does not require making assumptions about the distribution of effects across studies [@hedges2010robust\]. These approaches also have other substantial advantages, like their ability to handle effects that are clustered (e.g., because some papers contribute multiple estimates; @hedges2010robust; @pustejovsky2021meta) and their ability to provide better inference in meta-analyses with relatively few studies [@tipton2015small]. For these reasons, we tend to use these methods by default when conducting meta-analyses.

[^10]: A confidence interval and $p$-value for the random-effects estimate $\Mhat$ can be obtained using standard theory for maximum likelihood estimates [DOES THIS HAVE AN ANTECEDENT?] with an additional adjustment that helps account for uncertainty in estimating $\tau$ [@knapp2003improved].


The resulting estimate $\widehat{\mu}$ is still a weighted average of studies' estimates, as for fixed-effects meta-analysis:

$$\widehat{\mu} = \frac{ \sum_{i=1}^k w_i \widehat{\theta}_i}{\sum_{i=1}^k w_i}$$ but the inverse-variance weights now incorporate the heterogeneity as well: $w_i = 1/\left(\widehat{\tau}^2 + \widehat{\sigma}^2_i \right)$. These weights represent the inverse of studies' *marginal* variances, comprising not only statistical error due to their finite sample sizes ($\widehat{\sigma}^2_i$) but also genuine effect heterogeneity ($\widehat{\tau}^2$).[^3]

[^3]: The estimate of $\widehat{\tau}^2$ is a bit more complicated, but is essentially a weighted average of studies' residuals, $\widehat{\theta_i} - \widehat{\mu}$, while subtracting away variation due to statistical error, $\widehat{\sigma}^2_i$ [@dersimonian1986meta; @brockwell2001comparison].

The upshot is that small studies contribute more strongly to a random-effects meta-analysis (i.e., they have larger weights) than they do to a fixed-effects meta-analysis. Take another look at the forest plot: the column of percentages tells us the relative contribution of each study in the meta-analysis, which is larger for more precise studies than for less precise studies. Confidence intervals are typically wider in random-effects meta-analysis than in fixed-effects meta-analysis; heuristically, this is because random-effects meta-analysis has to draw inference to a distribution of heterogeneous effects rather than to only one population effect that is shared by all studies.

<!-- ### Reporting on heterogeneity -->

Remember that in random-effects meta-analysis, the estimate $\widehat{\mu}$ represents only the *mean* population effect across studies. It tells us nothing about how *variable* the effects are across studies. In practice, we would recommend always reporting the heterogeneity estimate $\widehat{\tau}$ as well, perhaps supplemented by other related metrics [@riley2011interpretation; @wang2019simple; @mathur_mam; @npphat]. Reporting on the heterogeneity can help indicate how consistent or inconsistent the effects are across studies, which may point to the need to investigate moderators of the effect.[^4]

[^4]: For example, one approach to investigate moderators in meta-analysis is meta-regression, in which moderators (e.g., type of intergroup contact) are included as covariates in a random-effects meta-analysis model [@thompson2002should]. As in standard regression, coefficients can then be estimated for each moderator, representing the mean difference in population effect between studies with versus without the moderator.

<!-- ### Applied example -->

Conducting a random-effects meta-analysis of Paluck et al.'s dataset yields $\widehat{\mu}$ = `r round(as.numeric(re.m$b), 2)` ; 95% confidence interval [ `r paste0(round(re.m$ci.lb, 2), ", ", round(re.m$ci.ub, 2))` ]; $p=$ < .001. That is, we estimated that, *on average across studies*, intergroup contact was associated with a decrease in prejudice of `r round(as.numeric(re.m$b), 2)` standard deviations. However, these effects appeared to differ considerably across studies; we estimated that the standard deviation of effects across studies was $\widehat{\tau}$ = `r tauStats[1]` ; 95% confidence interval [`r tauStats[2]`, `r tauStats[3]`]. To conveniently visualize these results, we can plot the estimated density of the population effects, which is just a normal distribution with mean $\widehat{\mu}$ and standard deviation $\widehat{\tau}$. 


```{r meta-densities, fig.margin = TRUE, fig.cap= "Estimated distribution of population effects from random-effects meta-analysis of Paluck et. al's dataset (heavy red curve) and estimated density of studies' point estimates (thin black curve)."}
# red line: fitted density of *population* effects from meta-analysis
# gray line: just a dumb density estimate from the *point estimates*
library(ggplot2)

ggplot(data = data.frame(x = c(-2, 2)),
       aes(x)) +
  
  # reference lines
  geom_vline(xintercept = 0,
             lwd = 1,
             color = "gray") +
  
  geom_vline(xintercept = re.m$b,
             lty = 2,
             lwd = 1,
             color = "red"
  ) +
  
  # estimated density of estimates (nonparametric)
  geom_density( data = DF,
                aes(x = d),
                adjust = 1.2 ) +
  
  # estimated density from meta-analysis (parametric)
  stat_function( fun = dnorm,
                 n = 101,
                 args = list( mean = re.m$b,
                              sd = sqrt(re.m$tau2) ),
                 lwd = 1.2,
                 color = "red") +

  ylab("") +
  scale_x_continuous( breaks = seq(-2, 2, 0.5)) +
  xlab("Standardized mean difference") +
  theme_minimal() +
  scale_y_continuous(breaks = NULL) +
  theme(text = element_text(size=16),
        axis.text.x = element_text(size=16))
```


## Bias in meta-analysis

Meta-analysis is an invaluable tool to synthesize evidence across studies. However, meta-analyses can be compromised by two categories of bias: **within-study biases** and **across-study biases**. Either type of bias can lead to meta-analysis estimates that are too large, too small, or in the wrong direction. We will now discuss examples of each type of bias. We will also discuss ways to address these biases when conducting a meta-analysis, including mitigating the biases at the outset through sound meta-analysis design and also assessing the robustness of the ultimate conclusions to possible remaining bias.

### Within-study biases

Within-study biases affect the internal validity of the meta-analyzed studies' internal validity (Chapter XXX; \[use examples of types of bias that were discussed earlier\]). These within-study biases can propagate to the results of the meta-analysis. For example, @paluck2019contact note that early studies on intergroup contact almost exclusively used nonrandomized designs. Imagine a bias-prone study in which participants were not randomly assigned to receive an intergroup contact intervention, but instead, the participants who already had the least prejudice at baseline were also the most likely to opt into receiving the contact intervention. In that case, baseline prejudice could confound the relationship between contact interventions and subsequent prejudice, perhaps resulting in a spuriously inflated estimate. Now, if many or all studies in a meta-analysis are subject to this same form of bias, the meta-analysis might arrive at an impressively large and precise pooled estimate, yet this estimate itself could be badly biased. Garbage in, garbage out!

To mitigate this problem, meta-analysts should try to define study eligibility criteria that reduce within-study biases (for example, by excluding nonrandomized studies when confounding is of concern, as @paluck2019contact did). After data have been collected, meta-analysts should also qualitatively assess studies' risks of bias using established rating tools [@art]. Additionally, it is often informative to conduct statistical sensitivity analyses that help characterize how sensitive meta-analysis results may be to residual bias that cannot be eliminated by limiting study eligibility [@art].

In Paluck et al.'s meta-analysis, one way they tried to counteract within-study biases was by only including studies that randomly assigned participants to receive a contact intervention. However, they could have gone a step further by coding for the extent to which, for example, participants may have differentially dropped out of the study. For example, imagine that (a) both a contact intervention and control condition have an equal distribution of prejudice scores and (b) the contact intervention does not significantly impact prejudice. If highly prejudiced people are more likely to drop out of the study when they are in the contact vs. control condition (e.g., because they do not like interacting with a member of a minority group), the mean of the contact intervention will become smaller. In this scenario, researchers may erroneously conclude that the contact intervention reduced depression, despite the fact that the effect can be attributed to differential attrition.

### Across-study biases

Across-study biases affect the meta-analyzed literature holistically and include, for example, publication bias and selective reporting, as discussed in Chapter XXX. Often, these across-study biases favor statistically significant positive results; in this case, results that survive to publication and inclusion in the meta-analysis will typically have inflated estimates (they are too large compared to the truth) and so result in an inflated meta-analytic estimate.

::: accident-report
⚠️ Garbage in, garbage out? Meta-analyzing bad research (Coles et al. 2019)

You may have heard that Botox can help eliminate wrinkles. But some researchers have also suggested that it may help treat clinical depression when used to paralyze the muscles associated with frowning. As crazy as they may sound, a quick examination of the literature would lead many to conclude that this treatment works. Studies that randomly assign depressed patients to either receive Botox injections or saline injections do indeed find that Botox recipients exhibit decreases in depression. And when you combine all available evidence in a meta-analysis, you find that this difference is quite large: $\widehat{\mu}$ = 0.83, 95% CI [0.52, 1.14].

However, Coles and colleagues (2019) pointed out that there is a problem with within-study bias. Participants are not supposed to know whether they have been randomly assigned to receive Botox or a control saline injections. However, only one of these treatments leads the upper half of your face to be paralyzed, and after a couple of weeks you're likely to figure out whether you received the Botox treatment or control saline injection. Thus, it is possible that the apparent effect of Botox on depression is actually an effect of placebo. The meta-analytic conclusions are potentially undermined by within-study bias.

Coles and colleagues (2019) also found evidence of between-study bias. When coding the effect sizes in the literature, they found that 51% of the outcomes measured were not reported by the study authors. For example, researchers may have collected two measures of depression, but only reported one in the manuscript. This raises concerns about selective reporting: that researchers examining the effects of Botox on depression are only reporting the outcomes that demonstrate an effect, while hiding away the outcomes that do not. In this scenario, any meta-analytic conclusions are potentially undermined by between-study bias.

This is sometimes called the Garbage In, Garbage Out problem in meta-analysis. If within- and between-study bias is not properly mitigated, it is difficult to conclude make valid inferences in a meta-analysis.
:::

::: accident-report
🔬 Accident Report: Quantifying publication bias in the social sciences

In 2014, @franco2014 and colleagues examined the population of 221 studies conducted through a funding initiative that helps researchers run experiments on nationally-representative samples in the U.S.. First, Franco and colleagues examined the records of these studies to determine whether the researchers found statistically significant results, a mixture of statistically significant and nonsignificant results, or only nonsignificant results. Then, Franco and colleagues examined the likelihood that these results were published in the scientific literature.

What results do you think were lease likely to be published? If you guessed "nonsignificant results" you are correct. Less than 25% of studies that produced only statistically non-significant results were published in the scientific literature, compared to over 60% of studies that produced statistically significant results. These results are consistent with fears about publication bias: that studies yielding non-significant results are less likely to be published than those that produce significant results.
:::

As in the context of within-study biases, meta-analysts should try to mitigate across-study biases during the design process. That is, meta-analysis inclusion criteria should be designed to cast a broad net, capturing not only high-profile, published studies on the topic, but also studies published in low-profile journals, and "gray literature" such as unpublished dissertations and theses (@lefebvre2019searching\].[^5] A broad search will also help capture not only studies in which the analysis that is relevant to the meta-analysis was primary "headline" finding, but also studies that reported the relevant analysis only as a secondary or supplemental result.

[^5]: However, evidence is mixed regarding whether including gray literature actually reduces across-study biases in meta-analysis [@tsuji2020addressing; @sapbe].

There are also statistical methods to help assess how robust the results may be to across-study biases. Among the most popular tools to assess and correct for publication bias is the **funnel plot** [@duval2000trim; @egger1997bias], a graph relating the meta-analyzed studies' point estimates to some measure of their precision, such as sample size or standard error. Here is an example of a type of funnel plot [@sapb] for a simulated meta-analysis of 100 studies with no publication bias:

<!-- MM: I used a type of enhanced funnel here that I believe helps visualize publication bias better than standard funnels. Mike, up to you if you want regular funnels instead. I've included commented code for those as well. I didn't describe the purpose of the grey diamonds for now. Text snippet if we decide to briefly mention that: "the grey diamonds represent worst-case point estimate within only the studies with $p \ge 0.05$ or negative estimates." -->


```{r meta-funnel-unbiased, fig.cap = "Significance funnel plot for a meta-analysis simulated to have no publication bias. Orange points: studies with $p<0.05$ and positive estimates. Grey points: studies with $p \ge 0.05$ or negative estimates. Black diamond: random-effects estimate of $\Mhat$ within all studies."}
# simulate meta with publication bias, but keep all studies
set.seed(451)
d.all = sim_data2( data.frame( k = 100,
                            per.cluster = 1,
                            mu = .5,
                            V = 1,
                            V.gam = 0,  # no clustering
                            sei.min = 0.10,
                            sei.max = 1,
                            true.dist = "norm",
                            SE.corr = FALSE,
                            select.SE = FALSE,
                            eta = 10  # set publication bias
),
keep.all.studies = TRUE )  

# published studies only
dp = d.all %>% filter(publish == TRUE)

meta.all = rma(yi = yi,
            vi = vi,
            data = d.all,
            method = "REML",
            knha = TRUE)

meta.NA = rma(yi = yi,
            vi = vi,
            data = d.all %>% filter(pval > 0.05 | yi < 0),
            method = "REML",
            knha = TRUE)

p1 = significance_funnel(yi = meta.all$yi,
                         vi = meta.all$vi,
                         favor.positive = TRUE,
                         plot.pooled = TRUE,
                         est.all = meta.all$b,
                         est.N = meta.NA$b) 
p1 + scale_x_continuous( breaks = seq(-2.5, 3.5, 0.5) )

# alternative: standard funnel without contour enhancement
# MM considers less preferable
# funnel(meta.all, 
#        xlab = "Standardized mean difference")
```

Notice that larger studies (those with smaller standard errors) cluster more closely around the mean of `r round(meta.all$b, digits)` than do smaller studies, but large and small studies alikehave point estimates centered around the mean. That is, the funnel plot is symmetric. In contrast, a funnel plot might look asymmetric. Here is what happens to our hypothetical meta-analysis if all studies with $p<0.05$ and positive estimates are published, but only 10% of studies with $p \ge 0.05$ or with negative estimates are published:

```{r meta-funnel-biased, fig.cap = "Significance funnel plot for the same simulated meta-analysis after publication bias has occurred. Orange points: studies with $p<0.05$ and positive estimates. Grey points: studies with $p \ge 0.05$ or negative estimates. Black diamond: random-effects estimate of $\Mhat$ within all studies."}
meta.p = rma(yi = yi,
            vi = vi,
            data = dp,
            method = "REML",
            knha = TRUE)

meta.p.NA = rma(yi = yi,
            vi = vi,
            data = dp %>% filter(pval > 0.05 | yi < 0),
            method = "REML",
            knha = TRUE)

p2 = significance_funnel(yi = meta.p$yi,
                         vi = meta.p$vi,
                         favor.positive = TRUE,
                         plot.pooled = TRUE,
                         est.all = meta.p$b,
                         est.N = meta.p.NA$b) 

p2 + scale_x_continuous( breaks = seq(-1, 3.5, 0.5) )
```


Notice that the introduction of publication has dramatically inflated the pooled estimate from `r round(meta.all$b, digits)` to `r round(meta.p$b, digits)`.[^8] Also, there appears to be a correlation between studies' estimates and their standard errors, such that smaller studies tend to have larger estimates than do larger studies. Such a correlation can indicate the presence of **"small-study effects"**, meaning that effect sizes differ systematically between small and large studies [@sterne2011recommendations].

Several popular statistical methods, such as Trim-and-Fill [@duval2000trim\] and Egger's regression \[@egger1997bias\] are designed to quantify this type of asymmetry. It is critical to note that small-study effects need not always represent publication bias: they can also reflect genuine substantive differences between small and large studies \[@egger1997bias; @lau2006case]. For example, in a meta-analysis of intervention studies, if the most effective interventions are also the most expensive or difficult to implement, these highly effective interventions might be used primarily in the smallest studies. Funnel plot methods detect these types of small-study effects as well as certain limited forms of publication bias [@smt].[^6]

[^6]: Essentially, funnel plots and most related methods can detect publication bias in which (1) small studies with large positive point estimates are more likely to be published than small studies with small or negative point estimates; and (2) the largest studies are published regardless of the magnitude of their point estimates. Funnel plots may not detect publication bias that favors significant results. For more detail on these points, see @smt.

For these reasons, assessments of publication bias in meta-analysis should rely not only on funnel plots [@smt]. There are numerous other methods that can be applied as well, such as **selection models**.[^7] These models can detect other forms of publication bias that funnel plots may not detect, such as publication bias that favors significant results.



You may also have heard of "$p$-methods" to detect across-study biases such as $p$-curve and $p$-uniform [@simonsohn2014p, @simonsohn2014b, @van2015meta\]. These methods essentially assess whether the significant $p$-values "bunch up" just under 0.05, which is taken to indicate publication bias. These methods are increasingly popular in psychology and have merits. However, it is important to note that these methods are actually simplified versions of selection models \[e.g., @hedges1984estimation\] that work only under considerably more restrictive settings than do the original selection models \[for example, when there is not heterogeneity across studies; @mcshane2016adjusting]. For this reason, it is usually (although not always) better to use selection models in place of the more restrictive $p$-methods.

In Paluck and colleagues' meta-analysis, they:

-   Used a regression-based approach for publication bias. Found a significant relation between standard error and effect size, which may be driven by publication bias. Used this same regression-based approach to try to correct for publication bias. Effect size was close-to-zero when doing so. Concluded that publication bias is a problem.
-   Informally compared studies that were pre-registered vs. not pre-registered. Found that the effect size among pre-registered studies is close to zero. Concluded that publication bias is a problem.



[^7]: High-level overviews of selection models are given in @mcshane2016adjusting and @smt. for more methodological detail, see for example @hedges1984estimation, @iyengar1988, and @vevea1995. For a tutorial on fitting and interpreting selection models, see @smt.

<!-- ## Summary -->

::: exercise
You read the following result in the abstract of a meta-analysis: "In 83 randomized studies of middle school children, replacing one hour of class time with mindfulness meditation significantly improved standardized test scores (standardized mean difference $\widehat{\mu} = 0.05$; 95% confidence interval: \[$0.01, 0.09$\]; $p<0.05$)." Why is this a problematic way to report on meta-analysis results? Suggest a better sentence to replace this one.

You read the rest of the meta-analysis, which concludes with: "These findings demonstrate robust benefits of meditation for children, suggesting that test scores improve even when the meditation is introduced as a replacement for normal class time." You recall that the heterogeneity estimate was $\widehat{\tau} = 0.90$. Do you think that this result regarding the heterogeneity tends to support, or rather tends to undermine, the concluding sentence of the meta-analysis? Why?

What kinds of within-study biases would concern you in this meta-analysis? How might you assess the credibility of the meta-analyzed studies and of the meta-analysis as whole in light of these possible biases?
:::

::: exercise
You do a meta-analysis on a literature in which statistically significant results in either direction are much more likely to be published that nonsignificant results. Draw the funnel plot you would expect to see. Is the plot symmetric or asymmetric?
:::

::: exercise
Why do you think small studies receive more weight in random-effects meta-analysis than in fixed-effects meta-analysis? Can you see why this is true mathematically based on the equations given above, and can you also explain the intuition in simple language?
:::

