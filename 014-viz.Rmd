# (PART) Analysis and Reporting {-}

# Visualization {#viz}

::: {.learning-goals}
üçé Learning goals:

-   Analyze the principles behind informative visualizations
-   Incorporate visualization into analysis workflow
-   Learn to make "the design plot"
-   Select different visualizations of variability and distribution
-   Connect visualization concepts to measurement principles.
:::

```{marginfigure, echo=TRUE}
<img src="images/snow_cholera_voronoi.jpeg"/>
Mapping out a cholera epidemic (1854). Line shows region for which Broad Street pump is nearest.
```

What makes visualizations so useful, and what role do they play in the toolkit of experimentology?
Simply put, data visualization is the act of "making the invisible visible." 
Our visual systems are remarkably powerful pattern detectors, and relationships that aren't at all clear when scanning through rows of raw data can immediately jump out at us when presented in an appropriate graphical form [@zacks2020designing].
Good visualizations aim to delibrately harness this power and put it to work at every stage of the research process, from the quick sanity checks we run when first reading in our data to the publication-quality figures we design when we are ready to communicate our findings.
Yet our powerful pattern detectors can also be a liability; if we're not careful, we can easily be fooled into seeing patterns that are unreliable or even misleading.
As psychology moves into an era of bigger data and more complex behaviors, we become increasingly reliant on **data visualization literacy** [@borner2019data] to make sense of what is going on.

::: {.case-study}
üî¨ Case study: Mapping a pandemic

In 1854, a deadly outbreak of cholera was sweeping through London.
The scientific consensus at the time was that diseases like cholera spread through breathing poisonous and foul-smelling vapors, an idea known as the "miasma theory" [@halliday2001death].
An obstetrician and anesthesiologist named John Snow, however, had proposed an alternative theory: rather than spreading through foul air, he thought that it was spreading a polluted water supply [@snow1855mode].
To make a public case for this idea, he started counting cholera deaths.
He marked each case on a map of the area, and indicated the locations of the water pumps for reference.
Furthermore, a line could be drawn representing the region that was closest to each water pump, a technique which is now known as a [Voronoi diagram](https://en.wikipedia.org/wiki/Voronoi_diagram).
The resulting illustration clearly reveals that cases clustered around an area called Golden Square, which received water from a pump on Broad Street.
Although the precise causal role of these maps in Snow's own thinking is disputed, and it is likely that he produced them well after the incident [@brody2000map], they have nonetheless played a significant role in the history of data visualization [@friendly2021history].^[Actually, the use of disease maps goes back even further! [@seaman1798inquiry] mapped an outbreak of yellow fever in New York City to argue that deaths clustered around a handful of waste sites. He turned out to be right, but for the wrong reasons! These waste sites were breeding grounds for mosquitos, which were the real culprits. Coincidentally, Seaman is also known as the first to introduce vaccines to the United States. He vaccinated his children against smallpox and later organized a program to provide free vaccines to the public.].

Nearly two centuries later, as the Covid-19 pandemic swept through the world, governmental agencies like the [CDC](https://covid.cdc.gov/covid-data-tracker) produced maps of the outbreak that have become much more familiar:

```{r viz-covid, fig.cap="Map showing the known locations of cumulative coronavirus cases by share of the population in each county (reproduced from the New York Times)"}
knitr::include_graphics("images/viz/covid-hot-spots.png")
```

These maps make abstract statistics visible: By assigning higher cumulative case rates to darker colors, we can see at a glance which areas have been most affected.
And we're not limited by the spatial layout of a map.
We're now also used to seeing the horizontal axis correspond to *time* and the vertical axis correspond to some value at that time.
Curves like the following, showing the 7-day average of new cases, allow us to see other patterns, like the *rate of change*.
Even though more and more cases accumulate every day, we can see at a glance the different 'waves' of cases, and when they peaked. 

```{r viz-cases, fig.cap="7-day average of new reported COVID cases (reproduced from the New York Times)"}
knitr::include_graphics("images/viz/covid-cases.png")
```

While these visualizations capture purely descriptive statistics, we often want our visualizations to answer more specific questions.
For example, we may ask about the effectiveness of vaccinations: how do case rates differ across vaccinated and unvaccinated populations?
In this case, we may talk about ``breaking out'' a curve by some other variable, like vaccination status:

```{r viz-cases, fig.cap="Rates of COVID cases by vaccination status (reproduced from [`covid.cdc.gov`](https://covid.cdc.gov/covid-data-tracker/#rates-by-vaccine-status))"}
knitr::include_graphics("images/viz/vaccination.png")
```

From this visualization, it is apparent that unvaccinated individuals are about 6x more likely to test positive.
At the same time, these visualizations were produced using *observational* data, and therefore make it challenging to draw causal inferences.
For example, people were not randomly assigned to vaccination conditions, and those who have avoided vaccinations may differ in other ways than those who sought out vaccinations.
Additionally, you may have noticed that these visualizations typically do not give a sense of the raw data, the sample sizes of each group, or uncertainty about the estimates. 
In this chapter, we will explore how to use visualizations in our own carefully controlled behavioral experiments. 
:::

## Basic principles of (confirmatory) visualization

In this section, we begin by introducing a few simple guidelines to keep in mind when making informative visualizations in the context of experimental psychology^[Given this relatively narrow focus, a full treatment of visualization is outside the scope of this book. The classic volumes are by @tukey1977exploratory and [@tufte2001visual,@tufte1997visual], and we recommend @healy2018data for a more contemporary guide. For the purposes of understanding the examples in this chapter, it should be sufficient to work through our R tutorials for data manipulation and visualization in Appendices C and D].
Remember that these needs may be distinct from other contexts, such as journalism or public policy.
You may have seen beautiful and engaging full-page graphics with small print and a wealth of information.
The art of designing and producing these graphics is typically known as **infoviz** and should be distinguished from what we call **statistical visualization** [@gelman2013].
Roughly, infoviz aims to construct rich and immersive worlds to visually explore: a reader can spend hours pouring over the most intricate graphics and continue to find new and intruiging patterns.
Statistical visualization, on the other hand, aims to convey the logic of the experiment at a glance, supporting specific questions or statistical inferences.
The principles below are tailored toward statistical visualizations.

```{marginfigure, echo=TRUE}
<img src="images/viz_infoviz.png"/>
Unlike statistical visualization, which aims to clearly expose the logic of an experiment at a glance, infoviz aims to provide a rich world of patterns to explore [reproduced from @infoviz].
```

### Principle 1: Show the design

There are so many different kinds of graphs (bar graphs, line graphs, scatter plots, and pie charts) and so many different possible attributes of those graphs (colors, sizes, line types).
How do we begin to decide how to navigate these decisions?
The most important principle guiding good statistical visualizations is to *show the design* of your experiment.
There are strong (unwritten) conventions about how your design maps onto your graphics, and following these conventions can minimize confusion.
Start with the variables you manipulate, and make sure they are clearly visible.
Conventionally, the primary manipulation of interest (e.g. condition) goes on the x-axis, and the primary measurement of interest (e.g. responses) goes on the y-axis.
Other variables of interest that you want to compare (e.g. secondary manipulations, demographics) are then 'broken out' by different colors or on different `facets' (see below).

### Principle 2: Show the data

Looking at older papers, you may be alarmed to notice how little information is contained in the graphs.
The worst offenders might show just two bars, representing averaged values for two conditions.
This kind of plot adds very little beyond a sentence in the text reporting the two numbers, but it can also be seriously misleading.
It hides variation in the data, making a noisy effect based on a few data points look the same as a more systematic one based on a larger sample.
The second principle of modern statistical visualization is therefore to *show the data* alongside summary statistics.
The minimal form of this principle is to *always include error bars* and to tell the reader what the error bars represent (a 95% confidence interval? a standard error of the mean?)
But we can often do even better, overlaying the distribution of the actual data points on the same plot to give the reader more information about the actual distribution.
For example, one recently proposed style that captures this principle is known as the "raincloud plot" [@allen2019raincloud].
A raincloud plot essentially combines the raw data (the 'rain') with a smoothed density (the 'cloud') and a boxplot giving the median and quartiles of the distribution. 

```{r raincloud, fig.cap="Example of a raincloud plot, reproduced from @allen2019raincloud"}
knitr::include_graphics("images/viz/raincloud.png")
```

### Principle 3: Facilitate comparison

```{marginfigure, echo=TRUE}
<img src="images/viz_hierarchy.png"/>
A suspiciously uniform distribution abruptly cutting off at 50k miles, reproduced from @datacolada.
```

make sure that the contrasts you want to interpret are highlighted visually

### Maximize information, minimize ink

following Tufte, use the simplest possible presentation of the maximal amount of information.

### ``Fix the axis labels''

::: {.case-study}
üî¨ Case study: Sklar et al. (2012) reported evidence of ‚Äúunconscious‚Äù arithmetic. Further reanalyses didn‚Äôt support this finding (Moors and Hesselmann 2018). Rabagliati et al. (2018) also failed to replicate. Visualizations provide a framework for asking about the ways the measurements relate to the manipulation that might have shed light on the issues. 
:::

## Confirmatory visualization 

- ‚ÄúThe design plot‚Äù
- Mapping ‚Äúvisual variables‚Äù (size, color, etc.) to the critical variables in your design
- How to make a plot that maps onto the key, pre-registered analyses
- Different levels of specificity for different audiences 

Visualizing variability

- Measures of precision: confidence intervals and how to extract them from our statistical models.

::: {.interactive}
‚å®Ô∏è Interactive box: bootstrapping for confidence intervals, a nice generic method for getting some sense of precision. But be careful: there are many common abuses of the bootstrap, for example to obtain inference in small samples.
:::

## Exploratory visualization

```{marginfigure, echo=TRUE}
<img src="images/viz_anscombe.png"/>
[@anscombe1973graphs]
```

- Visualization as a source of "data diagnostics" - can find artificats.
- The critical importance of visualizing the data distribution (histograms). 

::: {.accident-report}
‚ö†Ô∏è Accident report: [Distributional] gorillas in the midst: Many data analysts don‚Äôt bother checking whether they are violating distribution assumptions. If they did, they‚Äôd sometimes realize there is a gorilla in the midst (Yanai and Lercher 2020).
:::

::: {.accident-report}
‚ö†Ô∏è Accident report: Visualizations have been instrumental in detecting fraudulent data. For example, when [@datacolada] made a simple histogram of the car mileage data reported in [@shu2012signing] and released publicly by [@kristal2020signing], they were immediately able to observe that it followed an unusually uniform distribution, truncated at exactly 50,000 miles. Over a given period of time, we would typically expect something more bell-shaped: a small number of people will drive very little (e.g. 1000 miles), a small number of people will drive a lot (e.g. 50,000 miles), but most people will fall between these tails. It is highly surprising to find exactly the same number of drivers in every mileage bin. While more specialized analyses revealed further evidence of fraud (e.g. based on patterns of rounding and pairs of duplicated data points), this humble histogram was already enough to set off alarm bells. A recurring regret raised by the co-authors of this paper is that they never thought to visualize the data before reporting statistics.

```{marginfigure, echo=TRUE}
<img src="images/data_colada_uniform.png"/>
A suspiciously uniform distribution abruptly cutting off at 50k miles. Ring the alarm!
```
:::
