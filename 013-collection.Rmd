# Data collection {#collection}

::: {.learning-goals}
üçé Learning goals: 

* Outline the key features of informed consent
* Describe the four components of participant debriefing 
* Identify the additional federal provisions made for vulnerable populations 
* Review best practices for online and in person data collection
* Implement data integrity checks, manipulation checks, and pilot testing
:::


You have selected your measure and manipulation and planned your sample. Your preregistration is set. Now it's time to think about the nuts and bolts of collecting data. 

While the details of data collection may vary from context to context and sample to sample, this chapter will highlight some general best practices for the data collection process. We organize these practices around two perspectives. 

The first and second sections take participants' perspective. We begin by reviewing the importance of informed consent. onsent is a key part of running experiments that respect the autonomy of their participants. When we neglect the impact of our research on the populations we study, we not only violate our governing principles and bodies (like the Belmont Report and our institutional review board, respectively) that guide the work we do, but we also establish a pattern of imbalance and distrust that hinders our efforts. Next, we will take a look at a few vulnerable populations represented in research, and describe the steps to take that ensure appropriate and equitable participation for all who wish to do so. Finally, we take a design perspective on experiments, reviewing some concerns regarding how to provide a positive experience for participants in both in-person and online experiments. 

We then end by taking the experimenter's perspective, asking how we can collect high quality data. We review some best practices in pilot testing and the inclusion of manipuation and attention checks. 


::: {.case-study}
üî¨ Case study: The rise of online data collection

Since the rise of experimental psychology laboratories in university settings during the period after World War 2 [@benjamin-jr2000], experiments have typically been conducted by recruiting participants from what has been referred to as the "subject pool." This term denotes a group of people who can be recruited for experiments, typically students from introductory psychology courses [@sieber1989] recruited via the requirement that students complete a certain quantity of experiments as part of their course work.^[At various times, students have raised ethical concerns about these requirements as being coercive of participationin precisely the way that should be off limits for psychology experiments (see Chapter \@ref(ethics)). As a result, most programs now provide some more or less onerous alternative to participation.] The ready availability of this convenience population led inevitably to the massive over-representation of US undergraduates in published psychology research, leading to persistent critiques of this practice for the generalizability of research [@sears1986;@henrich2012].

Yet in the period 2005--2015, there has been a revolution in data collection from convenience populations. Instead of focusing on university undergraduates, increasingly, published psychology work uses convenience samples of online workers recruited from crowdsourcing sites like Amazon Mechanical Turk (AMT). Originally designed to distribute micropayments to workers for business purposes like retyping reciepts, these services have become marketplaces to connect researchers with research participants who are willing to complete surveys and experimental tasks for small payments. As of 2015, more than a third of studies in top social and personality psychology journals were conducted on crowdsourcing platforms (another third were still conducted with college undergraduates) [@anderson2019] and this proportion is likely continuing to grow.

Online data collection 

[@mason2016;@buhrmester2016]


Crump et al. (2013) show through a set of beautiful experiments designed in the web browser how online data collection can replicate effects initially found in the lab.
:::


## Informed consent

Let's continue our conversation on informed consent from Chapter \@ref(ethics). Before we can run an experiment with human subjects, we first need to get them to agree to participate. As we mentioned before, this is an important step in data collection, and the both the Office for Human Research Protections (OHRP) and the IRB have clear guidelines about what this process looks like. Ultimately, your drafted consent form will need to be reviewed by your IRB office, and they can advise you on whether you have adequately described all important information for participants to review. Our goal in this section is to get you thinking about good practices for informed consent. Don't wing it; follow the OHRP's "Informed Consent Checklist", which we will summarize below [ohrp1998]. Take a look at this consent form from one of our studies to help you identify lines from the checklist.

<!--consent form and checklist-->

At this point, you might be wondering how exactly to engage in informed consent without "giving away" what your study is about. How much "informing" should we do? This is a reasonable concern because of **demand characteristics**, or cues participants pick up on while involved in a study that help them figure out what the study is about [@orne1962]. Demand characteristics have the power to change results because participants may act in ways they think you want them to. Perhaps the strongest demand characteristic is telling participants exactly what you are studying during the consenting process. But informed consent is really about explaining the impact a study may have on someone rather than about what we as researchers can learn from it. When consenting, focus on what someone might think or feel as a result of participating in the study. Are there any physical or emotional risks associated? What should someone know about the study that may give them pause about agreeing to participate in the first place? Center the participant in the consent process rather than the research question.

Now that we know what should be included in a consent form, we next need to make sure participants are adequately informed about their involvement. How can we be reasonably sure of this? Before signing a consent form, participants should have the cognitive capacity to make decisions (competence), understand what they are being asked to do (comprehension), and know that they have the right to withdraw consent at any time (voluntariness) [@kadam2017]. 

As we will discuss later, some people cannot reasonably make decisions or protect their own interests, so the consent process may involve a third party. Establishing competence is the first step of informed consent. There are no circumstances in which a person deemed unable to meet the criteria of competence should complete the consent process alone. This is partially because it creates an unequal distribution of relevant information for decision-making. It also exploits the diminished autonomy of these individuals. When working with populations with certain limitations, try to identify who can legally provide consent while also including participants in the process as reasonably as possible. Just because someone cannot provide consent on their own does not mean they should lose the right to be informed about their participation. If possible, obtain their **assent**, or agreement to participate when a person has no legal ability to consent. Finally, respect their decision if they choose not to assent even if you previously obtained consent.

Comprehension is critical in human subjects research. Even though it is a good practice to verbally review the consent form with participants, the consent form itself must be readable for a broad audience, and certainly written at no more than a 7th grade reading level [@young1990].But there are also other ways to improve comprehension. @young1990 suggests giving participants a copy of the consent form in advance so they can read at their own pace, think of any outstanding questions they might have, and decide how to proceed without any chance of coercion. You might also consider highlighting important details that could be overlooked. As you review the consent form with participants, be sure to address every section even if it feels redundant. Encourage questions and spend some time coming up with a few of your own if you can. Ultimately, the goal is to provide participants with the information they need to make the best decision for themselves (or for their dependents).

Finally, participants should understand that their involvement is voluntary. This means that they are under no obligation to be involved in a study, and that signing a consent form does not waive their right to withdraw at any time. And while this seems easy enough, there are other considerations that may drive someone to think they really don't have much of a choice in participating. These considerations are called **structural coercion**. Structural coercion refers to features or characteristics that may strongly influence someone's decision to participate in research [@fisher2013]. This includes things like race, gender, income, and social hierarchy. For example, you might feel obligated to participate in a research study at a University you hope to attend someday or that is paying enough money to cover this month's phone bill. Structural coercion is often unconscious, so researchers tend to focus on more overt signs of coercion, like telling a student they have to participate in a study if they want an A in their psychology class. But just because it is unconscious, doesn't mean we shouldn't be thinking about it. Our responsibility as researchers is to consider the population we aim to study and ensure that they can make decisions about their participation independent of external forces.

## Debriefing participants

Every study should have a debriefing to end the study. In general, a debriefing is composed of four parts: (1) participation gratitude, (2) discussion of goals, (3) explanation of deception, and (4) questions and clarification [@allen2017]. 

**Gratitude.** Thank participants for their involvement in research study! Sometimes thanks is enough (for a short experiment), but many studies also include an additional token of appreciation such as monetary compensation or course credit. If monetary or other physical compensation will be implemented, it should be commensurate with the amount of time and effort required for participation. Compensation structures vary widely from place to place; typically local IRBs will have guidelines that they ask researchers to comply with.

**Discussion of goals.** Researchers should briefly share the purpose of the research study with participants. Why were participants recruited for this study in the first place? What are the researchers hoping to learn by conducting this study? It is important to ensure that participants fully understand the goals of the study, so avoiding technical jargon or confusing language is critical. You might also consider sharing any preliminary findings or where to find the completed write-up at the study's conclusion -- many engaged participants really appreciate a link to research findings, even months or years after participation.^[Sharing goals is especially important when some aspect of the study appears evaluative -- participants will often be interested in knowing how well they preformed against their peers. For example, a parent whose child completed a word-recognition task may request information about their child's performance. It is often important to highlight that the goals of the study are not about individual evaluation and ranking.] 

**Explanation of deception.** Researchers must reveal any deception during debriefing, regardless of how minor the deception seems to the researcher. This component of the debriefing process can be thought of as "dehoaxing" because it is meant to illuminate any aspects of the study that were previously misleading or inaccurate [@holmes1976]. The goal is both to reveal the true intent of the study and to alleviate any potential anxiety associated with the deception. After identifying where the deception occurred, it is useful to explain why the deception was necessary for the study's success. 

**Questions and clarification.** Finally, researchers should answer any questions or address any concerns raised by participants. Many researchers use this opportunity to first ask participants about their interpretation of the study, what they thought were the study goals. This not only illuminates aspects of the study design that may have been unclear to or hidden from participants, but it also begins a discussion where both researchers and participants can communicate about this joint experience. This step is also helpful in identifying negative emotions or feelings resulting from the study [@allen2017]. When participants do express negative emotions, researchers are responsible for sharing resources participants can use to work though the discomfort. 

With the widespread use of data collection sites such as Amazon Mechanical Turk (MTurk) and Prolific, many researchers have elected to conduct some or all of their studies online. Debriefing is required regardless of the study's presentation method. When a study is fully automated and participants do not interact with experimenters, researchers can make use of **debriefing statements**. Debriefing statements are documents that summarize all four components of the debriefing process (participation gratitude, discussion of goals, explanation of deception, and questions and clarification). Because experimenters are not present at the end of the study to answer participant questions, the statement typically provides the contact information for both the principal investigator and the IRB office. These communication channels should be clearly conveyed should participants need to follow up about the study for any reason.^[Some studies may not be ethically appropriate for being run online. Studies that have substantial deception or that induce negative emotions may require an experimenter present to alleviate concerns and address points of deception rather than relying on a written statement.]

### Special considerations for vulnerable populations

Regardless of who is participating in research, investigators have an obligation to protect the rights and well-being of all participants. However, some populations should be considered **vulnerable** because of their decreased agency -- either in general or in the face of potentially coercive situations. These populations should receive additional considerations or oversight when involving them in research studies. In this section, we will consider a few common vulnerable populations and discuss whether and how to include them in research studies.

**People with disabilities.** There are thousands of disabilities that affect cognition, development, motor ability, communication, and decision-making with varying degrees of interference, so it is first important to remember that considerations for this population will be just as diverse as its members. Officially, a disability is a diagnosed mental or physical condition that limits or restricts a person's involvement in daily activities [@stineman2001]. Roughly 8% of the US population is disabled, which makes it likely that, in the context of a research study, researchers may come into contact with someone who is disabled. Assuming all general rules and regulations are followed, there are no laws that preclude people with disabilities from participating in research. However, those with cognitive disabilities who are unable to make their own decisions (importantly, this also applies to children with or without disabilities) may only participant with written consent from a legal guardian and with their individual assent (if applicable). This means that even if the person provides assent, researchers may not enroll them in the research study without also obtaining consent from their guardian. Those retaining full cognitive capacity but who have other disabilities that make it challenging to participate normally in the research study should receive appropriate accommodations to access the material, including the study's risks and benefits. 

**Children.** Children are some of the most commonly used vulnerable populations in research because their contributions to our understanding of behavior, development, and cognition [among others] are invaluable. Because they are so invaluable, studying them likely won't go away in your lifetime or mine, so we must be vigilant about how we involve them in studies. Like people with disabilities, children under the age of 18 (though the exact age depends on state and local laws) may only participate in research with written consent from a parent or guardian and sometimes with their individual assent (if applicable). This is because children are not capable of protecting their own well-being, so this responsibility is deferred to the legal caregiver. In addition to the typical protections offered to all human subjects, children are also granted further protections under the law. Most notably, the risks associated with a research study must be no greater than minimal unless participants may receive some direct benefit from participating or participating in the study may improve a disorder or condition of which the participant was formally diagnosed [@ohrp]. This generally means that healthy, or otherwise unaffected, control participants could not be subject to greater than minimal risk. In the event that a caregiver provides written consent but their child of reasonable age and cognitive capacity refuses to provide assent, participation must be abandoned altogether. 

**Prison population.** Nearly 2.1 million people are incarcerated in the United States alone [@gramlich2021]. The use of prisoners in research has been a source of debate for decades, so the Office for Human Research Protections (OHRP) has supported their involvement under limited circumstances [@ohrp2003]. In such circumstances, the IRB has made special provisions to ensure that those with diminished autonomy are protected when participating in research. When researchers propose to study incarcerated individuals, the IRB must reconfigure their board to include at least one active prisoner (or someone who can speak from a prisoner's perspective) and ensure that less than half of the board has any affiliation to the prison system, public or private. After study approval, researchers must follow similar protocols as with other populations we have discussed so far. Importantly, researchers must not suggest or promise that participation will have any bearing on prison sentences or parole eligibility. You can imagine how this incentive would be so great, that it might strongly influence whether a prisoner chooses to participate. Like any other person, prisoners must have the agency to weigh the advantages and disadvantages of their participation, and that any compensation they receive is commensurate with their direct involvement. A question you might ask when determining whether a study involving incarcerated individuals is appropriate is, "Would a reasonable adult participate if they were not imprisoned?"

**Low-income populations.** Low-income populations are exceptional cases because any one person can easily fall under this category in addition to another. Participants with fewer resources may be persuaded by monetary or other economic incentives (think cash payments or gift cards) in exchange for their participation. If you were an attorney, you would call this **undue influence**. In psychology, we call this **social influence**. No matter which you choose, coercing anyone to participate in a study with payment so excessive that it influences their decision to engage should be avoided. When participants are paid fairly for their time and are properly informed about how they will be involved in research, we reduce the risk of exploiting the very populations we seek to support. 

**Crowd workers.** In recent years, researchers have made use of online data collection, or crowdsourcing, to reach a wider population both domestically and internationally. Importantly, crowdsourcing platforms were originally designed for businesses to collect data, not for academic research [@litman2017]. While the IRB has not mandated explicit guidelines for this population at the time this textbook was written, we believe this conversation to be critical as many of you may go on to crowdsource your own data one day. Let's first discuss who are crowd workers. In 2020, nearly 130,000 people completed MTurk studies [@moss2020]. Of those, 70% identified as White, 56% identified as women, and 48% had an annual household income below $50,000. There are several other crowdsourcing websites that we won't share demographics for here, but understanding the general makeup of crowd workers is important for recognizing who we include in our studies. But because crowdsourced research is largely unregulated, it should come as no surprise that exploitation on these platforms exists. A sampling of crowd work determined that the average wage earned was just \$2.00 per hour, and less than 5% of workers were paid at least the federal minimum wage (\$7.25 per hour at the time this textbook was written) [@hara2018]. While some platforms like Prolific have implemented minimum payment standards that all researchers (or requesters on these platforms) must follow, no government agencies have mandated such practices. Requesters also have the option to reject a crowd worker's submission for any reason, meaning that they are paid for their performance rather than their time. Without platform or government standards, it is up to individual requesters to commit to fair pay, which is likely at least the federal minimum wage (if applicable). It may also be easier to commodify crowd workers in ways that would be challenging to do with participants in a lab setting. People who make a career out of evaluating these issues call this commoditization "**human-as-a-service**", or **HaaS** [@xia2022]. Crowd workers alleviated a number of concerns that plagued researchers for decades; with limited budgets and time, reduced person-power, and a push to develop bigger and bigger datasets, HaaS offered relatively simple solutions to these concerns. The only problem? Humans are not commodities, at least not in any reasonable sense. If we, instead, applied IRB guidelines to our interactions with crowd workers, we might conclude that they (1) are research volunteers, not employees or contractors, (2) should be paid for their time, even if their data is ultimately unusable, (3) are entitled to the same level of data protection and anonymity as any other participant, and (4) should be informed about what their participation involves before giving consent. Harnessing the power of online data collection is easy when we treat crowd workers like any other participant protected by the IRB. 

::: {.accident-report}
‚ö†Ô∏è Accident report: The Tuskegee Syphilis Study 

In 1929, The United States Public Health Service (USPHS) was perplexed by the effects of a particular disease with an epicenter in Macon County, Alabama, with an overwhelmingly Black population [@brandt1978]. Syphilis is a sexually transmitted bacterial infection that can either be in a visible and active stage or in a latent stage. At the time of the study's inception, roughly 36% of Tuskegee's adult population had developed some form of syphilis, one of the highest infection rates in America [@white2006]. 

USPHS recruited 400 Black males from 25--60 years of age with latent syphilis and 200 Black males without the infection to serve as a control group to participate in what would become one of the most exploitative research studies ever done on American soil [@brandt1978]. The USPHS sought the help of the Macon County Board of Health to recruit participants with the promise that they would provide treatment for community members with syphilis. The researchers sought poor, illiterate Blacks and, instead of telling them that they were being recruited for a research study, they merely informed them that they would be treated for "bad blood", a phrase used in that time to refer to syphilis. 

Because the study was only interested in tracking the natural course of latent syphilis without any medical intervention, the USPHS had no intention of providing any care to its participants. To assuage participants, the USPHS distributed an ointment not been shown to be effective in the treatment of syphilis, and only small doses of a medication actually used to treat the infection. In addition, participants underwent a spinal tap which was presented to them as another form of therapy and their "last chance for free treatment." By 1955, just over 30% of the original participants had died from syphilis complications. 

It took until the 1970s before the final report was released and (the lack of) treatment ended. In total, 128 participants died of syphilis or complications from the infection, 40 wives became infected, and 19 children were born with the infection [@katz2011]. The damage rippled through two generations, and many never actually learned what had been done to them. The Tuskegee experiment violates nearly every single guideline for research described above -- indeed in its many horrifying violations of research participants' agency, it provides a blueprint for future regulation to prevent any aspect of it from being repeated. 

Investigators did not obtain informed consent. Participants were not made aware of all known risks and benefits involved with their participation. Instead, they were deceived by researchers who led them to believe that diagnostic and invasive exams were directly related to their treatment.^[In human subjects research, **deception** is a specific technical term that refers to cases when (1) experimenters withhold any information about its goals or intentions, (2) experimenters hide their true identity (such as when using a confederate), (3) some aspects of the research are under- or overstated to conceal certain information, or (4) participants receive any false or misleading information [@tai2011].]

Participants were denied appropriate treatment following the discovery that penicillin was effective at treating syphilis [@mahoney1943]. The USPHS requested that medical professionals overseeing their care outside of the research study not offer treatment to participants so as to preserve the study's integrity. This intervention violated participants' rights to equal access to care, which should have taken precedence over the results of the study. 

Finally, recruitment was both imbalanced and coercive. Not only were participants selected from the poorest of neighborhoods in the hopes of finding vulnerable populations with little agency, but they were also bribed with empty promises of treatment and a monetary incentive (payment for burial fees, a financial obstacle for many sharecroppers and tenant farmers at the time).
:::



### Data collection online

Online data collection is increasingly ubiquitous in the behavioral sciences. Further, the web browser ‚Äì alongside survey software like Qualtrics ‚Äì can be a major aid to transparency in sharing experimental materials.

- Validating the process of collecting data online. We briefly review studies suggesting that for general data collection across many paradigms, online data collection is valid. 
- When is online not enough? We describe cases where in-person data collection is necessary, highlighting psychophysical and physiological measurement and social interaction as two common classes of experiments that still cannot be done effectively online. 



Class MTurk Guidelines
Intro
In this class we will be running our replication projects on Amazon's Mechanical Turk (AMT/mTurk). mTurk is a platform on which Workers (or "Turkers") can complete Human Intelligence Tasks (HITs) for monetary compensation. HITs are put up by Requesters. mTurk originally was set up to do large-scale tasks that require human intelligence (e.g. labeling photos, finding telephone numbers, etc), but has recently been used by social scientists to conduct (large-scale) online experiments.

In this class, all replication projects will be launched using a common class account. This saves you the hassle of applying for a personal Requester account (which lately has become not-as-straightforward...), and also simplifies funding logistics. We will be giving more detailed instructions in class on how to access the class account and launch HITs etc.

General notes to keep in mind:

Many Turkers multi-task and do tens of HITs for many hours a day, so design your study with this in mind. Include appropriate attention checks, manipulation checks, and exclusion criteria (depending on the original study design as well).
Amazon worker IDs are actually tied to their (public) Amazon account and thus constitute identifiable information. Keep Turk IDs and all other identifiable information private. Anonymize all data before pushing to your git project repo, especially if your repo is public. There are useful tools for quickly automatically anonymizing IDs so reach out to the TAs if you want help with this!
Do all analyses on anonymized data. (This is to prevent cases where others are unable to reproduce your analyses because it might rely somehow on identifiable information. If you start with anonymized data, your analyses would never use any of this information.)
Resources
Beginner:
A gentle guide to MTurk (Links to an external site.)
Guide to Mturk basics (Links to an external site.)
MTurkers are people (Links to an external site.) (& the problem with common paradigms on MTurk)
Survey within MTurk platform (Links to an external site.)
Qualtrics Link (Links to an external site.)
Intermediate:
Turker Nation (Links to an external site.): Discussion board for Turkers
Creating a launch page to external site (Links to an external site.): advantage is that your study is embedded within MTurk, then opens a full page window for your task when required. Note launcher.html and task.html are separate.
Getting around the ridiculous extra 20% fee for 9+ participants (Links to an external site.)
MTurk fee structure  (Links to an external site.) (Links to an external site.)
IRB
We have approval under protocol #IRB-23274Links to an external site.: ‚ÄúReproducibility of psychological science and instruction.‚Äù

Before running your study, you will need to complete a short CITI human subjects training if you have not already. (See assignment).  

Please include the following text on the first page / consent form of your study:

By answering the following questions, you are participating in a study being performed by cognitive scientists in the Stanford Department of Psychology. If you have questions about this research, please contact us at stanfordpsych251@gmail.com. You must be at least 18 years old to participate. Your participation in this research is voluntary. You may decline to answer any or all of the following questions. You may decline further participation, at any time, without adverse consequences. Your anonymity is assured; the researchers who have requested your participation will not receive any personal information about you. We have recently been made aware that your public Amazon.com profile can be accessed via your worker ID if you do not choose to opt out. If you would like to opt out of this feature, you may follow instructions available here (Links to an external site.).

Please include a short debriefing in your experiment, thanking the participant, explaining in 2-4 lines what your study was about, and asking them not to share this information with other potential participants.

Additional notes on MTurk
Communicating with participants

When you are running either Pilot B or your actual study, please keep this gmail window open and monitor traffic on it.
If you get complaints about your study, please address them courteously and quickly (ideally, within a few hours). Turkers can be very helpful if you are responsive. Always assure them that they will be paid for their work.
Payment policies

When in doubt as to technical issues, pay the Turker. The TAs can help you bonus those who had technical issues with the experiment or completion code!
If a Turker seems especially difficult, then please bring their complaint to the attention of the course team. Turkers can potentially complain to IRB so err on the side of doing this more frequently if you have issues.

::: {.ethics-box}
Best practices for online research

The rise of Amazon Mechanical Turk was a

* Fair payment. 
* Good user experience for participants
* Clear communication 
:::


## Ensuring high quality data

In the second section of this chapter, we review a few key practices for 


::: {.accident-report}
‚ö†Ô∏è Accident report: Does data quality vary throughout the semester? 

Every lab that collects empirical data repeatedly using the same population builds up lore about how that population varies. One infant development lab famously repainted their walls a particularly bright shade of blue and claimed that their studies did not yield significant findings (even replicating highly robust paradigms) until they went back to a more neutral color. ...

The ManyLabs studies were a series of large-scale, collaborative studies that involved the same experimental protocol being run at a variety of different sites. 
:::


### Run pilot studies

A **pilot study** is a small study conducted before you collect your main sample. Smooth and successful data collection is typically difficult without piloting, at least the first time you do an experiment of a given type. Fundamentally, experiments induce a particular experience in their participants, and careful attention to the nature of that experience^[Even if the experience is somewhat tedious, like searching for a T amongst Ls for hundreds of trials!] requires iterative development. 

Pilot studies cannot tell you about expected effect size (as we discussed in Chapter \@ref(sampling)). They also cannot tell you about the significance of your main result. What they *can* do is tell you about whether your paradigm works. They can reveal:
* if your code crashes under certain circumstances
* if your instructions confuse a substantial portion of your participants
* if you have a very high dropout rate
* if your data collection procedure fails to log variables of interest
* if participants are disgruntled by the end of the experiment

We recommend that all experimenters do -- at the very minimum -- two pilot studies before they launch their experiment. 

The first pilot study, **pilot A**^[Good name, right?], is a test with non-naive participants. Your parents can do this experiment, or in a pinch you can run yourself a bunch of times (though this isn't preferable because you're likely to miss a lot of aspects of the experience that you are habituated to, especially if you've been debugging the software). The goal of pilot A is to ensure that your experiment is comprehensible, that participants can complete it, and that the data are logged appropriately. This last goal means that you must *analyze* the data from pilot A, at least to the point of checking that the relevant data about each trial is logged.^[At a minimum, for each trial you need to know a subject ID, a trial ID, the state of any manipulation (condition, trial type, etc.), and the value for the measure.] 

The second pilot study, **pilot B**, consists of a test of a small set of naive participants. Pilot size will depend on the costliness of running the experiment (in time, money, and opportunity cost) as well as your worries about the paradigm. If we're talking about a short online survey experiment, then running a pilot of 10--20 people is reasonable. A more extensive laboratory study might be better served by piloting just two or three people. The goal of this second study is to understand properties of the participant experience: for example, were they confused? Did they withdraw before the study finished? You won't have the numbers to make robust statistical inferences about these questions, but even a small number of pilots can tell you that your dropout rate is likely too high: if 5 of 10 pilot participants withdraw you may need to reconsider aspects of your design. It's critical for pilot B  that you debrief more extensively with your participants. This debriefing often takes the form of an interview questionnaire after the study is over ("what did you think the study was about?" and "is there any way we could improve the experience of being in the study?" can be helpful questions). 

Piloting is often an iterative process. We frequently launch studies for a pilot B, then recognise from the data or from participant feedback that they can be improved. We make tweaks and pilot again. Be careful not to overfit to small differences in pilot data -- the samples are small and so inferences will not be robust. The process should be more like workshopping a manuscript to remove typos and make it read better than doing a study.

In the case of especially expensive experiments, it can be a dilemma whether to run a larger pilot to identify difficulties since such a pilot will be costly. In these cases, one possibility can be to preregister a contingent strategy. For example, in a planned sample of 100 participants, you could preregister running 20 as a pilot sample with the stipulation that you will look only at their dropout rate and not at any condition differences in the target measure. Then the registration could state that if the dropout rate is lower than 25%, you will collect the next 80 participants and analyze the whole dataset including the initial pilot. This sort of registration can help you split the difference between cautious piloting and conservation of rare or costly data. 

### Keep consistent data collection records

Important to put checks in place on your data collection pipeline early.



### Measure participant compliance

Data collection in the field: An opinionated discussion of common pitfalls of field experiments in psychology.

- Blinding and randomization. Fieldwork makes it harder to maintain these critical principles of experimental design, potentially leading to bias. 
- Reasoning about and combatting selection bias.

https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00998/full


## Chapter summary: Data collection

In this chapter, we took both 




Consent is an ongoing and fluid process that doesn't end once you've gotten a signature. In this chapter, we applied what we learned about human subjects data collection to issues related to informed consent, debriefing, and special considerations for vulnerable populations. Regardless of who you choose to recruit, you'll want to think carefully about how you share the research opportunity with participants. By the time you have finished explaining what their involvement will look like, participants should understand what is being asked of them, have the cognitive capacity to give consent (or have someone who can), and provide this consent voluntarily. We've also shown you examples where these practices were ignored, causing devastating outcomes. History doesn't have to repeat itself when it comes to (un)informed consent. 

From the particpant's perspective, we emphasized fair payment, a good "user experience", nad clear communication as the three key factors ensuring that they 
