# Data collection {#collection}

::: {.learning-goals}
üçé Learning goals: Reason about the biases that can be introduced by different data collection methods; deploy manipulation checks. 
:::

::: {.case-study}
üî¨ Case study: Crump et al. (2013) show through a set of beautiful experiments designed in the web browser how online data collection can replicate effects initially found in the lab. 
:::


## The critical importance of piloting

A **pilot study** is a small study conducted before you collect your main sample. Smooth and successful data collection is typically difficult without piloting, at least the first time you do an experiment of a given type. Fundamentally, experiments induce a particular experience in their participants, and careful attention to the nature of that experience^[Even if the experience is somewhat tedious, like searching for a T amongst Ls for hundreds of trials!] requires iterative development. 

Pilot studies cannot tell you about expected effect size (as we discussed in Chapter \@ref(sampling)). They also cannot tell you about the significance of your main result. What they *can* do is tell you about whether your paradigm works. They can reveal:
* if your code crashes under certain circumstances
* if your instructions confuse a substantial portion of your participants
* if you have a very high dropout rate
* if your data collection procedure fails to log variables of interest
* if participants are disgruntled by the end of the experiment

We recommend that all experimenters do -- at the very minimum -- two pilot studies before they launch their experiment. 

The first pilot study, **pilot A**^[Good name, right?], is a test with non-naive participants. Your parents can do this experiment, or in a pinch you can run yourself a bunch of times (though this isn't preferable because you're likely to miss a lot of aspects of the experience that you are habituated to, especially if you've been debugging the software). The goal of pilot A is to ensure that your experiment is comprehensible, that participants can complete it, and that the data are logged appropriately. This last goal means that you must *analyze* the data from pilot A, at least to the point of checking that the relevant data about each trial is logged.^[At a minimum, for each trial you need to know a subject ID, a trial ID, the state of any manipulation (condition, trial type, etc.), and the value for the measure.] 

The second pilot study, **pilot B**, consists of a test of a small set of naive participants. Pilot size will depend on the costliness of running the experiment (in time, money, and opportunity cost) as well as your worries about the paradigm. If we're talking about a short online survey experiment, then running a pilot of 10--20 people is reasonable. A more extensive laboratory study might be better served by piloting just two or three people. The goal of this second study is to understand properties of the participant experience: for example, were they confused? Did they withdraw before the study finished? You won't have the numbers to make robust statistical inferences about these questions, but even a small number of pilots can tell you that your dropout rate is likely too high: if 5 of 10 pilot participants withdraw you may need to reconsider aspects of your design. It's critical for pilot B  that you debrief more extensively with your participants. This debriefing often takes the form of an interview questionnaire after the study is over ("what did you think the study was about?" and "is there any way we could improve the experience of being in the study?" can be helpful questions). 

Piloting is often an iterative process. We frequently launch studies for a pilot B, then recognise from the data or from participant feedback that they can be improved. We make tweaks and pilot again. Be careful not to overfit to small differences in pilot data -- the samples are small and so inferences will not be robust. The process should be more like workshopping a manuscript to remove typos and make it read better than doing a study.

In the case of especially expensive experiments, it can be a dilemma whether to run a larger pilot to identify difficulties since such a pilot will be costly. In these cases, one possibility can be to preregister a contingent strategy. For example, in a planned sample of 100 participants, you could preregister running 20 as a pilot sample with the stipulation that you will look only at their dropout rate and not at any condition differences in the target measure. Then the registration could state that if the dropout rate is lower than 25%, you will collect the next 80 participants and analyze the whole dataset including the initial pilot. This sort of registration can help you split the difference between cautious piloting and conservation of rare or costly data. 

## Data integrity checking

Important to put checks in place on your data collection pipeline early.

## Data collection online

Online data collection is increasingly ubiquitous in the behavioral sciences. Further, the web browser ‚Äì alongside survey software like Qualtrics ‚Äì can be a major aid to transparency in sharing experimental materials.

- Validating the process of collecting data online. We briefly review studies suggesting that for general data collection across many paradigms, online data collection is valid. 
- When is online not enough? We describe cases where in-person data collection is necessary, highlighting psychophysical and physiological measurement and social interaction as two common classes of experiments that still cannot be done effectively online. 


## Manipulation checks

Data collection in the field: An opinionated discussion of common pitfalls of field experiments in psychology.

- Blinding and randomization. Fieldwork makes it harder to maintain these critical principles of experimental design, potentially leading to bias. 
- Reasoning about and combatting selection bias.

https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00998/full


## Experimental practices, beliefs, and superstitions

::: {.accident-report}
‚ö†Ô∏è Accident report: Does data quality vary throughout the semester? 

Every lab that collects empirical data repeatedly using the same population builds up lore about how that population varies. One infant development lab famously repainted their walls a particularly bright shade of blue and claimed that their studies did not yield significant findings (even replicating highly robust paradigms) until they went back to a more neutral color. ...

The ManyLabs studies were a series of large-scale, collaborative studies that involved the same experimental protocol being run at a variety of different sites. 
:::