<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 1 Experiments and theories | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 1 Experiments and theories | Experimentology">

<title>Chapter 1 Experiments and theories | Experimentology</title>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />




<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="toc/toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-intro.html#intro"><span class="toc-section-number">1</span> Experiments and theories</a></li>
<li><a href="2-replication.html#replication"><span class="toc-section-number">2</span> Replication and reproducibility</a></li>
<li><a href="3-ethics.html#ethics"><span class="toc-section-number">3</span> Ethics</a></li>
<li class="part"><span><b>II Statistics</b></span></li>
<li><a href="4-estimation.html#estimation"><span class="toc-section-number">4</span> Estimation</a></li>
<li><a href="5-inference.html#inference"><span class="toc-section-number">5</span> Inference</a></li>
<li><a href="6-models.html#models"><span class="toc-section-number">6</span> Models</a></li>
<li class="part"><span><b>III Design and Planning</b></span></li>
<li><a href="7-measurement.html#measurement"><span class="toc-section-number">7</span> Measurement</a></li>
<li><a href="8-design.html#design"><span class="toc-section-number">8</span> Design of experiments</a></li>
<li><a href="9-sampling.html#sampling"><span class="toc-section-number">9</span> Sampling</a></li>
<li><a href="10-prereg.html#prereg"><span class="toc-section-number">10</span> Preregistration</a></li>
<li class="part"><span><b>IV Execution</b></span></li>
<li><a href="11-selection.html#selection"><span class="toc-section-number">11</span> Experimental strategy</a></li>
<li><a href="12-collection.html#collection"><span class="toc-section-number">12</span> Data collection</a></li>
<li><a href="13-management.html#management"><span class="toc-section-number">13</span> Project management</a></li>
<li class="part"><span><b>V Analysis and Reporting</b></span></li>
<li><a href="14-viz.html#viz"><span class="toc-section-number">14</span> Visualization</a></li>
<li><a href="15-eda.html#eda"><span class="toc-section-number">15</span> Exploratory data analysis</a></li>
<li><a href="16-writing.html#writing"><span class="toc-section-number">16</span> Reproducible writing</a></li>
<li><a href="17-meta.html#meta"><span class="toc-section-number">17</span> Meta-analysis</a></li>
<li><a href="18-conclusions.html#conclusions"><span class="toc-section-number">18</span> Conclusions</a></li>
<li class="part"><span><b>VI Appendices</b></span></li>
<li><a href="19-git.html#git"><span class="toc-section-number">19</span> GitHub Tutorial</a></li>
<li><a href="20-rmarkdown.html#rmarkdown"><span class="toc-section-number">20</span> R Markdown Tutorial</a></li>
<li><a href="21-tidyverse.html#tidyverse"><span class="toc-section-number">21</span> Tidyverse Tutorial</a></li>
<li><a href="22-ggplot.html#ggplot"><span class="toc-section-number">22</span> ggplot Tutorial</a></li>
<li><a href="23-instructors.html#instructors"><span class="toc-section-number">23</span> Instructor‚Äôs guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="intro" class="section level1" number="1">
<h1><span class="header-section-number">Chapter 1</span> Experiments and theories</h1>
<div class="learning-goals">
<p>üçé Learning goals:</p>
<ul>
<li>Define ‚Äúexperiment‚Äù and ‚Äútheory‚Äù</li>
<li>Reason about the relationship between the experimental method and causal inference</li>
<li>Contrast different views on scientific theories</li>
<li>Analyze features of an experiment that can lead to weaker or stronger tests of theory</li>
</ul>
</div>
<p>Welcome to Experimentology. This is a book about how to do psychology experiments! Much of what we cover in the book is about the nitty gritty of how to design your study, how to analyze your data, or even how to name your files! But before we can get into all that, we‚Äôre going to need to have a conversation about what an experiment is. And that in turn will lead us pretty quickly to talk about <strong>causality</strong>, since the unique contribution of experiments is to help us measure causal effects.</p>
<p>The guiding mantra of this book is that experiments are for estimating causal effects. and that good experiments do so in a maximally precise and unbiased way, leading to strong generalizations. Much of our advice about how to navigate decision-making with respect to measurement, design, and sampling comes directly from this mantra.</p>
<p>The second part of this chapter is about theory building. Sometimes you just want to see what happens, like a kid knocking down a tower. And sometimes you want to know the answer to a specific applied question, like ‚Äúwill giving a midterm vs.¬†weekly quizzes lead students in a class to perform better on the final.‚Äù But more often, our goal is to create psychological theories that help us explain and predict new observations. These theories are contentful hypotheses about causal structures and how they relate to observed effects.</p>
<p>Having discussed experiments and theories, we will then be in a position to talk a bit about what features an experiment will to contribute to theory, our last topic. But, as we‚Äôll do in most chapters in the book, we‚Äôll begin with a concrete example.</p>
<div class="case-study">
<p>üî¨ Case study: Generalization and similarity</p>
<!-- Let's play a number game. Imagine I give you the number 16, and ask you how similar other numbers are to that one. You'll probably agree that 32 is more similar than 33. Is 96 more similar than 94? How about 6 vs. 8? Perhaps surprisingly, people's judgments on such problems are quite systematic [@tenenbaum2000]: the more "features" that two numbers share, the more similar they are judged to be. Not only is 32 even like 16, it's also a power of 2, and divisible by 8; in contrast, 33 shares none of these features.  -->
<!-- Predicting arbitrary similarity is one of the hardest problems of psychology, and is likened to one of the biggest challenges for organisms: generalization.  -->
<p>How do you take what you know and apply it to a new situation? One answer is that you use the same answer that has worked in similar situations ‚Äì but to do this kind of extrapolation, you need a notion of similarity. Early learning theorists were obsessed with these issues of similarity and generalization because their view was that organisms (humans being no exception) learned conditioned associations with specific stimuli and then generalized from these associations to determine their response to new situations. So an animal conditioned to salivate to a tone of a particular frequency might salivate slightly less to a tone that was close in pitch, and salivate substantially less or not at all to a tone that was further away.</p>
<p>Roger Shepard worked for much of his career on understanding this problem of similarity and generalization, culminating in what he called the ‚Äúuniversal law of generalization,‚Äù which allowed generalizations to be derived in a wide range of stimulus spaces <span class="citation">(<a href="#ref-shepard1987" role="doc-biblioref">Shepard, 1987</a>)</span>. The first step in this process was establishing a stimulus space. For a stimulus domain like size, color, or even speech sounds, he used a procedure called ‚Äúmultidimensional scaling‚Äù to infer how stimulus items could be placed in a low-dimensional (often 2D) Cartesian space. Then, when he visualized generalization gradients within this space, he found the incredibly consistent pattern shown in Figure <a href="1-intro.html#fig:intro-shepard">1.1</a>. Working backwards from this pattern, he was able to establish a derivation for this exponential generalization gradient that allowed him to claim it as a universal law.</p>
<div class="figure"><span id="fig:intro-shepard"></span>
<p class="caption marginnote shownote">
Figure 1.1: Figure 1 from Shepard (1987). Generalization gradients for twelve diffent kinds of stimuli.
</p>
<img src="images/intro/shepard1987.png" alt="Figure 1 from Shepard (1987). Generalization gradients for twelve diffent kinds of stimuli." width="\linewidth"  />
</div>
<p>The pattern shown in Shepard‚Äôs work is an example of <strong>inductive theory building</strong>. In the vocabulary we‚Äôre developing, Shepard ran (or obtained the data from) <strong>randomized experiments</strong> in which the <strong>manipulation</strong> was stimulus dimension (e.g., circle size) and the <strong>measure</strong> was an explicit similarity judgment (e.g., how similar is this circle to that one). Then the theory that Shepard proposes links three <strong>constructs</strong> ‚Äì latent entities whose relationships the theory specifies. In particular, 1) similarity is used to derive 2) an internal psychological space, and 3) generalizations are then derived from distance in this space.</p>
<!-- ^[If you have some background in the philosophy of science, you may be ready to call the philosophy police! If you're feeling that way, just wait -- we'll try to link some of our ideas to at least a lay understanding of the philosophy of science literature!] -->
<!-- Could it be that the "number game" judgments above are the same kinds of generalizations that Shepard observed in his analyses of perceptual data? @tenenbaum2001 presented a theory of similarity that preserved almost all of Shepard's assumptions, but did not require a continuous metric space at all. In its simplest form, the model just stated that the strength of a generalization was proportional to its specificity. This simplification allowed the model to be used to compute similarity and generalization for arbitrary domains, even the features of whole numbers! So for example, the property of being even does not confer much similarity because it is shared by half of all whole numbers; in contrast, being a power of 2 conveys much more because it is rarer. When applied to the number game, this model produced a close correspondence with human judgments.  -->
<!-- Since the development of this generalization model, it has been applied to a wide variety of domains including word learning [@xu2007], object exploration [@gweon2010], and sequential rule learning [@frank2011]. Yet this model has been critiqued as psychologically implausible [@endress2013], since it requires people to know a priori how specific each particular feature is. Further, the general framework within which the model was posed has been critiqued extensively for its assumption that human reasoning conforms to optimal statistical computations [@jones2011]. One way of thinking about these critiques is that they question which phenomena are in the scope of the theory: and in particular, whether Bayesian models should explain human psychological processes, or whether they are just a descriptive account of certain average behavioral outcomes. -->
<!-- In our discussion below, we'll look at the broader goal of theory building and how this kind of critique echoes some of the main tensions around what theories are for and how theories can be evaluated. -->
<p>Shepard wrote in the conclusion of his 1987 paper, ‚ÄúPossibly, behind the diverse behaviors of humans and animals, as behind the various motions of planets and stars, we may discern the operation of universal laws.‚Äù While Shepard‚Äôs dream is an ambitious one, it defines one potential ideal for psychological theorizing. In our discussion below, we‚Äôll look at the broader goals of theory building and how theories can be evaluated.</p>
</div>
<div id="what-is-an-experiment-and-why-would-you-do-one" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> What is an experiment, and why would you do one?</h2>
<!-- - On one hand, experiments are ‚Äúthe worst way to learn about the world‚Äù (in the words of one of our mentors). ‚ÄúYou can't play 20 questions with nature and win‚Äù (Newell 1973). But experiments are also one of our best tools for making strong causal inferences about the hidden structure of the world. They allow us to not just observe the world but to systematically intervene on it. -->
<!-- - We used to just poke things or people and measure what happened (see Hacking, 1990 on the 19th century craze of just measuring everything for fun.) Now we typically want our experiments to resolve deeper questions or test hypotheses. -->
<p>When you do an experiment, you change the world in order to learn something new. This common-sense definition has two parts to it: the <strong>manipulation</strong> and the <strong>measure</strong>. The manipulation is the thing you are doing to the world, and the measure is the way you quantify the effects that your actions had. The manipulation licenses <strong>causal inference</strong>, which is our first topic. A second ingredient, <strong>randomization</strong>, licenses inferences about the locus of the causal effect.
<!-- The result is some **generalization** that can be made about unseen observations.  -->
We‚Äôll talk about each of these in turn.</p>
<p>Let‚Äôs think through an example. If you‚Äôve ever tried to write a paper or even a tricky email while listening to vocal music with lyrics, you might have had the feeling that the lyrics of the music interfered with your own writing. Let‚Äôs call this the ‚ÄúDylan Hypothesis‚Äù ‚Äì listening to music like Bob Dylan‚Äôs lyrically rich songs decreases writing skill in the moment while you‚Äôre listening to its.</p>
<p>The Dylan Hypothesis is a <strong>causal hypothesis</strong> ‚Äì meaning that we ascribe responsibility for the decrease in writing skill to this factor particularly.<label for="tufte-sn-1" class="margin-toggle sidenote-number">1</label><input type="checkbox" id="tufte-sn-1" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">1</span> Defining causality is one of the trickiest and oldest problems in philosophy, and we won‚Äôt attempt to solve it here! But from a psychological perspective, we‚Äôre fond of <span class="citation"><a href="#ref-lewis1973" role="doc-biblioref">Lewis</a> (<a href="#ref-lewis1973" role="doc-biblioref">1973</a>)</span>‚Äôs ‚Äúcounterfactual‚Äù analysis of causality. On this view, the Dylan Hypothesis amounts to the claim that, in some situation, if we <em>hadn‚Äôt</em> played Dylan, we <em>wouldn‚Äôt</em> have experienced a decrement in writing ability.</span> In what follows, we‚Äôll try to be precise about the causal inferences we‚Äôre discussing.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:intro-dylan1"></span>
<img src="images/intro/dylan1.png" alt="The hypothesized causal relationship of the Dylan Hypothesis." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 1.2: The hypothesized causal relationship of the Dylan Hypothesis.<!--</p>-->
<!--</div>--></span>
</p>
<p>In Figure <a href="1-intro.html#fig:intro-dylan1">1.2</a>, we show the Dylan Hypothesis using a kind of diagram called a <strong>causal graphical model</strong> <span class="citation">(<a href="#ref-pearl1998" role="doc-biblioref">Pearl, 1998</a>)</span>. Our outcome is writing skill (<span class="math inline">\(Y\)</span>) and our predictor is Dylan listening (<span class="math inline">\(X\)</span>). The edge between them represents a hypothesized causal relationship. Dylan listening is hypothesized have to a causal effect on writing skill, and not vice versa. Now let‚Äôs talk about how experiments allow us to make these causal inferences.</p>
<div id="causal-inference" class="section level3" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Causal inference</h3>
<p>Most science textbooks don‚Äôt start a definition of what an experiment is; psychology textbooks are an exception <span class="citation">(<a href="#ref-winston1996" role="doc-biblioref">Winston &amp; Blais, 1996</a>)</span>. Perhaps this is because experiments are a critical method for making strong causal inferences, which are otherwise in short supply in psychology, and so the contrast between experimental and non-experimental research is very salient for psychologists (and economists too). In contrast, the role of causality is much more straightforward in the physical sciences ‚Äì so straightforward that they don‚Äôt talk about it at all.</p>
<p>Causal inference is a central issue in any field that deals with human beings. People are very complex systems. Even from an intuitive perspective, there are many obvious factors ‚Äì personality, values, culture, physiology, genes ‚Äì that influence any individual choice. Further, scientists typically have relatively limited opportunities to intervene on complex social systems. Although we can carry out some kinds of experiments within ethical and practical limits, we‚Äôre not just allowed to go around doing what we want to the people around us, just to see what happens! This situation stands in stark contrast to the physical sciences: you can do pretty much anything you want with a chemical solution.</p>
<p>Returning to the Dylan Hypothesis, suppose we did an observational study where we measured our variables ‚Äì Dylan listening and writing quality ‚Äì in a large population.<label for="tufte-sn-2" class="margin-toggle sidenote-number">2</label><input type="checkbox" id="tufte-sn-2" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">2</span> This sounds impractical, but let‚Äôs imagine some kind of dystopian, Google Docs+Spotify surveillance in which the companies team up to monitor your writing and listening habits and provide quantitative estimates of writing quality and lyric density. Big data!</span> We might compute a correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and find some non-zero relationship between them. If we did this study, you might predict that listening to Dylan would be related to better writing.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:intro-dylan2"></span>
<img src="images/intro/dylan2.png" alt="Cofounding with age in the Dylan Hypothesis." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 1.3: Cofounding with age in the Dylan Hypothesis.<!--</p>-->
<!--</div>--></span>
</p>
<p>Can we make a causal inference? No.¬†Correlation doesn‚Äôt equal causation here. There is (at least one) confounding third variable: age (<span class="math inline">\(Z\)</span>). Age is positively related to both Dylan listening and writing skill in our population of interest. Older people tend to be good writers and also tend to be more into folk rockers.<label for="tufte-sn-3" class="margin-toggle sidenote-number">3</label><input type="checkbox" id="tufte-sn-3" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">3</span> We won‚Äôt even put a question mark on this edge because it seems likely to be true.</span></p>
<p>The causal relationship of age to our other two variables means that variation in <span class="math inline">\(Z\)</span> can induce a correlation in <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, even in the absence of a true causal link. When <span class="math inline">\(Z\)</span> is higher, so are <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> on average, so they are correlated. This gives us a slightly more precise definition of <strong>confounding</strong>, a concept that everyone learns to use and recognize in discussing experimental design. In our observational experiment, age is <strong>confounded</strong> with Dylan listening because it has a causal relationship with both the predictor and the outcome.<label for="tufte-sn-4" class="margin-toggle sidenote-number">4</label><input type="checkbox" id="tufte-sn-4" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">4</span> There‚Äôs a more technical definition that‚Äôs more precise here: that is, age provides a ‚Äúbackdoor‚Äù from Dylan listening into writing skill. The ‚Äúbackdoor criterion‚Äù (it‚Äôs really called that!) is a way to define confounding in more complex causal graphs.</span></p>
<p>Experiments are when we intervene on the world and measure the consequences. Here, this means forcing some people to listen to Dylan. In the language of graphical models, if we control the Dylan listening, variable <span class="math inline">\(X\)</span> is causally exogenous ‚Äì not caused by anything else in the system). We ‚Äúsnipped‚Äù the causal link between age and Dylan listening, shown by the scissors icon in Figure <a href="1-intro.html#fig:intro-dylan3">1.4</a>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:intro-dylan3"></span>
<img src="images/intro/dylan3.png" alt="Experimental intervention snips the causal link between the confounded variable and the outcome." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 1.4: Experimental intervention snips the causal link between the confounded variable and the outcome.<!--</p>-->
<!--</div>--></span>
</p>
<p>In sum, experiments allow us to make strong causal inferences because they allow us to wield the causal scissors, severing the links that would ordinarily allow ‚Äúupstream‚Äù variables like age to confound our inferences.<label for="tufte-sn-5" class="margin-toggle sidenote-number">5</label><input type="checkbox" id="tufte-sn-5" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">5</span> There is a whole wonderful field of causal inference methods for non-experimental and pseudo-experimental designs. Much of this work happens in econometrics and not psychology, and the methods are extremely sophisticated. We recommend <span class="citation"><a href="#ref-cunningham2021" role="doc-biblioref">Cunningham</a> (<a href="#ref-cunningham2021" role="doc-biblioref">2021</a>)</span> as an open access introductory text.</span></p>
</div>
<div id="randomization" class="section level3" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Randomization</h3>
<p>When we talk about experiments in psychology, we are talking about experiments that are designed based on the logic of <strong>comparison</strong>. In the physical sciences, we can intervene on a system, comparing before and after the intervention. An example of a simple experiment would be to measure the temperature of some water, heat it, and then measure the temperature again. We feel relatively warranted in making the inference that the heat caused the temperature to rise.</p>
<p>This logic feels sound because we believe that our action heating the water is the only thing that‚Äôs different between our initial measurement (which we might call a <strong>control</strong> measurement<label for="tufte-sn-6" class="margin-toggle sidenote-number">6</label><input type="checkbox" id="tufte-sn-6" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">6</span> Interestingly, this term is not much more than a hundred years old; it starts appearing in the medical literature in the 1890s and the psychological literature soon after that <span class="citation">(<a href="#ref-boring1954" role="doc-biblioref">Boring, 1954</a>)</span>.</span>) and the measurement after our experimental intervention. It feels like we ‚Äúheld everything constant‚Äù between the two observations. This is <span class="citation"><a href="#ref-mill1869" role="doc-biblioref">Mill</a> (<a href="#ref-mill1869" role="doc-biblioref">1869</a>)</span>‚Äôs ‚Äúmethod of differences‚Äù:</p>
<blockquote>
<p>If an instance in which the phenomenon under investigation occurs, and an instance in which it does not occur, have every circumstance in common save one, that one occurring only in the former; the circumstance in which alone the two instances differ, is the effect, or the cause, or an indispensable part of the cause, of the phenomenon.‚Äù</p>
</blockquote>
<p>In other words, comparison allows for causal inferences.</p>
<p>The trouble is that it‚Äôs not always as easy to ‚Äúhold everything constant.‚Äù Let‚Äôs do two different analogously-designed Dylan Hypothesis experiments. The first will be a <strong>within-participants</strong> design. We ask someone to produce two writing samples for us to rate; the first is written in silence and the second while listening to <em>Blood on the Tracks</em>. Now we have a problem: our two measurements occur at different times. Being the second observation as opposed to the first might have a causal effect on writing skill ‚Äì perhaps the participant practiced, perhaps they feel more comfortable in our experimental setup. Perhaps they feel more tired. In other words, we have introduced another confound ‚Äì this time between parts of the experiment and Dylan listening.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:intro-dylan4"></span>
<img src="images/intro/dylan4.png" alt="Confounding with order." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 1.5: Confounding with order.<!--</p>-->
<!--</div>--></span>
</p>
<p>Here‚Äôs our second experimental design, a <strong>between-participants</strong> design.
We get some individuals to listen to Dylan or not ‚Äì say we have our parents listen to Dylan and our friends do the experiment in silence ‚Äì and then measure their writing during the assigned listening (or non-listening) period. Now we have a different issue. The participants in the control and experimental (Dylan) are different from one another. Even though we intervened in the world, we might as well be in the situation of Figure <a href="1-intro.html#fig:intro-dylan2">1.3</a>, in the sense that we have differences between our participants that could make a causal difference (e.g., their age). Again, it doesn‚Äôt seem like everything got held constant.</p>
<p>Although structurally these two situations are quite different, they have a single theoretical solution: <strong>randomization</strong>. Randomization ‚Äì whether of the order of an intervention in a sequence or the assignment of participants to conditions ‚Äì is the key intervention that is guaranteed to break causal dependencies. In the case of the order confound, random assignment of condition order guarantees that our estimate of Dylan on writing skill is <strong>unbiased</strong><label for="tufte-sn-7" class="margin-toggle sidenote-number">7</label><input type="checkbox" id="tufte-sn-7" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">7</span> Unbiased with respect to order, of course! It could be biased by many other things.</span> on average.<label for="tufte-sn-8" class="margin-toggle sidenote-number">8</label><input type="checkbox" id="tufte-sn-8" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">8</span> It could be biased in any one situation. Because order has two known states, we can also <strong>balance</strong> order rather than randomizing it. We‚Äôll talk in some depth in Chapter <a href="8-design.html#design">8</a> about how to deal with these sorts of nuisance procedural confounds and when randomization vs.¬†counterbalancing strategies are most appropriate.</span> In the case of the participant confound, <strong>random assignment</strong> of participants to conditions is the key step that breaks the connection between the myriad confounds related to individual participants‚Äô identities and the manipulation of interest.</p>
<p>Random assignment is so central to the recipe of experimental psychology that sometimes we don‚Äôt even think about it as an ingredient! But without it, the cake simply doesn‚Äôt rise. One classic example of a violation of random assignment is the sequential recruitment of participants to conditions (e.g., first testing the non-Dylan group and then the Dylan group). This kind of practice can seem unproblematic to a research assistant carrying out a study (we‚Äôve done it!). But it confounds time with group participation and can lead to all sorts of unforeseen issues in causal inference. Imagine that Dylan wins the Nobel prize somewhere during the recruitment process. This event obviously could change the nature of your manipulation. But so could infinite other events ‚Äì from a global pandemic all the way to the normal course of the seasons changing. Random assignment is the only way to avoid these confounds.</p>
<p>Random assignment is great, but there is a caveat. Random assignment cannot rescue your <em>individual</em> sample from biases. For example, in your randomly-assigned conditions, maybe a few more Dylan condition participants happened to be run after his Nobel than before. Instead, random assignment guarantees that <em>on average</em> your estimate of the causal effect will be unbiased ‚Äì even if you get lucky or unlucky on this individual instance of your experiment.<label for="tufte-sn-9" class="margin-toggle sidenote-number">9</label><input type="checkbox" id="tufte-sn-9" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">9</span> We‚Äôll return to the topic of whether you should test for or adjust for ‚Äúunhappy randomization‚Äù in Chapter <a href="6-models.html#models">6</a>. A brief answer here, to pique your curiosity: no.</span> That‚Äôs pretty good, and maybe it‚Äôs the best you can hope for!</p>
<!-- ### Generalization -->
<!-- A third feature of typical psychology experiments is the desire to generalize from a **sample** to a **population**. Our causal effect of interest is not the effect of Dylan on *these particular* participants, it's the predicted effect for the population we are sampling from.  -->
<!-- This kind of statement gets us into deep water quite quickly because it raises questions about what populations we want to theorize about and in turn how we would sample in such a way as to make these inferences valid. We're going to discuss these issues in Chapter \@ref(sampling).  -->
</div>
</div>
<div id="what-is-a-theory" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> What is a theory?</h2>
<p>Experiments are a terrible way to learn about the world. They are costly and time-consuming, and sometimes they fail in uninterpretable ways. Yet they are our best way to estimate causal effects. Sometimes we just need to know about the existence or magnitude effects for some applied goal, say deciding whether implementing a particular curriculum will lead to changes in mathematics test scores. But often we want to do more. The ‚Äúextra‚Äù thing we would like to do is to build theories.</p>
<p>If you have a single experimental result, you don‚Äôt need to construct a theory. But once you have accumulated several experimental results, keeping track of them as a separate entities can become very difficult. As <span class="citation"><a href="#ref-newell1973" role="doc-biblioref">Newell</a> (<a href="#ref-newell1973" role="doc-biblioref">1973</a>)</span> remarked, ‚Äúyou can‚Äôt play 20 questions with nature and win‚Äù: eventually you need to synthesize your results into some sort of theory. This theory acts as a ‚Äúcompression‚Äù of the experimental data, allowing you to remember generalizations rather than just a list of specifics; the specifics can then be reconstructed from the generalizations <span class="citation">(<a href="#ref-mach1882" role="doc-biblioref">Mach, 1882</a>)</span>.</p>
<p>But what is a theory? We‚Äôll begin by talking about the specific enterprise of constructing psychological theories. We‚Äôll then move to discussing how theories make contact with experiments. Philosophers of science have discussed this issue extensively and we‚Äôll give you the briefest of tours of the landscape here since some of the key issues in topics like replication (Chapter <a href="2-replication.html#replication">2</a> and inference (Chapter <a href="5-inference.html#inference">5</a>) make use of this material. Finally, we‚Äôll discuss the role of formalized, quantitative models in psychological theorizing (connecting to our discussion of statistical modeling in Chapter <a href="6-models.html#models">6</a>).</p>
<div id="psychological-theories" class="section level3" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Psychological theories</h3>
<p>A psychological theory is a set of proposed causal relationships among variables. For the most part, the variables we theorize about are unseen (latent) entities that we call <strong>constructs</strong>. Constructs can range widely in specificity. <em>g</em> (general intelligence) is the classic psychological example of a broad construct. In contrast, the psychological distance metric that Shepard used in our case study above is a rather more specific example.<label for="tufte-sn-10" class="margin-toggle sidenote-number">10</label><input type="checkbox" id="tufte-sn-10" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">10</span> A tradition of research in cognitive science posits that human knowledge is organized into theories <span class="citation">(<a href="#ref-gopnik1994" role="doc-biblioref">Gopnik &amp; Wellman, 1994</a>)</span>, and that in particular we organize our knowledge about other people into an <strong>intuitive theory of psychology</strong>. This intuitive theory includes concepts like ‚Äúbelief,‚Äù ‚Äúdesire,‚Äù and ‚Äúemotion,‚Äù but it‚Äôs important to distinguish the constructs we posit in <em>psychological</em> theories from the terms in our <em>intuitive</em> theories. Often one is grounded in the other, but the failure to distinguish the two can lead to sloppy reasoning.</span> Theories then are groups of assumptions about the relationships between these constructs.</p>
<p>Occasionally, defining a construct is enough to describe a theory. Defining general intelligence as <em>g</em>, the shared variance between different tests of intelligence, is itself a theoretically-loaded move. But more frequently the constructs gain their meaning in part via their relationship to other constructs. For example, in Shepard‚Äôs theory, while <em>generalization</em> and <em>distance</em> were the two key constructs, the substantive part of the theory was the quantitative links between the two that he posited.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:intro-shepard-net"></span>
<img src="images/intro/shepard_network.png" alt="A causal model (&quot;nomological network&quot; for Shepard's universal law." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 1.6: A causal model (‚Äúnomological network‚Äù for Shepard‚Äôs universal law.<!--</p>-->
<!--</div>--></span>
</p>
<p>This web of constructs and assumptions is what <span class="citation"><a href="#ref-cronbach1955" role="doc-biblioref">Cronbach &amp; Meehl</a> (<a href="#ref-cronbach1955" role="doc-biblioref">1955</a>)</span> referred to as a <strong>nomological network</strong> ‚Äì a set of proposals about how different entities are connected to one another. This idea is illustrated by the philosopher of science <span class="citation"><a href="#ref-hempel1952" role="doc-biblioref">Hempel</a> (<a href="#ref-hempel1952" role="doc-biblioref">1952</a>)</span>, who wrote that:</p>
<blockquote>
<p>A scientific theory might‚Ä¶ be likened to a complex spatial network: Its terms are represented by the knots, while the threads connecting the latter correspond, in part, to the definitions and, in part to the fundamental and derivative hypotheses included in the theory. The whole system floats, as it were, above the plane of observation and is anchored to it by rules of interpretation.These might be viewed as strings which are not part of the network but link certain points of the latter with specific places in the plane of observation. By virtue of those interpretive connections, the network can function as a scientific theory: From certain observational data, we may ascend, via an interpretive string, to some point in the theoretical network, thence proceed, via definitions and hypotheses, to other points, from which another interpretive string permits a descent to the plane of observation. (p.¬†36)</p>
</blockquote>
<p>One way to sketch this kind of network would be to use the kind of causal graph we used above. So then a nomological network looks like a causal model linking a set of constructs. Shepard‚Äôs proposal about generalization could be drawn this way (Figure <a href="1-intro.html#fig:intro-shepard-net">1.6</a>), but notice that the arrow connecting distance and generalization is doing a lot of work! Shepard‚Äôs proposal wasn‚Äôt just that there was <em>some</em> causal link between these two constructs, but that this relationship had a very specific parametric form.<label for="tufte-sn-11" class="margin-toggle sidenote-number">11</label><input type="checkbox" id="tufte-sn-11" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">11</span> Calling the theory a ‚Äúnetwork‚Äù sounds like it‚Äôs a <strong>structural equation model</strong> (SEM) where there are circles and lines and the lines represent something akin to the correlations between the numbers in the circles. That‚Äôs one way to define a psychological theory, but it‚Äôs certainly not the only way! Structural equation models are also primarily designed for observational data, they don‚Äôt encode causal hypotheses the way we‚Äôve been talking about them.</span></p>
<p>Having such a theory allows you to <strong>explain</strong> and <strong>predict</strong>; both of these are good things for a theory to do. A theory provides a compression of potentially complex data into a smaller set of general factors whose interactions produce the observed data. Thus, a theory is doing its job if it allows you to say things like ‚Äúfactor A was responsible for effect B.‚Äù Shepard‚Äôs theory fulfills both of these criteria. It‚Äôs explanatory in the sense that it allows you to answer questions like ‚Äúwhy is generalization so much lower for shapes that are a bit further apart in psychological space‚Äù or broader questions like ‚Äúwhy does psychological generalization have an exponential form.‚Äù And it‚Äôs also predictive in that it suggests the (quantitative) consequence for generalization judgments that should result from a particular manipulation of distance.</p>
</div>
<div id="theory-testing-popper-kuhn-and-lakatos-oh-my" class="section level3" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Theory testing: Popper, Kuhn, and Lakatos, oh my!</h3>
<p>We have said that experiments allow the estimation of causal effects and theories describe causal relationships. How do theories make contact with evidence? Perhaps unsurprisingly, there is no consensus around this topic in the philosophy of science. Yet there are viewpoints on theory construction and testing that have emerged that are very useful for the working scientist to know about. We‚Äôll introduce one normative view of what theories are (and how they should be evaluated) that resonates with many scientists: Popper‚Äôs <strong>falsificationism</strong>. We‚Äôll then discuss three critiques of this viewpoint: 1) the descriptive challenge posed by <span class="citation"><a href="#ref-kuhn1962" role="doc-biblioref">Kuhn</a> (<a href="#ref-kuhn1962" role="doc-biblioref">1962</a>)</span>, 2) the challenge of holism, and 3) questions about confirmation that come from modern Bayesian views.<label for="tufte-sn-12" class="margin-toggle sidenote-number">12</label><input type="checkbox" id="tufte-sn-12" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">12</span> This is nowhere near a comprehensive introduction to this huge topic. Instead, we‚Äôre just picking and choosing a couple of topics that help set up discussions we‚Äôll be having throughout the rest of the book. For a very readable introduction to the broader field, we recommend <span class="citation"><a href="#ref-godfrey-smith2009" role="doc-biblioref">Godfrey-Smith</a> (<a href="#ref-godfrey-smith2009" role="doc-biblioref">2009</a>)</span>.</span></p>
<p>For Popper, a scientific theory is a set of generalizations about the world. These generalizations embody substantive hypotheses (like ‚Äúlyrically dense music decreases writing ability‚Äù). What makes such a statement a scientific hypothesis is that it is falsifiable, for example by an observation that contradicts it (someone not experiencing such a decrease despite listening to Dylan). On this view, theories are never <strong>confirmed</strong> ‚Äì they are posited and then at best can fail to be falsified for many years, despite repeated attempts to do so.</p>
<p>Popper‚Äôs viewpoint is appealing. The logic of falsificationism resonates with many scientists, who view themselves as trying to make correct generalizations with increasing scope and then to subject these to increasingly stringent tests. On the other hand, it has a number of serious weaknesses that mean that it‚Äôs not a great tool with which to analyze scientific practices (which is something we‚Äôd like to do here). Ideally our philosophy of science should give us a sense of what we are doing when we attempt to construct theories and do experiments.</p>
<p><strong>The descriptive challenge.</strong> A highly influential critique of Popper (and other related viewpoints that treat scientific progress as making logical generalizations) comes from the sociological and historical work of <span class="citation"><a href="#ref-kuhn1962" role="doc-biblioref">Kuhn</a> (<a href="#ref-kuhn1962" role="doc-biblioref">1962</a>)</span>. Kuhn studied scientific revolutions and suggested that they looked nothing like the falsification of a clear generalization via an incontrovertible observation. Instead, Kuhn described scientists as mostly working within <strong>paradigms</strong>. These paradigms are sets of questions, assumptions, methods, phenomena, and explanatory hypotheses that together allow for activities Kuhn described as <strong>normal science</strong> ‚Äì that is, testing questions within the paradigm, explaining new observations or modifying theory to fit these paradigms.</p>
<p>On Kuhn‚Äôs view, normal science is punctuated by periods of <strong>crisis</strong> and and <strong>revolution</strong> when the working assumptions of the paradigm break down. This doesn‚Äôt happen just because a single observation is inconsistent with the current set of assumptions (as falsification would suggest). Rather, there will often be a transition to a new paradigm that typically involves a striking success (such as the transition to a heliocentric viewpoint in astronomy).</p>
<p>For Popper, subsequent theories necessarily contain better generalizations and have greater scope. For Kuhn, that‚Äôs not necessarily true. Paradigms are <strong>incommensurable</strong> with one another. The new paradigm doesn‚Äôt necessarily explain all the same things the old one did (or test its hypotheses via the same measures). This is often very disconcerting for scientists because they like the feeling that they are moving towards ever greater truth.</p>
<p>On the other hand, in psychology, there are clear examples of paradigm shifts where the new paradigm does not subsume the old one. For example, behaviorist psychology produced a series of reliable generalizations about particular phenomena in animal learning (e.g., operant conditioning). The cognitive revolution that followed led to a transition in research topics, experimental methods, and theories, but the cognitivist theories did not typically provide an alternative account of conditioning behaviors. The sense in which behaviorism was overturned was that its scope was narrowed and fewer researchers focused on it as an account of other phenomena in language and mind.</p>
<p>The example of behaviorism is probably a relatively rare example of a true Kuhnian paradigm in psychology.<label for="tufte-sn-13" class="margin-toggle sidenote-number">13</label><input type="checkbox" id="tufte-sn-13" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">13</span> If you are interested in a perspective on this paradigm shift, <a href="https://www.psychologicalscience.org/observer/what-happened-to-behaviorism">here‚Äôs a relevant essay</a>.</span> Mostly we aren‚Äôt thinking of the entire discipline changing at once. To deal with this more incremental change, <span class="citation"><a href="#ref-lakatos1976" role="doc-biblioref">Lakatos</a> (<a href="#ref-lakatos1976" role="doc-biblioref">1976</a>)</span> defines the notion of <strong>research programs</strong> that are more like little paradigms that can run in parallel with one another. A research program is a set of hypotheses, phenomena, and methods that all cohere together and allow for progress in their explanatory and predictive scope. Psychology ‚Äì even experimental psychology ‚Äì to us looks more like it has a series of interconnected research programs than one big paradigm. When one research program seems like it‚Äôs not going anywhere, researchers are free to jump ship to another more promising one.</p>
<p><strong>The challenge of holism</strong>. A second point that makes Popper‚Äôs falsificationism a bad match for working scientists is the observation that no individual hypothesis can be falsified independently. Instead, the reasoning about whether a hypothesis relies on a large series of what are called <strong>auxiliary hypotheses</strong> that are necessary to link the observation to the theory. To take our running example, if we found no decrement in writing skill in one of our planned experiments, that observation might still not falsify the Dylan Hypothesis. Instead, the fault might be in any one of our auxiliary assumptions, like our measurement of writing skill, or our choice of Dylan songs. One major source of auxiliary hypotheses in psychology is the process of making variables like ‚Äúwriting skill‚Äù or ‚Äúlistening to lyrically-dense music‚Äù into concrete statements that make contact with the world.</p>
<p>The consequence of holism for psychology research is that we often must reason about our experimental observations through the lens of these auxiliary hypotheses. An unexpected observation may cause us to give up on our main hypothesis ‚Äì but it will often cause us to question our auxiliary assumptions instead (or as well). The work of developing good measures that we discuss in Chapter <a href="7-measurement.html#measurement">7</a> is tightly related. Selecting or creating a good measure is basically the same thing as considering the auxiliary assumption that your measure is a good stand-in for a quantity like writing skill.</p>
<p><strong>Bayesian confirmation</strong>. The final issue to raise with falsificationism is that on this view, theories are never confirmed, but rather only falsified. This idea is at odds with much of the intuitive way that scientists feel that they work. We often describe what we do as deriving a prediction from a theory, testing that prediction, and then feeling that our theory has gained credibility from the observations that are consistent with it. A falsificationist says that this sensation is an illusion and in fact the theory is simply surviving to be tested another day.</p>
<p>An alternative perspective comes from the Bayesian tradition that we‚Äôll learn more about in Chapters <a href="4-estimation.html#estimation">4</a> and <a href="5-inference.html#inference">5</a>. In a nutshell, Bayesians consider the idea that our subjective belief in a particular hypothesis can be captured by a probability, and that our scientific reasoning can then be described by a process of normative probabilistic reasoning <span class="citation">(<a href="#ref-strevens2006" role="doc-biblioref">Strevens, 2006</a>)</span>. The Bayesian scientist distributes probability across a wide range of alternative hypotheses; observations that are more consistent with a hypothesis increase its probability <span class="citation">(<a href="#ref-sprenger2019" role="doc-biblioref">Sprenger &amp; Hartmann, 2019</a>)</span>. This view is both intuitively plausible and empirically a nice match to people‚Äôs reasoning, as shown by research building on Shepard‚Äôs <span class="citation">(<a href="#ref-tenenbaum2001" role="doc-biblioref">Tenenbaum &amp; Griffiths, 2001</a>)</span>!</p>
<p>In sum, there‚Äôs no one view on the processes of theory testing and revision. If we think of a theory as a posited set of causal relationships, then it‚Äôs clear that we can‚Äôt just put this theory in direct contact with evidence. We need to think about its scope of application (in terms of the phenomena and measures its meant to have scope over), as well as the auxiliary hypotheses that link it to specific observations and the alternative theories that we are comparing it to.</p>
<!-- ^[We might also want to add to the theory some assumptions about how these constructs ground out into measurements. Some philosophers of science distinguish between the "core" assumptions of a theory and the "periphery" -- and we might be tempted to say that specific measurements are peripheral. If we came up with a better measure of the construct, or it turned out that our measure wasn't particularly good, we could replace it without too much damage to the underlying theory.] -->
</div>
<div id="models-and-theories" class="section level3" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> Models and theories</h3>
<p>Say we have a set of constructs whose relationship we are interested in. How do we describe this relationship? There is no one single vocabulary for psychological theories. Some relationships are stated qualitatively via <strong>verbal theories</strong> while others are stated quantitatively in <strong>formal theories</strong>.</p>
<p>Much of the psychology literature from Freud onward consists of verbal theories, including our Dylan Hypothesis. We could even elaborate this hypothesis into Dylan Theory, with some hypothesized mechanism such as ‚Äúverbal processing resources‚Äù that might be exhausted by both listening and writing at the same time. The strength of verbal theories is that they can be expressed and communicated easily. But typically their predictions are at best qualitative in nature. In our discussion of the Dylan Hypothesis, we just expected writing skill to go down while listening, and so we would have accepted any decrement as evidence confirming our hypothesis.</p>
<p>It‚Äôs very hard to make a strong test of a theory that makes only qualitative predictions <span class="citation">(<a href="#ref-meehl1990" role="doc-biblioref">Meehl, 1990</a>)</span>. For example, if we fail to see a decrement in writing skill while listening to Dylan, we can typically call on one of our auxiliary assumptions to salvage the issue. Because we have no quantitative expectations about how big a difference our auxiliary variables should make, it‚Äôs quite plausible they could erase or even reverse our effect. Language is also vague and context sensitive <span class="citation">(<a href="#ref-grice1975" role="doc-biblioref">Grice, 1975</a>)</span>. While these are arguably positive features from a communication perspective, they can lead to disagreements about the interpretation of a theory unless substantial care is taken in its expression.</p>
<p>The answer to these issues is to theorize using a framework such as a logical, mathematical, or statistical formalism. From a philosophical perspective, it‚Äôs perhaps surprising that mathematics is so ‚Äúunreasonably effective‚Äù as a vocabulary for the science <span class="citation">(<a href="#ref-wigner1990" role="doc-biblioref">Wigner, 1990</a>)</span>. But some sort of formalism is necessary for a theory to make quantitative predictions. Indeed, there have been calls for greater formalization of theory in psychology for at least the last 50 years <span class="citation">(<a href="#ref-harris1976" role="doc-biblioref">Harris, 1976</a>)</span>.<label for="tufte-sn-14" class="margin-toggle sidenote-number">14</label><input type="checkbox" id="tufte-sn-14" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">14</span> Due to the recent suggestion that we are in a ‚Äútheory crisis‚Äù (e.g., that psychology lacks theory; cf.¬†Chapter <a href="2-replication.html#replication">2</a>), recently there have been a spate of articles advocating for the development of formal theories throughout psychology <span class="citation">(<a href="#ref-oberauer2019" role="doc-biblioref">Oberauer &amp; Lewandowsky, 2019</a>; <a href="#ref-smaldino2020" role="doc-biblioref">Smaldino, 2020</a>)</span>.</span></p>
<p>There is no one framework that will be right for theorizing across all areas of psychology. Mathematical theories like Shepard‚Äôs have long been one tool that allows for precise and parametric statement of particular relationships. Yet stating such clear and general laws feels out of reach in many cases. If we had more Shepard-style theorists or theories, perhaps we‚Äôd be in a better place. Or perhaps such ‚Äúuniversal laws‚Äù are simply out of reach for most of human behavior.</p>
<p>An alternative approach creates and fits models of data that incorporate substantive assumptions about the structure of the data. The structural equation models we referred to above are one example of this approach, where linear regression ‚Äì in combination with some assumptions about which measurement is related to what ‚Äì is used to infer the strength of relationships.</p>
<p>Models that start from data are what we use all the time for data analysis. The trouble is, we often don‚Äôt interpret them as having substantive assumptions about the structure of the data, even when they do! For example, the choice of a linear regression implies a number of assumptions about how measurements are distributed and how they relate to your manipulation. For example, we could fit a simple regression model to our Dylan Hypothesis data, predicting writing skill as a function of condition. This model would yield a coefficient estimate estimating the effect of Dylan on writing skill. If we were drawing our theory, we could then put that estimate on the line between the two constructs.</p>
<p>This simple example raises all kinds of questions. What are the <em>units</em> of the relationship we‚Äôre describing? How do we generalize this theory to other music beyond the Dylan album we used? How specific is our estimate to the writing task we used? Should we expect this estimate to hold for other populations. From our viewpoint, these sorts of questions are not distractions ‚Äì they are the critical work of moving from experiment to theory! If we declared success via observing <span class="math inline">\(p&lt;.05\)</span> and moved on, we would not have avoided these questions but rather just ignored them.</p>
<p>Linear models in particular are ubiquitous in the social sciences because they are convenient to fit, but as theoretical models they are deeply impoverished. There is a lot you can do with a linear regression, but in the end, most interesting processes are not linear combinations of factors! In Chapter <a href="6-models.html#models">6</a> we try to draw out this idea further, reconstruing common statistical tests as models that can be repurposed to express contentful scientific hypotheses while recognizing the limitations of their assumptions.</p>
<p>One of the strengths of modern cognitive science is that it provides a very rich set of tools for expressing more complex models. For example, the modern Bayesian cognitive modeling tradition grew out of work like Shepard‚Äôs; in these models, a system of equations defines a probability distribution that can be used to estimate parameters, predict new data, or make other inferences <span class="citation">(<a href="#ref-probmods2" role="doc-biblioref">Goodman et al., 2016</a>)</span>. And neural network models ‚Äì which are now fueling innovations in artificial intelligence ‚Äì have a long history of being used as substantive models of human psychology <span class="citation">(<a href="#ref-elman1996" role="doc-biblioref">Elman et al., 1996</a>)</span>.</p>
<p>Computational or formal artifacts are not themselves psychological theories, but all of them can be used to create psychological theories via the mapping of constructs onto entities in the model and the use of the principles of the formalism to instantiate psychological hypotheses or assumptions. This book won‚Äôt go into more details about routes to building computational theories, but if you are interested, we encourage you to explore these frameworks as a way to deepen your theoretical contributions and to sharpen your experimental choices.</p>
</div>
</div>
<div id="theory-testing-in-psychology" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Theory testing in psychology</h2>
<p>Say we have a theory like Shepard‚Äôs universal law or the Dylan Hypothesis. How should we go about putting it in contact with data? If we wanted to follow Popper‚Äôs falsification strategy, we should simply go ahead and start deriving predictions from the theory, then checking to see if it is falsified. But without further guidance, this strategy might be pretty boring. In the classic example that‚Äôs used in the philosophy literature, this is like going around checking each and every swan to see if all swans are white, just hoping to find a black swan to falsify your generalization. Maybe we don‚Äôt know how to do better with swans but we do with scientific observations.</p>
<div id="risky-and-informative-tests" class="section level3" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> ‚ÄúRisky‚Äù and informative tests</h3>
<p>The key insight of theory testing is that you should look for observations that are likely for your theory but unlikely for other theories. <span class="citation"><a href="#ref-meehl1978" role="doc-biblioref">Meehl</a> (<a href="#ref-meehl1978" role="doc-biblioref">1978</a>)</span> calls these ‚Äúrisky tests‚Äù: gambles where the theory could be wrong.<label for="tufte-sn-15" class="margin-toggle sidenote-number">15</label><input type="checkbox" id="tufte-sn-15" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">15</span> Even if you‚Äôre not a <em>falsificationist</em> like Popper, you can still think it‚Äôs useful to try and falsify theories! Although holism is an important challenge, and so often a single observation is not perfectly probative, it‚Äôs still a great research strategy to look for those observations that are most inconsistent with a particular theory.</span> There are four major ingredients of an informative test of theories: the precision of the measurement, the precision of the theoretical prediction, the flexibility of the theory, and the availability of other theoretical alternatives.</p>
<p>These components together are not always discussed as thoroughly as they should be in psychological theories!<label for="tufte-sn-16" class="margin-toggle sidenote-number">16</label><input type="checkbox" id="tufte-sn-16" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">16</span> Much of Meehl‚Äôs writing is about the failures of theory testing in what he called ‚Äúsoft psychology.‚Äù It‚Äôs worth reading his litany of problems <span class="citation">(<a href="#ref-meehl1990" role="doc-biblioref">Meehl, 1990</a>)</span>, if only to understand what the hurdles are!</span> But you can see them all on display in the this schematic justification for an experiment: ‚ÄúWe made a precise measurement of behavior in condition P because theory A predicts a specific result X in this condition, whereas theory B predicts Y.‚Äù Let‚Äôs discuss each of these pieces.</p>
<p>First, if the experimental measurement is not precise, then it could be consistent with any number of results. Precision of measurement is a prerequisite for theory testing ‚Äì so much so that this type of precision is a major focus of the rest of the book! Many psychology experiments are designed merely to provide directional evidence and to ‚Äúreject the null hypothesis‚Äù of no difference. This kind of evidence is often consistent with many different theories.</p>
<p>Second, for a test to be ‚Äúrisky‚Äù for a theory, the theory needs to have a precise prediction that <em>could</em> be wrong. To take an example from Meehl, if your theory predicts that it‚Äôs going to be rainy in April, seeing confirmation of that prediction is not very surprising or informative. The theory hasn‚Äôt specified how rainy it should be, so it will be easy to claim many different observations are consistent. Only the most extreme dry weather will appear truly inconsistent. On the other hand, it would be very impressive to see confirmation of a theory that specified which days in April it was going to rain and how much rain fell on each of them.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:unnamed-chunk-2"></span>
<img src="images/intro/roulette.png" alt="A roulette wheel." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 1.7: A roulette wheel.<!--</p>-->
<!--</div>--></span>
</p>
<p>One way to think about theories is that they allow us to make <em>bets</em>. If we bet on a spin of the roulette wheel that it will show us red as opposed to black, we have almost a 50% chance of winning the bet. Winning such a bet is not impressive. But if we call a particular number, say 34, and win this bet, that is certainly riskier because we have a much smaller chance of being right.</p>
<p>Consider our running examples. The Dylan Hypothesis makes ‚Äì at best ‚Äì directional predictions about what should happen in a Dylan interference experiment. Seeing such predictions confirmed is not all that surprising, and hence winning this bet is not likely to be that impressive. On the other hand, Shepard‚Äôs universal law makes much more specific predictions. Because it states the functional form of the generalization curve, all of the measurements that deviate from that curve more than their measurement error will be inconsistent. Much riskier, and hence much more impressive.</p>
<p>The third ingredient to the risky test idea is that the theory must not be so flexible as to be able to ‚Äúpredict‚Äù any data. This issue comes up most commonly when theories are used to ‚Äúpost-dict‚Äù data ‚Äì that is, to explain them after the fact. It would obviously be ridiculous to say after the fact that those 14 days were precisely the days we thought it would rain in April. That‚Äôs one reason why we think of preregistrations as a type of bet, where you write down what you think will happen ‚Äì perhaps according to your theory ‚Äì before you conduct the experiment!</p>
<p>Verbal theories that do not have any formal or computational representation are at risk for this kind of flexibility. Especially when a theory predicts that multiple factors interact with one another, it‚Äôs easy to say that one or the other was ‚Äúdominant‚Äù in a particular situation. But the same issue can come up in computational theories as well. When a theory has many <strong>free parameters</strong> ‚Äì numerical choices that change its predictions and that can be fit to a particular dataset ‚Äì then it can often predict a wide range of possible datasets. This kind of flexibility reduces the risk of any particular experimental test, because the theorist can always say after the fact that the parameters were wrong but not the theory itself <span class="citation">(<a href="#ref-roberts2000" role="doc-biblioref">Roberts &amp; Pashler, 2000</a>)</span>!</p>
<p>The final ingredient of an informative experimental test of a theory is that there are other theories that do not predict the same thing! Imagine that we observe an effect of Dylan interference on writing quality. Before we declare victory for the Dylan hypothesis and start building our broader Dylan theory, we need to ask whether there are other hypotheses that also predict the same effects. For example, we might wonder whether models of phonological working memory would also predict inference <span class="citation">(e.g., <a href="#ref-baddeley1974" role="doc-biblioref">Baddeley &amp; Hitch, 1974</a>)</span>!</p>
<p>Going back to the idea of a Bayesian view of confirmation, the experiment that teaches us the most is going to be the one where the data are high probability according to one theory and low probability according to all others. We can use this idea try to figure out what the right experiment is by considering which specific experimental conditions derive differences between theories. In fact, this idea has a long history in statistics <span class="citation">(<a href="#ref-lindley1956" role="doc-biblioref">Lindley, 1956</a>)</span> and now goes by the name <strong>optimal experiment design</strong> <span class="citation">(<a href="#ref-myung2013" role="doc-biblioref">Myung et al., 2013</a>; <a href="#ref-ouyang2018" role="doc-biblioref">Ouyang et al., 2018</a>)</span>. The idea is, if you have two or more quantitative models, you can evaluate their predictions across a lot of conditions and pick the most informative one. If your theories aren‚Äôt as well-developed, this kind of formal analysis may not be possible, but the principle still holds.</p>
<p>In sum, experiments are not design equal with respect to comparing theories. We should look for experiments that are ‚Äúrisky‚Äù and informative tests of theories ‚Äì precise measurements of cases where theories make strong predictions that contrast with one another.</p>
</div>
<div id="theories-frameworks-and-paradigms" class="section level3" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Theories, frameworks, and paradigms</h3>
<p>You may be thinking at this point in the chapter, ‚Äúpsychology is full of theories but they don‚Äôt look that much like the ones you‚Äôre talking about!‚Äù Very few of the theories that bear that label in psychology are describe causal relationships linking explicitly defined constructs.</p>
<p>Here‚Äôs an example. <span class="citation"><a href="#ref-bronfenbrenner1992" role="doc-biblioref">Bronfenbrenner</a> (<a href="#ref-bronfenbrenner1992" role="doc-biblioref">1992</a>)</span>‚Äôs ecological systems theory is pictured in Figure <a href="1-intro.html#fig:intro-bronfenbrenner">1.8</a>. The key thesis of this theory is that children‚Äôs development occurs in a set of nested contexts that each affect one another and in turn affect the child. This theory has been immensely influential. Yet if it‚Äôs read as a causal theory, it‚Äôs almost meaningless: everything connects to everything, across all levels of description ‚Äì very hard to figure out what kind of risky predictions it makes!</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:intro-bronfenbrenner"></span>
<img src="images/intro/bronfenbrenner.png" alt="Bronfenbrenner's ecological systems theory. (From SimplyPsychology.org)." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 1.8: Bronfenbrenner‚Äôs ecological systems theory. (From SimplyPsychology.org).<!--</p>-->
<!--</div>--></span>
</p>
<p>Our solution is simply to say that this kind of thing gets called a theory all the time. But it is not really a theory in the sense that we are talking about ‚Äì or the sense that many scientists and philosophers of science think about when they talk about how theories are testable groups of assumptions that allow specific predictions about future observations.</p>
<p>That doesn‚Äôt mean that such ideas are valueless! Far from it. Ideas like Bronfenbrenner‚Äôs have inspired huge amounts of interesting research. They can also make a big difference to practice. Bronfenbrenner‚Äôs ideas have shifted practice as well ‚Äì for example by supporting a more ecological model in social work <span class="citation">(<a href="#ref-ungar2002" role="doc-biblioref">Ungar, 2002</a>)</span>. We think of these ideas as <strong>frameworks</strong> (or perhaps paradigms, using some part of the Kuhnian sense of the word). They inspire theory without being theories themselves.</p>
</div>
<div id="who-and-what-are-we-theorizing-about" class="section level3" number="1.3.3">
<h3><span class="header-section-number">1.3.3</span> Who (and what) are we theorizing about?</h3>
<p>In our running example of the Dylan hypothesis, we have been quite vague about the scope and limitations of generalization. Is this a theory of all people? Of US college students? Of Boomers that grew up listening to Dylan? A critical part of theorizing is saying when your theory <em>doesn‚Äôt</em> apply, and as a group, psychologists tend to be terrible at this <span class="citation">(<a href="#ref-yarkoni2020" role="doc-biblioref">Yarkoni, 2020</a>)</span>. Maybe Shepard is one of our few exceptions in that he very explicitly claimed a completely universal scope: his law is intended to apply to all people and all stimuli!</p>
<p>Generalizability across populations is a key challenge for psychological theories. For example, <span class="citation"><a href="#ref-henrich2010" role="doc-biblioref">Henrich et al.</a> (<a href="#ref-henrich2010" role="doc-biblioref">2010</a>)</span> coined the acronym <strong>WEIRD</strong>. This catchy name describes the oddness of making generalizations about all of humanity from experiments on a sample that is quite unusual because it is Western, Educated, Industrialized, Rich, and Democratic. We‚Äôll discuss the problem of sample generalizability ‚Äì and some ways to reason about the WEIRD problem ‚Äì in more depth in Chapter <a href="9-sampling.html#sampling">9</a>.</p>
<p>A second generalizability challenge is that experimental manipulations are typically realized across specific stimuli. Yet we too rarely reason about how general our hypothesis across the broader space of stimuli. As we‚Äôll see in Chapters <a href="6-models.html#models">6</a> and <a href="8-design.html#design">8</a>, this issue has consequences for our statistical analysis as well as for our experimental designs <span class="citation">(<a href="#ref-yarkoni2020" role="doc-biblioref">Yarkoni, 2020</a>)</span>.</p>
<p>Questions of generalizability are pervasive, but the first step is simply to acknowledge and reason about them. One mechanism for enforcing thinking about theory scope is the idea that all papers should have a section reporting their limitations. We tend to think this is a good idea. <span class="citation"><a href="#ref-simons2017" role="doc-biblioref">Simons et al.</a> (<a href="#ref-simons2017" role="doc-biblioref">2017</a>)</span> calls this a Constraints on Generality statement.<label for="tufte-sn-17" class="margin-toggle sidenote-number">17</label><input type="checkbox" id="tufte-sn-17" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">17</span> In an early working draft, the COG statement was called a Statement of Limits on Generality (SLOG), which we found more memorable.</span> They recommend explicit qualification of generalizability across 1) experimental participants, 2) experimental stimuli, 3) procedures, and 4) historical or temporal features.</p>
<div class="ethics-box">
<p>üåø Ethics box: Who are we theorizing about?</p>
<p>Generalizability of theoretical claims seems abstract, but it can have substantial practical consequences when theory is put into application. To take an example from medical research, it has been common practice for decades to study drug effects in mouse models. What is less well-appreciated is that the typical mouse model has been the <em>male</em> mouse only, with a six-fold difference in use of male relative to female mice. This seemingly practical decision may have widespread consequences <span class="citation">(<a href="#ref-shansky2021" role="doc-biblioref">Shansky &amp; Murphy, 2021</a>)</span>. Women experience a greater proportion of adverse consequences from pharmaceutical treatments than men, likely because research on development and evaluation of such treatments is conducted on male animals.</p>
<p>When it comes to research with humans, the analogy is clear. We hope that our theories are implemented in practice through the creation of psychologically-based policies, treatments, or educational interventions. But if these theories are formed using a population that mismatches the population to which the interventions are applied, then we should expect lowered efficacy and perhaps even unanticipated negative effects. To combat this issue, researchers have an ethical obligation to give careful consideration to the limitations of their samples and to the scope of applicability of their theories.</p>
</div>
</div>
</div>
<div id="experiments-and-theories-chapter-summary" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Experiments and Theories: Chapter summary</h2>
<p>In this chapter, we defined an experiment as a combination of a manipulation and a measure. This combination, along with randomization, licenses strong causal inferences about the structure of the world. Because they provide strong evidence on causality, experiments have a privileged position in building theories, although there are many other sources of information that can help us along the way!</p>
<p>We‚Äôve presented theories here as static entities that are presented, tested, confirmed, and falsified. That‚Äôs a simplification that doesn‚Äôt take into account the ways that theories ‚Äì especially with formal models as their instantiation ‚Äì can be flexibly adjusted to accommodate new data <span class="citation">(<a href="#ref-navarro2019" role="doc-biblioref">Navarro, 2019</a>)</span>. Lakatos‚Äôs idea of research programs, which we referenced above, incorporates this flexibility. The idea is that the research program ‚Äì as a combination of core principles, auxiliary assumptions, and supporting empirical assumptions ‚Äì is constantly and slowly shifting. Then a productive research program is one where the core principles are being adjusted to describe a larger base of observations. This idea sounds right to us: in our experience, the best theories are always being enlarged and refined in response to new data.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-1" class="exercise"><strong>Exercise 1.1  </strong></span>Identify an influential theory in your field or sub-field. Can you draw the ‚Äúnomological network‚Äù for it? What are the key constructs and how are the measured? Are the links between constructs well-specified? Or does this description of a theory not fit?</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-2" class="exercise"><strong>Exercise 1.2  </strong></span>Design an experiment for testing the Dylan Hypothesis. (A) What is the condition structure, what is the measure? How do you control for nuisance variables? (B) What are the best arguments against your proposed experimental design? What would an advocate of the Dylan Hypothesis say if it failed to yield a non-zero effect?</p>
</div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-baddeley1974" class="csl-entry">
Baddeley, A. D., &amp; Hitch, G. (1974). Working memory. In <em>Psychology of learning and motivation</em> (Vol. 8, pp. 47‚Äì89). Elsevier.
</div>
<div id="ref-boring1954" class="csl-entry">
Boring, E. G. (1954). The nature and history of experimental control. <em>The American Journal of Psychology</em>, <em>67</em>(4), 573‚Äì589. <a href="http://www.jstor.org/stable/1418483">http://www.jstor.org/stable/1418483</a>
</div>
<div id="ref-bronfenbrenner1992" class="csl-entry">
Bronfenbrenner, U. (1992). <em>Ecological systems theory.</em> Jessica Kingsley Publishers.
</div>
<div id="ref-cronbach1955" class="csl-entry">
Cronbach, L. J., &amp; Meehl, P. E. (1955). Construct validity in psychological tests. <em>Psychol. Bull.</em>, <em>52</em>(4), 281‚Äì302.
</div>
<div id="ref-cunningham2021" class="csl-entry">
Cunningham, S. (2021). <em>Causal inference</em>. Yale University Press.
</div>
<div id="ref-elman1996" class="csl-entry">
Elman, J. L., Bates, E. A., &amp; Johnson, M. H. (1996). <em>Rethinking innateness: A connectionist perspective on development</em> (Vol. 10). MIT press.
</div>
<div id="ref-godfrey-smith2009" class="csl-entry">
Godfrey-Smith, P. (2009). <em>Theory and reality</em>. University of Chicago Press.
</div>
<div id="ref-probmods2" class="csl-entry">
Goodman, N. D., Tenenbaum, J. B., &amp; Contributors, T. P. (2016). <em><span class="nocase">Probabilistic Models of Cognition</span></em> (Second). <a href="http://probmods.org/v2">http://probmods.org/v2</a>.
</div>
<div id="ref-gopnik1994" class="csl-entry">
Gopnik, A., &amp; Wellman, H. M. (1994). The theory theory. <em>An Earlier Version of This Chapter Was Presented at the Society for Research in Child Development Meeting, 1991.</em>
</div>
<div id="ref-grice1975" class="csl-entry">
Grice, H. P. (1975). Logic and conversation. In <em>Speech acts</em> (pp. 41‚Äì58). Brill.
</div>
<div id="ref-harris1976" class="csl-entry">
Harris, R. J. (1976). The uncertain connection between verbal theories and research hypotheses in social psychology. <em>Journal of Experimental Social Psychology</em>, <em>12</em>(2), 210‚Äì219.
</div>
<div id="ref-hempel1952" class="csl-entry">
Hempel, C. G. (1952). <em>Fundamentals of concept formation in empirical science, vol. Ii. No. 7</em>.
</div>
<div id="ref-henrich2010" class="csl-entry">
Henrich, J., Heine, S. J., &amp; Norenzayan, A. (2010). The weirdest people in the world? <em>Behavioral and Brain Sciences</em>, <em>33</em>(2-3), 61‚Äì83.
</div>
<div id="ref-kuhn1962" class="csl-entry">
Kuhn, T. (1962). <em>The structure of scientific revolutions</em>. Princeton University Press.
</div>
<div id="ref-lakatos1976" class="csl-entry">
Lakatos, I. (1976). Falsification and the methodology of scientific research programmes. In <em>Can theories be refuted?</em> (pp. 205‚Äì259). Springer.
</div>
<div id="ref-lewis1973" class="csl-entry">
Lewis, D. (1973). <em>Counterfactuals</em>. John Wiley &amp; Sons.
</div>
<div id="ref-lindley1956" class="csl-entry">
Lindley, D. V. (1956). On a measure of the information provided by an experiment. <em>The Annals of Mathematical Statistics</em>, 986‚Äì1005.
</div>
<div id="ref-mach1882" class="csl-entry">
Mach, E. (1882). The economical nature of physical inquiry. <em>Popular Scientific Lectures</em>, 186‚Äì213.
</div>
<div id="ref-meehl1978" class="csl-entry">
Meehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir karl, sir ronald, and the slow progress of soft psychology. <em>J. Consult. Clin. Psychol.</em>, <em>46</em>(4), 806‚Äì834.
</div>
<div id="ref-meehl1990" class="csl-entry">
Meehl, P. E. (1990). Why summaries of research on psychological theories are often uninterpretable. <em>Psychological Reports</em>, <em>66</em>(1), 195‚Äì244.
</div>
<div id="ref-mill1869" class="csl-entry">
Mill, J. S. (1869). <em>A system of logic, ratiocinative and inductive: Being a connected view of the princilples of evidence and the methods of scientific investigation</em>. Harper; brothers.
</div>
<div id="ref-myung2013" class="csl-entry">
Myung, J. I., Cavagnaro, D. R., &amp; Pitt, M. A. (2013). A tutorial on adaptive design optimization. <em>Journal of Mathematical Psychology</em>, <em>57</em>(3-4), 53‚Äì67.
</div>
<div id="ref-navarro2019" class="csl-entry">
Navarro, D. J. (2019). Between the devil and the deep blue sea: Tensions between scientific judgement and statistical model selection. <em>Computational Brain &amp; Behavior</em>, <em>2</em>(1), 28‚Äì34.
</div>
<div id="ref-newell1973" class="csl-entry">
Newell, A. (1973). <em>You can‚Äôt play 20 questions with nature and win: Projective comments on the papers of this symposium</em>.
</div>
<div id="ref-oberauer2019" class="csl-entry">
Oberauer, K., &amp; Lewandowsky, S. (2019). Addressing the theory crisis in psychology. <em>Psychonomic Bulletin &amp; Review</em>, <em>26</em>(5), 1596‚Äì1618.
</div>
<div id="ref-ouyang2018" class="csl-entry">
Ouyang, L., Tessler, M. H., Ly, D., &amp; Goodman, N. D. (2018). Webppl-oed: A practical optimal experiment design system. <em>CogSci</em>.
</div>
<div id="ref-pearl1998" class="csl-entry">
Pearl, J. (1998). Graphical models for probabilistic and causal reasoning. <em>Quantified Representation of Uncertainty and Imprecision</em>, 367‚Äì389.
</div>
<div id="ref-roberts2000" class="csl-entry">
Roberts, S., &amp; Pashler, H. (2000). How persuasive is a good fit? A comment on theory testing. <em>Psychological Review</em>, <em>107</em>(2), 358.
</div>
<div id="ref-shansky2021" class="csl-entry">
Shansky, R. M., &amp; Murphy, A. Z. (2021). Considering sex as a biological variable will require a global shift in science culture. <em>Nature Neuroscience</em>, <em>24</em>(4), 457‚Äì464.
</div>
<div id="ref-shepard1987" class="csl-entry">
Shepard, R. N. (1987). Toward a universal law of generalization for psychological science. <em>Science</em>, <em>237</em>(4820), 1317‚Äì1323.
</div>
<div id="ref-simons2017" class="csl-entry">
Simons, D. J., Shoda, Y., &amp; Lindsay, D. S. (2017). Constraints on generality (COG): A proposed addition to all empirical papers. <em>Perspectives on Psychological Science</em>, <em>12</em>(6), 1123‚Äì1128.
</div>
<div id="ref-smaldino2020" class="csl-entry">
Smaldino, P. E. (2020). How to translate a verbal theory into a formal model. <em>Social Psychology</em>.
</div>
<div id="ref-sprenger2019" class="csl-entry">
Sprenger, J., &amp; Hartmann, S. (2019). <em>Bayesian philosophy of science</em>. Oxford University Press.
</div>
<div id="ref-strevens2006" class="csl-entry">
Strevens, M. (2006). <em>The bayesian approach to the philosophy of science</em>.
</div>
<div id="ref-tenenbaum2001" class="csl-entry">
Tenenbaum, J. B., &amp; Griffiths, T. L. (2001). Generalization, similarity, and bayesian inference. <em>Behavioral and Brain Sciences</em>, <em>24</em>(4), 629‚Äì640.
</div>
<div id="ref-ungar2002" class="csl-entry">
Ungar, M. (2002). A deeper, more social ecological social work practice. <em>Social Service Review</em>, <em>76</em>(3), 480‚Äì497.
</div>
<div id="ref-wigner1990" class="csl-entry">
Wigner, E. P. (1990). The unreasonable effectiveness of mathematics in the natural sciences. In <em>Mathematics and science</em> (pp. 291‚Äì306). World Scientific.
</div>
<div id="ref-winston1996" class="csl-entry">
Winston, A. S., &amp; Blais, D. J. (1996). What counts as an experiment?: A transdisciplinary analysis of textbooks, 1930-1970. <em>The American Journal of Psychology</em>, <em>109</em>(4), 599‚Äì616. <a href="http://www.jstor.org/stable/1423397">http://www.jstor.org/stable/1423397</a>
</div>
<div id="ref-yarkoni2020" class="csl-entry">
Yarkoni, T. (2020). The generalizability crisis. <em>Behav. Brain Sci.</em>, 1‚Äì37.
</div>
</div>
<p style="text-align: center;">
<a href="index.html"><button class="btn btn-default">Previous</button></a>
<a href="2-replication.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="toc/toc.js"></script>


</body>
</html>
