<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 7 Models | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 7 Models | Experimentology">

<title>Chapter 7 Models | Experimentology</title>

<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.18/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-experiments.html#experiments"><span class="toc-section-number">1</span> Experiments</a></li>
<li><a href="2-theories.html#theories"><span class="toc-section-number">2</span> Theories</a></li>
<li><a href="3-replication.html#replication"><span class="toc-section-number">3</span> Replication and reproducibility</a></li>
<li><a href="4-ethics.html#ethics"><span class="toc-section-number">4</span> Ethics</a></li>
<li class="part"><span><b>II Statistics</b></span></li>
<li><a href="5-estimation.html#estimation"><span class="toc-section-number">5</span> Estimation</a></li>
<li><a href="6-inference.html#inference"><span class="toc-section-number">6</span> Inference</a></li>
<li><a href="7-models.html#models"><span class="toc-section-number">7</span> Models</a></li>
<li class="part"><span><b>III Design and Planning</b></span></li>
<li><a href="8-measurement.html#measurement"><span class="toc-section-number">8</span> Measurement</a></li>
<li><a href="9-design.html#design"><span class="toc-section-number">9</span> Design of experiments</a></li>
<li><a href="10-sampling.html#sampling"><span class="toc-section-number">10</span> Sampling</a></li>
<li class="part"><span><b>IV Execution</b></span></li>
<li><a href="11-prereg.html#prereg"><span class="toc-section-number">11</span> Preregistration</a></li>
<li><a href="12-collection.html#collection"><span class="toc-section-number">12</span> Data collection</a></li>
<li><a href="13-management.html#management"><span class="toc-section-number">13</span> Project management</a></li>
<li class="part"><span><b>V Analysis and Reporting</b></span></li>
<li><a href="14-viz.html#viz"><span class="toc-section-number">14</span> Visualization</a></li>
<li><a href="15-writing.html#writing"><span class="toc-section-number">15</span> Writing</a></li>
<li><a href="16-meta.html#meta"><span class="toc-section-number">16</span> Meta-analysis</a></li>
<li><a href="17-conclusions.html#conclusions"><span class="toc-section-number">17</span> Conclusions</a></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li><a href="A-git.html#git"><span class="toc-section-number">A</span> GitHub Tutorial</a></li>
<li><a href="B-rmarkdown.html#rmarkdown"><span class="toc-section-number">B</span> R Markdown Tutorial</a></li>
<li><a href="C-tidyverse.html#tidyverse"><span class="toc-section-number">C</span> Tidyverse Tutorial</a></li>
<li><a href="D-ggplot.html#ggplot"><span class="toc-section-number">D</span> ggplot Tutorial</a></li>
<li><a href="E-instructors.html#instructors"><span class="toc-section-number">E</span> Instructor‚Äôs guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="models" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> Models</h1>
<div class="learning-goals">
<p>üçé Learning goals:</p>
<ul>
<li>Re-conceptualize statistical ‚Äútests‚Äù as models of data</li>
<li>Build intuitions about how specific ‚Äútests‚Äù (e.g., t-tests) relate to more general frameworks (e.g., regression, mixed effects models)</li>
<li>Identify which models are best suited for which research questions</li>
<li>Describe what it means to ‚Äòcontrol for‚Äô something</li>
<li>Explore what kinds of clustered variance are present in our designs</li>
<li>Select models appropriate for different kinds of dependent variables</li>
</ul>
</div>
<p>In the previous two chapters, we introduced concepts surrounding estimation of an experimental effect and inference about its relationship to the effect in the population. The tools we introduced are very general, but they are limited in their applicability. Once you get beyond the world of two-condition experiments in which each participant contributes one data point from a continuous measure, the simple <span class="math inline">\(t\)</span>-test is not sufficient.</p>
<p>In some statistics textbooks, the next step would be to present a whole host of other statistical tests that are designed for other special cases. We could even show a decision-tree: what if you have repeated measures? Or categorical data? Or three conditions? But this isn‚Äôt a statistics book, and even if it were, we don‚Äôt advocate that approach. The idea of finding a specific narrowly-tailored test for your situation is part and parcel of the dichotomous NHST approach that we tried to talk you out of in the last chapter. If all you want is your <span class="math inline">\(p&lt;.05\)</span>, then it makes sense to look up the test that can allow you to compute a <span class="math inline">\(p\)</span> value in your specific case. But we prefer an approach that is more focused on getting a good estimate of the magnitude of the causal effect ‚Äì and the relation of that estimate to the population mean.</p>
<p>In this chapter, we begin to explore how to to go about making these estimates and making inference about them ‚Äì and that brings us to the world of <strong>statistical models</strong>. A statistical model is a way of writing down a set of assumptions about how particular data are generated. Statistical models are the bread and butter tools for estimating particular <strong>parameters</strong> of interest ‚Äì like the magnitude of a causal effect associated with an experimental manipulation ‚Äì and making inferences about their relationship to the population parameter.</p>
<p>For example, a simple statistical model might assume that observed datapoints are generated via with the flip of a weighted coin. Then the process of estimation is to assess the most likely weight of the coin given the data. This model can then be used to make inferences about whether the coin‚Äôs weight differs from some null model (a fair coin, perhaps).</p>
<p>This example sounds a lot like the kinds of simple inferential tests we talked about in the previous chapter; not very ‚Äúmodel-y.‚Äù But things get more interesting when there are multiple parameters to be estimated, as in many real-world experiments. In the tea-tasting scenario we‚Äôve belabored over the past two chapters, a real experiment might involve multiple people tasting different types of tea in different orders, all with some cups randomly assigned to be milk-first or tea-first. What we‚Äôll learn to do in this chapter is to make a model of this situation that allows us to reason about the magnitude of the milk-order effect while also estimating variation due to different people, orders, and tea types.</p>
<p>We‚Äôll begin by discussing the ubiquitous framework for building statistical models, <strong>linear regression</strong>, building up connections between regression and the <span class="math inline">\(t\)</span>-test. This section will discuss how to add covariates to regression models, and when linear regression does and doesn‚Äôt work. In the next section, we‚Äôll discuss the <strong>generalized linear model</strong>, an innovation that allows us to make models of a broader range of data types. We‚Äôll then briefly introduce <strong>mixed models</strong>, which allow us to model clustering in our datasets (such as clusters of observations from a single individual or single stimulus item). We‚Äôll end with some opinionated practical advice on model building.</p>
<div class="case-study">
<p>üî¨ Case study: Stimulus-specific effects</p>
<p>Imagine you‚Äôre a psycholinguist who has the hypothesis that nouns are processed faster than verbs. You run an experiment where you pick out ten verbs and ten nouns, then measure a large sample of participants‚Äô reading time for each of these. You find strong evidence for the predicted effect and publish a paper on your claim. The only problem is that, at the same time, someone else has done exactly the same study ‚Äì with different nouns and verbs ‚Äì and published a paper making the opposite claim. The problem in this example is that each effect is driven by the specific experimental items that were chosen <span class="citation">(<a href="#ref-clark1973" role="doc-biblioref">Clark, 1973</a>)</span>. Out of hundreds of thousands of possible words, why these in particular?</p>
<p>The problem of generalization from sample to population is not new ‚Äì as we discussed in Chapter <a href="6-inference.html#inference">6</a>, we are constantly doing this kind of inference with the samples of people that participate in our experiments. Our classic statistical techniques are designed to generalize from sample to population and we are typically sensitive to the weakness of generalizations made from very small samples of experimental participants. Not so with stimuli.<label for="tufte-sn-92" class="margin-toggle sidenote-number">92</label><input type="checkbox" id="tufte-sn-92" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">92</span> <span class="citation">Clark (<a href="#ref-clark1973" role="doc-biblioref">1973</a>)</span>, from whom this example is adapted, calls this the ‚Äúlanguage-as-fixed-effect‚Äù fallacy. This is a great label for folks who already know about fixed vs.¬†random effects, but it doesn‚Äôt highlight how it connects to the broader set of issues of generalizability that we highlight here and in Chapter <a href="10-sampling.html#sampling">10</a>, so we‚Äôll mostly use the label ‚Äústimulus generalizability.‚Äù</span></p>
<p>Stimulus generalizability problems have reared their head across a surprising range of different areas of psychology. In one example, hundreds of papers were written about a phenomenon called the ‚Äúrisky shift‚Äù ‚Äì in which groups deliberating about a decision would produce riskier decisions than individuals. Unfortunately, this phenomenon appeared to be completely driven by the specific choice of vignettes that groups deliberated about, with some producing a risky shift and others producing a more conservative shift <span class="citation">(<a href="#ref-westfall2015" role="doc-biblioref">Westfall et al., 2015</a>)</span>.</p>
<p>Another example comes from the memory literature, where a classic paper by <span class="citation">Baddeley et al. (<a href="#ref-baddeley1975" role="doc-biblioref">1975</a>)</span> suggested that words that take longer to pronounce (‚Äútycoon‚Äù or ‚Äúmorphine‚Äù) would be remembered worse than words that took a shorter amount of time (‚Äúember‚Äù or ‚Äúwicket‚Äù) even when they had the same number of syllables. This effect also appears to be driven by the specific sets of words chosen in the original paper; the effect is robustly replicable with that set but not generalizable across other sets <span class="citation">(<a href="#ref-lovatt2000" role="doc-biblioref">Lovatt et al., 2000</a>)</span>.</p>
<p>The implication of these examples is clear: experimenters need to take care in both their experimental design and analysis to avoid overgeneralizing from their stimuli to a broader construct. Three primary steps can help experimenters avoid this pitfall:</p>
<ol style="list-style-type: decimal">
<li>To maximize generality, use samples of experimental items ‚Äì words, pictures, or vignettes ‚Äì that are comparable in size to your samples of participants.</li>
<li>When replicating an experiment, consider taking a new sample of items as well as a new sample of participants.</li>
<li>When experimental items are sampled random from a broader population, use a statistical model ‚Äì such as the ones described below ‚Äì that includes this sampling process.</li>
</ol>
</div>
<div id="regression-models" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Regression models</h2>
<p>There are many types of statistical models, but this chapter will focus primarily on regression, a broad and extremely powerful class of models. A regression model relates a dependent variable to one or more independent variables. Dependent variables are sometimes called <strong>outcome variables</strong>, and independent variables are sometimes called <strong>predictor variables</strong>, <strong>covariates</strong>, or <strong>features</strong>. We will see that many common statistical estimators (like the sample mean) and methods of inference (like the <span class="math inline">\(t\)</span>-test) are actually simple regression models. Understanding this point will help you see many statistical methods as special cases of the same underlying framework, rather than as unrelated, ad hoc methods.</p>
<div id="regression-for-estimating-a-simple-treatment-effect" class="section level3" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Regression for estimating a simple treatment effect</h3>
<!-- : Treatment effect estimation in a two-group experiment -->
<!-- MBM to MCF: Estimation chapter actually foreshadows *Welch's* t-test because it describes unequal vars. Inference chapter generically refers to a t-test without making distinction. Let's discuss. -->
<p>Let‚Äôs start with one of these special cases, namely estimating a treatment effect, <span class="math inline">\(\beta\)</span>, in a two-group design. In Chapter <a href="5-estimation.html#estimation">5</a>, we solved this exact challenge for the tea-tasting experiment. We posited a model in which the milk-first ratings were normally distributed with mean <span class="math inline">\(\theta_{milkfirst} = \theta_{teafirst} + \beta_{teafirst}\)</span> and with standard deviation <span class="math inline">\(\sigma_{milkfirst}\)</span>.<label for="tufte-sn-93" class="margin-toggle sidenote-number">93</label><input type="checkbox" id="tufte-sn-93" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">93</span> Here‚Äôs a quick reminder that ‚Äúmodel‚Äù here is a way of saying ‚Äúset of assumptions about the data generating procedure.‚Äù So saying that some equation is a ‚Äúmodel‚Äù is the same as saying, we think this is where the data came from. We can ‚Äúturn the crank‚Äù ‚Äì generate data through the process that‚Äôs specified in those equations, e.g., pulling numbers from a normal distribution with mean <span class="math inline">\(\theta_{milkfirst}\)</span> and standard deviation <span class="math inline">\(\sigma_{milkfirst}\)</span>. In essence, we‚Äôre committing to the idea that this process will give us data that are substantively similar to the ones we have already.</span></p>
<p>Let‚Äôs now write that model as a regression model, that is, as a model relating each participant‚Äôs tea rating, <span class="math inline">\(Y_i\)</span>, given that participant‚Äôs treatment assignment, <span class="math inline">\(X_i\)</span>. <span class="math inline">\(X_i=0\)</span> represents the control (milk-first) group and <span class="math inline">\(X_i=1\)</span> represents the treatment (tea-first) group. Here, <span class="math inline">\(Y_i\)</span> is the dependent variable, and <span class="math inline">\(X_i\)</span> is the independent variable. <span class="math inline">\(i\)</span>s are an index variable for each of the participants. To make this concrete, you can see some sample tea-tasting data (N=24 for simplicity) below, with the index <span class="math inline">\(i\)</span>, the condition and its predictor <span class="math inline">\(X_i\)</span>, and the rating <span class="math inline">\(Y\)</span>.</p>
<div id="htmlwidget-916c5bc403e1f7b162d7" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-916c5bc403e1f7b162d7">{"x":{"filter":"none","caption":"<caption>Example tea tasting data in a regression format.<\/caption>","data":[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24],["milk first","milk first","milk first","milk first","milk first","milk first","milk first","milk first","milk first","milk first","milk first","milk first","tea first","tea first","tea first","tea first","tea first","tea first","tea first","tea first","tea first","tea first","tea first","tea first"],[0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1],[6,4,5,5,5,4,6,4,7,4,6,7,2,3,3,4,3,1,1,5,3,1,3,5]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>i<\/th>\n      <th>condition<\/th>\n      <th>X<\/th>\n      <th>rating (Y)<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[0,2,3]}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>Here‚Äôs the model:</p>
<!-- MBM to MCF: Let's discuss whether to switch to standard notation (e.g., betas) in this chapter or to stick with notation more like the previous chapter. I might lean toward using standard notation here and syncing earlier chapter with that notation. -->
<!-- MM: Not sure if align environment is supported in RMD. Might need to use this: https://stackoverflow.com/questions/47912278/align-environment-in-r-markdown-which-works-for-both-docx-and-pdf-output -->
<p><span class="math display">\[\begin{align}
\label{eq:ols_ttest}
Y_i &amp;= \theta_{milkfirst} + \beta_{teafirst} X_i + \epsilon_i \\
\end{align}\]</span></p>
<p>A model like this is called a <strong>linear regression of Y on X</strong>. <span class="math inline">\(\theta_{milkfirst} + \beta X_i\)</span> is called the <strong>linear predictor</strong>, and it describes the expected value of an individual‚Äôs tea rating, <span class="math inline">\(Y_i\)</span>, given that participant‚Äôs treatment group <span class="math inline">\(X_i\)</span> (the single independent variable in this model). That is, for a participant in the control group (<span class="math inline">\(X_i=0\)</span>), the linear predictor is just equal to <span class="math inline">\(\theta_{milkfirst}\)</span>, which is indeed the mean for the control group that we specified above. On the other hand, for a participant in the treatment group, the linear predictor is equal to <span class="math inline">\(\theta_{milkfirst} + \beta\)</span>, which is the mean for the treatment group that we specified. In regression jargon, <span class="math inline">\(\beta\)</span> is a <strong>regression coefficient</strong>, representing the association of the independent variable <span class="math inline">\(X_i\)</span> with the outcome <span class="math inline">\(Y_i\)</span>.</p>
<p>The term <span class="math inline">\(\epsilon_i\)</span> is the <strong>error term</strong>, referring to random variation of participants‚Äô ratings around the group mean.<label for="tufte-sn-94" class="margin-toggle sidenote-number">94</label><input type="checkbox" id="tufte-sn-94" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">94</span> Formally, we‚Äôd write <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span>. The tilde means ‚Äúis distributed as‚Äù, and what follows is a normal distribution with mean 0 and variance <span class="math inline">\(\sigma^2\)</span>.</span> Note that this is a very specific kind of ‚Äúerror‚Äù; it not to ‚Äúerror‚Äù due to bias, for example. Instead, you can think of the error terms as capturing the ‚Äúerror‚Äù that would be associated with predicting any given participant‚Äôs rating based on just the linear predictor. If you predicted a control group participant‚Äôs rating as <span class="math inline">\(\theta_{milkfirst}\)</span>, that would be a good guess ‚Äì but you still expect the participant‚Äôs rating to deviate somewhat from <span class="math inline">\(\theta_{milkfirst}\)</span> due to ‚Äúerror‚Äù. In our regression model, the linear predictor and error terms together say that participants‚Äô ratings scatter randomly (in fact, normally) around their group means with standard deviation <span class="math inline">\(\sigma\)</span>. And that is exactly the model we posited in Chapter <a href="5-estimation.html#estimation">5</a>.
<!-- [NOT TRUE BECAUSE PREVIOUS MODEL USED UNEQUAL VARIANCES.]  --></p>
<div class="figure"><span id="fig:models-ols-plot"></span>
<p class="caption marginnote shownote">
Figure 7.1: (left) Best-fitting regression coefficients for the tea-tasting experiment. (right) Much worse coefficients for the same data.
</p>
<img src="experimentology_files/figure-html/models-ols-plot-1.png" alt="(left) Best-fitting regression coefficients for the tea-tasting experiment. (right) Much worse coefficients for the same data." width="\linewidth"  />
</div>
<p>Now we have the model. How do we estimate the regression coefficients <span class="math inline">\(\theta_{milkfirst}\)</span> and <span class="math inline">\(\beta\)</span>? The usual method is called <strong>ordinary least squares (OLS)</strong>. Here‚Äôs the basic idea. For any given regression coefficient estimates <span class="math inline">\(\widehat{\theta}_{milkfirst}\)</span> and <span class="math inline">\(\widehat{\beta}\)</span>, we would obtain different <strong>predicted values</strong>, <span class="math inline">\(\widehat{Y}_i = \theta_{milkfirst} + \beta X_i\)</span> for each participant. Some regression coefficient estimates will yield better predictions than others. OLS estimation is designed to find the values of the regression coefficients that optimize these predictions, meaning that the predictions are as close as possible to participants‚Äô true outcomes, <span class="math inline">\(Y_i\)</span>.<label for="tufte-sn-95" class="margin-toggle sidenote-number">95</label><input type="checkbox" id="tufte-sn-95" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">95</span> Specifically, OLS minimizes squared error loss, in the sense that it will choose the regression coefficient estimates whose predictions minimize <span class="math inline">\(\sum_{i=1}^n \left( Y_i - \widehat{Y}_i\right)^2\)</span>, where <span class="math inline">\(n\)</span> is the sample size. A wonderful thing about OLS is that those optimal regression coefficients (generically termed <span class="math inline">\(\widehat{\mathbf{\beta}}\)</span>) turn out to have a very simple closed form: <span class="math inline">\(\widehat{\mathbf{\beta}} = \left( \mathbf{X}&#39;\mathbf{X} \right)^{-1} \mathbf{X}&#39;\mathbf{y}\)</span>. We are using more general notation here because there could be multiple independent variables. Therefore, <span class="math inline">\(\widehat{\mathbf{\beta}}\)</span> is a vector, <span class="math inline">\(\mathbf{X}\)</span> is a matrix of independent variables for each subject, and <span class="math inline">\(\mathbf{y}\)</span> is a vector of participants‚Äô outcomes. As more good news, the standard error for <span class="math inline">\(\widehat{\mathbf{\beta}}\)</span> has a similarly simple closed form.</span></p>
<p>Figure <a href="7-models.html#fig:models-ols-plot">7.1</a> gives a graphical illustration of the tea tasting data for each condition (the dots) along with the model predictions for each condition <span class="math inline">\(\theta_{milkfirst}\)</span> and <span class="math inline">\(\theta_{milkfirst} + \beta\)</span> (blue lines). The distance of each point to the predictions (the thing that OLS wants to minimize) is shown by the dotted lines. Another name for these errors is <strong>residuals</strong>: they are the ‚Äúresidual variation‚Äù that is not predicted by the model.</p>
<p>The left-hand plot shows the best coefficient values ‚Äì the ones that move the model as close as possible to each point, minimizing the total squared length of the dashed lines. The right-hand plot shows a substantially worse solution. The amazing thing about OLS is that it is a simple way to find the best solution for a wide range of useful models.</p>
<p>You‚Äôll notice that we aren‚Äôt talking much about <span class="math inline">\(p\)</span>-values in this chapter. Regression models can be used to produce <span class="math inline">\(p\)</span>-values on specific coefficients, representing inferences about the likelihood of a particular coefficient magnitude relative to some null hypothesis. You can also compute Bayes Factors on specific regression coefficients, or use Bayesian inference to fit these coefficients under some prior expectation about their distribution. We won‚Äôt talk much about this, or more generally how to fit the models we describe. As we said, we‚Äôre not going to give a full treatment of all the relevant statistical topics. Instead we want to help you begin thinking about making models of your data.</p>
</div>
<div id="adding-predictors" class="section level3" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Adding predictors</h3>
<p>The regression model we just wrote down is the same thing as the <span class="math inline">\(t\)</span>-test from Chapter <a href="6-inference.html#inference">6</a>. But the beauty of regression modeling is that much more complex estimation problems can also be written as regression models, essentially by extending the model we made above. For example, we might want to add another predictor variable, such as the age of the participant.<label for="tufte-sn-96" class="margin-toggle sidenote-number">96</label><input type="checkbox" id="tufte-sn-96" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">96</span> The ability to estimate multiple coefficients at once is a huge strength of regression modeling, so much so that sometimes people use the label <strong>multiple regression</strong> to denote that there is more than one predictor + coefficient pair.</span></p>
<p>Let‚Äôs add this new independent variable and a corresponding regression coefficient to our model:</p>
<p><span class="math display">\[\begin{align}
\label{eq:ols_one_covariate}
Y_i &amp;= \theta_{milkfirst} + \beta_{teafirst} X_{i1} + \theta_{age} X_{i2}  + \epsilon_i \\
\end{align}\]</span>
<!-- \epsilon_i &\sim N(0, \sigma^2) -->
<!-- WHY ARE WE USING THETA INSTEAD OF BETA FOR COEFS. LET'S FIX THIS. --></p>
<p>Now that we have multiple independent variables, we‚Äôve labeled them <span class="math inline">\(X_{group}\)</span> (treatment group) and <span class="math inline">\(X_{age}\)</span> for clarity.</p>
<p>To illustrate how to interpret the regression coefficients in this model, let‚Äôs use the linear predictor to compare the model‚Äôs predicted tea ratings for two hypothetical participants who are both in the treatment group: 20-year-old Alice and 21-year old Bob. Alice‚Äôs linear predictor tells us that her expected rating is <span class="math inline">\(\theta_{milkfirst} + \beta_{teafirst} + \theta_{age} \cdot 20\)</span>. In contrast, Bob‚Äôs linear predictor is <span class="math inline">\(\theta_{milkfirst} + \beta_{teafirst} + \theta_{age} \cdot 21\)</span>. We could therefore calculate the expected difference in ratings for 21-year-olds versus 20-year olds by subtracting Alice‚Äôs linear predictor from Bob‚Äôs, yielding just <span class="math inline">\(\theta_{age}\)</span>. How simple!</p>
<p>We would get the same result if Alice and Bob were instead 50 and 51 years old, respectively. This equivalence illustrates a key point about linear regression models in general: <em>the regression coefficient represents the expected difference in outcome when comparing participants who differ by 1 unit of the independent variable</em> (here, comparing participants who differ by 1 year of age). In ‚ÄúPractical modeling considerations‚Äù below, we discuss whether and when to ‚Äúcontrol for‚Äù additional variables (i.e., when to add them to your model).</p>
</div>
<div id="when-does-linear-regression-work" class="section level3" number="7.1.3">
<h3><span class="header-section-number">7.1.3</span> When does linear regression work?</h3>
<p>Linear regression modeling with OLS is an incredibly powerful technique for creating models to estimate the influence of multiple predictors on a single dependent variable. In fact, OLS is in a mathematical sense the <em>best</em> way to fit a linear model!<label for="tufte-sn-97" class="margin-toggle sidenote-number">97</label><input type="checkbox" id="tufte-sn-97" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">97</span> There is a precise sense in which OLS gives the <em>very best</em> predictions we could ever get from any model that posits linear relationships between the independent variables and the outcome. That is, you can come up with any other linear model you want, and yet if the assumptions of OLS are fulfilled, predictions from OLS will always be less noisy than those of your model. This is because of an elegant mathematical result called the Gauss-Markov Theorem.</span> But OLS only ‚Äúworks‚Äù ‚Äì in the sense of yielding good estimates ‚Äì if three big conditions are met.</p>
<ol style="list-style-type: decimal">
<li><strong>The predictor relationships being modeled must be linear.</strong> In our comparison of Alice‚Äôs and Bob‚Äôs expected outcomes based on their 1-year age difference, we were able to interpret the coefficient <span class="math inline">\(\theta_{age}\)</span> as the average difference in <span class="math inline">\(Y_i\)</span> when comparing participants who differ by 1 year of age, <em>regardless</em> of whether those ages are 20 vs.¬†21 or 50 vs.¬†51. But that‚Äôs not always true: plenty of things vary <strong>non-linearly</strong> with age ‚Äì for example, imagine growth in height over age! Linear regression will give bad answers in such cases.<label for="tufte-sn-98" class="margin-toggle sidenote-number">98</label><input type="checkbox" id="tufte-sn-98" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">98</span> One way to accommodate <strong>non-linearities</strong> is to modify the linear predictor to include polynomial terms, such as <span class="math inline">\(age^2\)</span>, which then allow us to fit a curve rather than just a straight line. It is always a good idea to use visualizations like scatter plots to look for possible problems with linearity.</span></li>
</ol>
<!-- [Suggested Figure. Panel A: Scatter plot showing a nice linear relationship with OLS fit superimposed. Panel B: Scatter plot showing a nonlinear relationship with OLS fit, OLS fit with x^2 term, and LOESS superimposed.] -->
<ol start="2" style="list-style-type: decimal">
<li><strong>Errors must be independent.</strong> In our example, observations in the regression model (i.e., rows in the dataset) were sampled independently: each participant was recruited independently to the study and each performed a single trial. On the other hand, suppose we have repeated-measures data in which we sample participants, and then obtained multiple measurements for each participant. Within each participant, measurements would likely be correlated (perhaps due to their general level of tea enjoyment). This correlation invalidates inferences from a model that does not include it. We‚Äôll discuss this problem in detail below.</li>
</ol>
<!-- Usually, outcome measurements within a participant will be correlated: if we measure 10 participants' blood pressures every day for a week, some participants will typically have high values whereas others will typically have low values, even though any given participant will also have some variation in their own measurements over time.[^models-3] To help identify such situations, it can be helpful to plot the distribution of **residuals** for each participant (or a random sample thereof) to see whether this distribution seems to differ across participants.  -->
<!-- [^models-3] To be specific, it is not the blood pressure values ($Y_i$) themselves that must be independent, but rather the error terms ($\epsilon$). The error terms are what is "left over" after accounting for systematic variation that is predicted by the independent variables. If, in principle, we managed to include in the linear predictor all variables that might explain individual differences in typical blood pressure (e.g., genetic factors, sex, age, etc.), then the errors would be independent even though the outcomes themselves are not. However, as a heuristic, considering possible sources of correlation in the outcomes themselves is often a reasonable proxy for thinking about the error terms. -->
<!-- [Suggested Figure. Violin plots, or similar, showing residuals for different subjects. The residuals exhibit non-independence.] -->
<ol start="3" style="list-style-type: decimal">
<li><strong>Errors must be normal and unrelated to the predictor.</strong> Imagine older people have very strong tea-ordering preferences while younger people do not. In that case, the models‚Äô error term would be more variable for older participants than younger ones. This issue is called <strong>heteroskedasticity</strong>. It is a good idea to plot each independent variable versus the residuals to see if the residuals are more variable for certain values of the independent variable than for others.</li>
</ol>
<p><!-- This is often the case for highly skewed outcome variables. For example, if we regressed participants' incomes on their years of education, we will find that higher education is associated with higher income. However, because income is highly right-skewed in many samples (some people have extremely high incomes), income will typically be more variable for individuals with more education than for those with less education. For this reason, if we predicted incomes for individuals with 16 years of education (i.e., they completed 4-year college and then stopped) and also for individuals with 8 years (i.e., they completed middle school and then stopped), the errors in the former predictions will probably be more variable than the errors in the latter predictions.  --></p>
<!-- [Suggested Figure. X vs. residuals, showing heteroskedasticity.] -->
<p>If any of these three conditions are violated, estimates and inferences from your model may be suspect.</p>
</div>
</div>
<div id="generalized-linear-models" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Generalized linear models</h2>
<p>So far we have considered continuous outcome measures, like tea ratings. What if we instead had a binary outcome, such as whether a participant liked or didn‚Äôt like the tea, or a count outcome, such as the number of cups a participant chose to drink? These and other non-continuous outcomes often violate the assumptions of OLS, in particular because they often induce heteroskedastic errors.</p>
<p>Binary outcomes inherently violate heteroskedasticity because the variance of a binary variable depends directly on the success probability. Errors will be more variable when the expected success probability closer to 0.50, and much less variable for when the expected success is probability is closer to 0 or 1.<label for="tufte-sn-99" class="margin-toggle sidenote-number">99</label><input type="checkbox" id="tufte-sn-99" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">99</span> Specifically, the variance of a binary variable with success probability <span class="math inline">\(p\)</span> is simply <span class="math inline">\(p(1-p)\)</span>, which is maximized at <span class="math inline">\(p=0.50\)</span>.</span> This heteroskedasticity in turn means that inferences from the model (e.g., <span class="math inline">\(p\)</span>-values) can be incorrect; sometimes just a little bit off but sometimes dramatically incorrect.<label for="tufte-sn-100" class="margin-toggle sidenote-number">100</label><input type="checkbox" id="tufte-sn-100" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">100</span> There‚Äôs a whole school of thought in economics that claims that it‚Äôs OK to use linear regression for binary outcomes. One commentator described this as ‚Äúwrong but super useful‚Äù because the coefficients are simple to interpret as probabilities. Our position is that the linear probability model is an approximation that can be useful but should only be deployed with care by researchers who have given some thought to its weaknesses.</span></p>
<p>Happily, <strong>generalized linear models</strong> (GLM) are regression models closely related to OLS that can handle non-continuous outcomes. These models are called ‚Äúgeneralized‚Äù because OLS is one of many members of this large class of models. To see the connection, let‚Äôs first write an OLS model more generally in terms of what it says about the expected value of the outcome, which we notate as <span class="math inline">\(E[Y_i]\)</span>:</p>
<p><span class="math display">\[\begin{align}
\label{eq:ols_general_form}
E[Y_i] &amp;= \beta_0 + \sum_{j=1}^p \beta_j X_j
\end{align}\]</span>
where <span class="math inline">\(p\)</span> is the number of independent variables, <span class="math inline">\(\beta_0\)</span> is the intercept, and <span class="math inline">\(\beta_j\)</span> is the regression coefficient for the <span class="math inline">\(j^{th}\)</span> independent variable. This equation is just a math-y way of saying that you predict from a regression model by adding up each of the predictors independently.</p>
<p>The linear predictor of a GLM (i.e., <span class="math inline">\(\beta_0 + \sum_{j=1}^p \beta_j X_j\)</span>) looks exactly the same as for OLS, but instead of modeling <span class="math inline">\(E[Y_i]\)</span>, a GLM models some <strong>transformation</strong>, <span class="math inline">\(g(.)\)</span>, of the expectation:</p>
<p><span class="math display">\[\begin{align}
\label{eq:glm_general_form}
g( E[Y_i] ) &amp;= \beta_0 + \sum_{j=1}^p \beta_j X_j 
\end{align}\]</span></p>
<p>GLMs involve transforming the <em>expectation</em> of the outcome, not the outcome itself! That is, we are not just taking the outcome variable in our dataset and transforming it before fitting an OLS model, but rather we are fitting a different model entirely, one that posits a fundamentally different relationship between the predictors and the expected outcomes. This transformation is called the <strong>link function</strong>. In other words, to fit different kinds of outcomes, all we need to do is construct a standard linear model and then just transform its output via the appropriate link function.</p>
<p>Perhaps the most common link function is the <strong>logit</strong> link, which is suitable for binary data. This link function looks like this:</p>
<p><span class="math display">\[g(x) = \log \left( \frac{x}{1 - x} \right)\]</span></p>
<p>The resulting model is called <strong>logistic regression</strong>. The term <span class="math inline">\(\frac{x}{1 - x}\)</span> is called an <strong>odds</strong> and represents the probability of an event occurring divided by the probability of its not occurring. Exponentiating the coefficients would yield <strong>odds ratios</strong>, which are the <em>multiplicative</em> increase in the odds of <span class="math inline">\(Y_i=1\)</span> that is associated with a one-unit increase in the relevant predictor variable.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:models-logistic-ex"></span>
<img src="experimentology_files/figure-html/models-logistic-ex-1.png" alt="An example of how logistic regression transforms a change in the predictor x into a change in the outcome y. The same change results in a large difference in outcome in the middle of the logistic curve (blue) vs. a small outcome at the top (red) or bottom." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 7.2: An example of how logistic regression transforms a change in the predictor x into a change in the outcome y. The same change results in a large difference in outcome in the middle of the logistic curve (blue) vs.¬†a small outcome at the top (red) or bottom.<!--</p>-->
<!--</div>--></span>
</p>
<p>Figure <a href="7-models.html#fig:models-logistic-ex">7.2</a> shows the way that a logistic regression model transforms a predictor (<span class="math inline">\(x\)</span>) into an outcome probability that is bounded at 0 and 1. Critically, although the predictor is still linear, the logit link means that the same change in <span class="math inline">\(x\)</span> can result in a different change in <span class="math inline">\(y\)</span> depending on where you are on the <span class="math inline">\(x\)</span> scale. In this example, if you are in the middle of the predictor range, a one unit change in <span class="math inline">\(x\)</span> results in a 0.24 change in probability (blue). At a higher value, the change is much smaller (0.02). Notice how this is different from the linear regression model above, where the same change in age always resulted in the same change in preference!</p>
<p>We have only scratched the surface of GLMs here. First, there are many different link functions that are suitable for different outcome types. And second, GLMs differ from OLS not only in their link functions, but also in how they handle the error terms. Our broader goal in this chapter is to show you how regression models are <em>models of data</em>. In that context, GLMs use link functions as a way to make models that generate many different times types of outcome data.<label for="tufte-sn-101" class="margin-toggle sidenote-number">101</label><input type="checkbox" id="tufte-sn-101" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">101</span> We sometimes think of linear models as a set of tinker toys you can snap together to stack up a set of predictors. In that context, link functions are an extra ‚Äúattachment‚Äù that you can snap onto your linear model to make it generate a different response type.</span></p>
</div>
<div id="accommodating-clustering-in-our-models" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Accommodating clustering in our models</h2>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:models-crossed-rm">Table 7.1: </span>Outcome data <span class="math inline">\(y\)</span> with indices indicating both participant and stimulus.</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Stimulus1</th>
<th align="left">Stimulus2</th>
<th align="left">Stimulus3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Participant1</td>
<td align="left">y<sub>1,1</sub></td>
<td align="left">y<sub>1,2</sub></td>
<td align="left">y<sub>1,3</sub></td>
</tr>
<tr class="even">
<td align="left">Participant2</td>
<td align="left">y<sub>2,1</sub></td>
<td align="left">y<sub>2,2</sub></td>
<td align="left">y<sub>2,3</sub></td>
</tr>
<tr class="odd">
<td align="left">Participant3</td>
<td align="left">y<sub>3,1</sub></td>
<td align="left">y<sub>3,2</sub></td>
<td align="left">y<sub>3,3</sub></td>
</tr>
</tbody>
</table>
<p>Experimental data often contain multiple measurements for each participant (so-called <strong>repeated measures</strong>). In addition, as we discussed in our case study, these measurements are often based on a sample of stimulus items (which then each have multiple measures as well). Table <a href="7-models.html#tab:models-crossed-rm">7.1</a> gives an example of what the outcome data <span class="math inline">\(y\)</span> might look like in this case. This clustering is problematic for OLS models, because the error terms for each datapoint are not independent.</p>
<p>Non-independence of datapoints may seem at first glance like a small issue, but it present a deep problem for making inferences. Take the tea-tasting data we looked at above, where we had 24 observations in each condition. If we fit an OLS model, we observe a highly significant tea-first effect. Here are the inferential statistics for that coefficient: <span class="math inline">\(t(22) = -4.63\)</span>, <span class="math inline">\(p &lt; .001\)</span>. Based on what we talked about in the previous chapter, it seems like we‚Äôd be licensed in rejecting the null hypothesis that this effect is due to sampling variation and interpret this instead as evidence for a generalizable difference in tea preference in our sampled population.</p>
<p>But suppose we told you that all of those 48 total observations (24 in each condition) were from one individual named George. That would change the picture considerably. Now we‚Äôd have no idea whether the big effect we observed reflected a difference in the population, but we would have a very good sense of what George‚Äôs preference is!<label for="tufte-sn-102" class="margin-toggle sidenote-number">102</label><input type="checkbox" id="tufte-sn-102" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">102</span> We discuss the strengths and weaknesses of repeated-measures designs like this in Chapter <a href="9-design.html#design">9</a> and the statistical tradeoffs of having many people with a small number of observations vs.¬†a small number of people with many observations in Chapter <a href="10-sampling.html#sampling">10</a>.</span> Our OLS model would be wrong now because all of the error terms would be highly correlated ‚Äì they would all reflect George‚Äôs preferences.</p>
</div>
<div id="linear-mixed-effects-models" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Linear mixed effects models</h2>
<p>How can we make models that deal with clustered data? There are a number of widely-used approaches for solving this problem including <strong>linear mixed effects models</strong>, <strong>generalized estimating equations</strong>, and <strong>clustered standard errors</strong> (often used in economics). Here we will illustrate how the problem gets solved in linear mixed models, which are an extension of OLS models that are fast becoming a standard in many areas of psychology <span class="citation">(<a href="#ref-bates2014" role="doc-biblioref">Bates et al., 2014</a>)</span>.</p>
</div>
<div id="modeling-random-variation-in-clusters" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Modeling random variation in clusters</h2>
<p>In linear mixed effects models, we modify the linear predictor itself to model differences across clusters. Instead of just measuring George‚Äôs preferences, suppose we modified the original tea-tasting experiment (without the age covariate) to collect ten ratings from each participant: five milk-first and five tea-first. We define the model the same way as we did before, with some minor differences:</p>
<p><span class="math display">\[
Y_{it} = \theta_{milkfirst} + \beta_{teafirst} X_{it} + \gamma_i + \epsilon_{it} 
\]</span></p>
<p>where <span class="math inline">\(Y_{it}\)</span> is participant <span class="math inline">\(i\)</span>‚Äôs rating in trial <span class="math inline">\(t\)</span> and <span class="math inline">\(X_{it}\)</span> is the participant‚Äôs assigned treatment in trial <span class="math inline">\(t\)</span> (i.e., milk-first or tea-first).</p>
<p>If you compare this equation to the OLS equation above, you will notice that we added two things. First, we‚Äôve added subscripts for trials that distinguish trials from participants. But the big one is that we added <span class="math inline">\(\gamma_i\)</span>, a separate intercept value for each participant. We call this a <strong>random intercept</strong> because it varies across participants (who are randomly selected from the population).<label for="tufte-sn-103" class="margin-toggle sidenote-number">103</label><input type="checkbox" id="tufte-sn-103" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">103</span> Formally, we‚Äôd notate this random variation by saying that <span class="math display">\[\gamma_i \sim N(0, \tau^2)\]</span>, in other words, that participants‚Äô random intercepts are sampled with a normal distribution with standard deviation <span class="math inline">\(\tau\)</span>.</span></p>
<p>The random intercept means that we have assumed that each participant has their own typical ‚Äúbaseline‚Äù tea rating ‚Äì some participants generally like tea more than others ‚Äì and these baseline ratings are normally distributed across participants. Thus, ratings are correlated within participants because ratings cluster around each participant‚Äôs <em>unique</em> baseline tea rating.</p>
<p>Following the same logic, we could fit random intercepts for different stimulus items. The addition of these <strong>crossed</strong> random intercepts of participants and items would begin to address the challenge posed by <span class="citation">Clark (<a href="#ref-clark1973" role="doc-biblioref">1973</a>)</span> in our case study above. We model participants as having normally distributed variation; we can model stimulus variation the same way, with each stimulus item assumed to produce a particular average level of performance sampled from a normally distributed population.</p>
<div id="random-slopes-and-the-challenges-of-mixed-effects-modelss" class="section level3" number="7.5.1">
<h3><span class="header-section-number">7.5.1</span> Random slopes and the challenges of mixed effects modelss</h3>
<p>Linear mixed effects models can be further extended to model clustering of the independent variables‚Äô effects across subjects. To do so, we can introduce <strong>random slopes</strong> (<span class="math inline">\(\delta_i\)</span>) to the model, which are multiplied by the condition variable <span class="math inline">\(X\)</span> and represent differences across participants in the effect of tea-tasting:</p>
<p><span class="math display">\[Y_i = \theta_{milkfirst} + \beta_{teafirst} X_{it} + \gamma_i + \delta_{i} X_{it} + \epsilon_{it}\]</span>
Just like the random intercepts, these random slopes will be assumed to vary as a normal distribution.<label for="tufte-sn-104" class="margin-toggle sidenote-number">104</label><input type="checkbox" id="tufte-sn-104" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">104</span> These random slopes and intercepts can be fit independently, or assumed to be correlated with one another.</span></p>
<p>This model now describes random variation in both overall how much someone likes tea <em>and</em> how strong their preference is. Both of these likely do vary in the population and so it seems like a good thing to put these in your model. Indeed under some circumstances, adding random slopes is argued to be very important for making appropriate inferences <span class="citation">(<a href="#ref-barr2013" role="doc-biblioref">Barr et al., 2013</a>)</span>. On the other hand, the model is much more complicated. When we had a simple OLS model above, we had only two parameters to fit (<span class="math inline">\(\theta_{milkfirst}\)</span> and <span class="math inline">\(\beta_teafirst\)</span>) but now we have those two plus 48: 24 different individual participant intercepts and 24 participant slopes. This complexity can lead to problems in fitting the models, especially with very small datasets (where these parameters are not very well-constrained by the data) or very large datasets (where computing all these parameters can be tricky).<label for="tufte-sn-105" class="margin-toggle sidenote-number">105</label><input type="checkbox" id="tufte-sn-105" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">105</span> Many R users may be familiar with the widely-used <code>lme4</code> package for fitting mixed effects models using frequentist tools related to maximum likelihood. Such models can also be fit using Bayesian inference with the <code>brms</code> package, which provides many powerful methods for specifying complex models.</span></p>
<p>More generally, linear mixed effects models are very powerful and they have become quite common in psychology. But they do have significant limitations. As we discussed, they can be tricky to fit in standard software packages. Further, the inferences we make using these models relies on our ability to specify the structure of the random effects correctly.<label for="tufte-sn-106" class="margin-toggle sidenote-number">106</label><input type="checkbox" id="tufte-sn-106" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">106</span> One particularly problematic situation is when the correlation structure of the errors is mis-specified, for example if observations within a participant are more correlated for participants in the treatment group than in the control group; in such cases, mixed model estimates can be substantially biased <span class="citation">(<a href="#ref-bie2021fitting" role="doc-biblioref">Bie et al., 2021</a>)</span>.</span> If we write an incorrect model our inferences are wrong, but it is sometimes difficult to know how to check whether your model is reasonable, especially with a small number of clusters or observations.</p>
<!-- Even if the model is correctly specified, LMM can severely underestimate standard errors (e.g., yielding too-small $p$-values) if the number of clusters is not large. -->
<!-- ^[ How many clusters are enough for LMM to behave well? This is an active area of research, and the answer will depend on the number of observations in each cluster and the model structure. However, some empirical findings suggest that, at least for certain models, LMM can considerably underestimate standard errors even with, for example, 20 clusters each with 100 observations [@bie2021fitting]. -->
<!-- ] -->
<!-- ### An alternative approach: Generalized estimating equations -->
<!-- A second class of methods that helps resolve these issues is **generalized estimating equations** (GEE). In this approach, we leave the linear predictor alone. We do not add random intercepts or slopes, nor do we assume anything about the distribution of the errors (i.e., we no longer assume that they are normal, independent, and homoskedastic). We instead provide the model with an initial "guess" about how we think the errors might be related to one another; for example, in a repeated-measures experiment, we might guess that the errors are exchangeable, meaning that they are correlated to the same degree within each participant but are uncorrelated across participants. Instead of *assuming* that our guess is correct, as does LMM, GEE estimates the correlation structure of the errors empirically, using our guess as a starting point, and it uses this correlation structure to arrive at point estimates and inference for the regression coefficients. Remarkably, as the number of clusters and observations become very large, GEE will *always* provide unbiased point estimates and valid inference, *even if* our guess about the correlation structure was bad. Additionally, with simple finite-sample corrections [@mancl2001covariance], GEE seems to provide valid inference at smaller numbers of clusters than does LMM. The price paid for these nice safeguards against model misspecification is that, in principle, GEE will typically have less statistical power than LMM *if* the LMM is in fact correctly specified, but the difference may be surprisingly slight in practice [@bie2021fitting]. For these reasons, we tend to prefer GEE with finite-sample corrections over LMM as the default model for clustered data. In general, we tend to favor LMM over GEE only when the number of observations and clusters are quite large, and when careful diagnostics also indicate that distributional assumptions are fulfilled. -->
<!-- [MBM to MCF: Above is what I usually say as my default recommendation for GEE vs. LMM, but happy to discuss and modify if you don't agree. I also haven't touched the marginal vs. conditional interpretation issue since this has been about LMM and not GLMM.] MCF TO MBM: Sorry - I think it's too much detail because we can't really present these methods. I prefer to mention them and point people to them, since our exposition fits nicely with LMMs. -->
</div>
</div>
<div id="how-do-you-use-models-to-analyze-data" class="section level2" number="7.6">
<h2><span class="header-section-number">7.6</span> How do you use models to analyze data?</h2>
<p>In the first part of this chapter, we‚Äôve described a suite of regression-based techniques ‚Äì standard OLS, the generalized linear model, and linear mixed effects models ‚Äì that can be used to model the data resulting from randomized experiments (as well as many other kinds of data). Further, when used appropriately, the regression-based estimates give an unbiased estimate of a causal effect of interest, the estimation of which is our main goal in doing an experiment. The advantage of regression models over the simpler estimation and inference methods we described in the prior two chapters is that these models can more effectively take into account a range of different kinds of variation including covariates, multiple manipulations, and clustered structure.</p>
<p>But ‚Äì practically speaking ‚Äì how should go you about building a model for your experiment? What should you put in and what should you leave out? There are many ways to use models to explore datasets, but in this section we will try to sketch a default approach for the use of models to estimate causal effects in experiments in the most straightforward way. Think of this as a starting point or default. We‚Äôll begin this section by giving a set of rules of thumb, then discuss a worked example. Our final subsection will deal with the complex issue of covariate adjustment and when you should include covariates in your model.</p>
<div id="modeling-rules-of-thumb" class="section level3" number="7.6.1">
<h3><span class="header-section-number">7.6.1</span> Modeling rules of thumb</h3>
<p>Our approach to building statistical modeling is to start with what we call the ‚Äúdefault model.‚Äù</p>
<p>The ‚Äúdefault‚Äù model of an experiment should include the full design of the experiment, and nothing else. If you are manipulating a variable, include it in your model. If you are manipulating two, include them both and their interaction. If your design includes repeated measurements for participants, include a random effect of participant; if it includes experimental items for which repeated measurements are made, include a random effect of stimulus. But
don‚Äôt include lots of other stuff in the default model. You are doing a randomized experiment, and the strength of randomized experiments is that you don‚Äôt have to worry about confounding based on the population (see Chapter <a href="1-experiments.html#experiments">1</a>). So don‚Äôt put a lot of covariates in your default model ‚Äì usually don‚Äôt put in any!<label for="tufte-sn-107" class="margin-toggle sidenote-number">107</label><input type="checkbox" id="tufte-sn-107" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">107</span> One corollary to having this kind of default perspective on data analysis: When you see an analysis that deviates substantially from the default, these deviations should provoke some questions. If someone drops a manipulation from their analysis, adds a covariate or two, or fails to control for some clustering in the data, did they deviate because of different norms in their sub-field, or was there some other rationale? This line of reasoning sometimes leads to questions about the extent to which particular analytic decisions are post-hoc and driven by the data (in other words, <span class="math inline">\(p\)</span>-hacked).</span></p>
<p>This default model then represents a simple summary of your experimental results. Its coefficients can be interpreted as estimates of the effects of interest, and it can be used as the basis for inferences about the relation of the experimental effect to the population using either frequentist or Bayesian tools. Here‚Äôs a bit more guidance about this default modeling strategy.</p>
<ol style="list-style-type: decimal">
<li><strong>Preregister your model</strong>. If you adjust your analysis after you see your data, you risk <span class="math inline">\(p\)</span>-hacking ‚Äì choosing an analysis that inflates the estimate of your effect of interest. As we discussed in Chapter <a href="3-replication.html#replication">3</a> and as we will discuss in more detail in Chapter <a href="11-prereg.html#prereg">11</a>, one important strategy for avoiding this problem is to <strong>preregister</strong> your analysis.<label for="tufte-sn-108" class="margin-toggle sidenote-number">108</label><input type="checkbox" id="tufte-sn-108" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">108</span> A side benefit of preregistration is it makes you think through whether your experimental design is appropriate ‚Äì that is, is there an analysis that estimates the effect you want from the data you intend to collect?</span></li>
</ol>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:models-ex-viz"></span>
<img src="experimentology_files/figure-html/models-ex-viz-1.png" alt="Raw data, means, and confidence intervals for the tea-tasting experiment." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 7.3: Raw data, means, and confidence intervals for the tea-tasting experiment.<!--</p>-->
<!--</div>--></span>
</p>
<ol start="2" style="list-style-type: decimal">
<li><p><strong>Visualize the model against the data</strong>. As we‚Äôll discuss in Chapter <a href="14-viz.html#viz">14</a>, the ‚Äúdefault model‚Äù for an experiment should go alongside a ‚Äúdefault visualization‚Äù that similarly reflects the full design structure of the experiment and the primary clusters. In visualizing data like those from our tea-tasting experiment, show individual participants‚Äô average ratings (as in Figure <a href="7-models.html#fig:models-ex-viz">7.3</a>). The best way to check whether a model fits your data is then to plot it on top of those data. Sometimes this combination of model and data can be as simple as a scatter plot with a regression line. But seeing the model plotted alongside the data can often reveal a mismatch between the two. A model that does not describe the data very well is not a good source of generalizable inferences!</p></li>
<li><p><strong>Interpret the predictions of the model</strong>. Once you have a model, don‚Äôt just read off the <span class="math inline">\(p\)</span>-values for your coefficients of interest. Walk through the each coefficient, considering how it relates to your outcome variable. For a simple two group design like we‚Äôve been considering, the condition coefficient is the estimate of the causal effect that you intended to measure! Consider its sign, its magnitude, and its precision (standard error).</p></li>
</ol>
</div>
<div id="a-worked-example" class="section level3" number="7.6.2">
<h3><span class="header-section-number">7.6.2</span> A worked example</h3>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:models-sgf-stim"></span>
<img src="images/models/sgf.png" alt="Example stimulus materials from @stiller2015." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 7.4: Example stimulus materials from <span class="citation">Stiller et al. (<a href="#ref-stiller2015" role="doc-biblioref">2015</a>)</span>.<!--</p>-->
<!--</div>--></span>
</p>
<p>All this advice may seem abstract, so let‚Äôs put it into practice on a simple example. For a change, let‚Äôs look at an experiment that‚Äôs not about tea tasting. Here we‚Äôll consider data from an experiment testing preschool children‚Äôs language comprehension [<span class="citation">Stiller et al. (<a href="#ref-stiller2015" role="doc-biblioref">2015</a>)</span>; we also use these data in Appendix <a href="C-tidyverse.html#tidyverse">C</a>]. In this experiment, 2‚Äì5 year old children saw displays like the one in Figure <a href="7-models.html#fig:models-sgf-stim">7.4</a>. In the experimental condition, a puppet said ‚ÄúMy friend has glasses! Which one is my friend?‚Äù The goal was to measure how many children made the inference that the puppets friend was the face with glasses and <em>no</em> hat. To estimate this effect, participants were randomly assigned to either the experimental condition or to a control condition in which the puppet had eaten too much peanut butter and couldn‚Äôt talk, but they still had to guess which face was his friend. Data from this experiment consisted of 588 total observations from 147 children, with four different experimental items ‚Äì faces, houses, beds, and plates of pasta ‚Äì each presented to each child. The primary hypothesis of this experiment was that an inference effect would provide support for the idea that preschool children could make pragmatic inferences.</p>
<p>This experimental design looks a lot like some versions of our tea-tasting experiment. We have one primary condition manipulation, presented between-participants so that some participants are in the experimental condition and others are in the control condition. Our measurements are repeated within participants across different experimental items. Finally, we have one important, pre-planned covariate: children‚Äôs age.<label for="tufte-sn-109" class="margin-toggle sidenote-number">109</label><input type="checkbox" id="tufte-sn-109" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">109</span> Our sampling plan for this experiment was actually <strong>stratified</strong> across age, meaning that we intentionally recruited the same number of participants for each one-year age group ‚Äì because we anticipated that age was highly correlated with children‚Äôs ability to succeed in this experiment. We‚Äôll present this kind of sampling in more detail in Chapter <a href="10-sampling.html#sampling">10</a>.</span> Experimental data are plotted in Figure <a href="7-models.html#fig:models-sgf-plot">7.5</a>.</p>
<div class="figure"><span id="fig:models-sgf-plot"></span>
<p class="caption marginnote shownote">
Figure 7.5: Data for <span class="citation">Stiller et al. (<a href="#ref-stiller2015" role="doc-biblioref">2015</a>)</span>. Each point shows a single participant‚Äôs proportion correct trials plotted by age group, jittered slightly to avoid overplotting. Larger points and associated confidence intervals show mean and 95% confidence intervals for each condition.
</p>
<img src="experimentology_files/figure-html/models-sgf-plot-1.png" alt="Data for @stiller2015. Each point shows a single participant's proportion correct trials plotted by age group, jittered slightly to avoid overplotting. Larger points and associated confidence intervals show mean and 95\% confidence intervals for each condition." width="\linewidth"  />
</div>
<p>How should we go about making our default model for this dataset?<label for="tufte-sn-110" class="margin-toggle sidenote-number">110</label><input type="checkbox" id="tufte-sn-110" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">110</span> This experiment was not preregistered, but the paper includes a separate replication dataset with the same analysis.</span> We simply include each of these design factors in a mixed effects model; we use a logistic variant of the linear mixed effects model (a <strong>generalized linear mixed effects model</strong>) because we would like to predict correct performance on each trial, which is a binary variable. So that gives us an effect of condition and age as a covariate. We further add an interaction between condition and age in case the condition effect varies meaningfully across groups. Finally, we add random effects of participant and experimental item.<label for="tufte-sn-111" class="margin-toggle sidenote-number">111</label><input type="checkbox" id="tufte-sn-111" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">111</span> There‚Äôs lots of debate in the literature about the best random effect structure for mixed effects models. This is a very tricky and technical subject. In brief, some folks argue for so-called <strong>maximal</strong> models, in which you include every random effect that is justified by the design <span class="citation">(<a href="#ref-barr2013" role="doc-biblioref">Barr et al., 2013</a>)</span>. In our case, that would mean including random slopes of condition for each experimental item. The problem is that these models can get very complex, and can be very hard to fit using standard software. We won‚Äôt weigh in on this topic, but as you start to use these models on more complex experimental designs, it might be worth reading up.</span></p>
<p>The resulting model looks like this:</p>
<p><span class="math display">\[
logit(Y_{it}) = \beta_{cntl} + \beta_{age} * age_y + \beta_{exp} * X_i + \beta_{age*exp} * age_i * X_i + \gamma_i + \delta_t + \epsilon_{it}
\]</span></p>
<p>Let‚Äôs break this down left to right:</p>
<ul>
<li><span class="math inline">\(logit(Y_{it})\)</span> says that we are predicting a logistic function of <span class="math inline">\(Y_it\)</span>, whether child <span class="math inline">\(i\)</span> was correct on trial <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(\beta_{cntl}\)</span> is the <strong>intercept</strong>, our estimate of the average log odds of correct responses for participants in the control condition.</li>
<li><span class="math inline">\(\beta_{age} * age_y\)</span> is the age predictor. <span class="math inline">\(\beta_{age}\)</span> represents change in log odds associated with a year more or less of age, and <span class="math inline">\(age_i\)</span> is the age for each participant.<label for="tufte-sn-112" class="margin-toggle sidenote-number">112</label><input type="checkbox" id="tufte-sn-112" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">112</span> We have <strong>centered</strong> our age predictor in this example so that all estimates from our model are for the average age of our participants. Centering is a good practice for modeling continuous predictors because it increases the interpretability of other parts of the model. For example, because age is centered in this model, the intercept <span class="math inline">\(\beta_{cntl}\)</span> can be interpreted as the predicted odds of a correct trial for a participant at the average age.</span></li>
<li><span class="math inline">\(\beta_{exp} * X_i\)</span> is the condition predictor. <span class="math inline">\(\beta_{exp}\)</span> represents the change in log odds associated with being in the experimental condition (the causal effect of interest!), and <span class="math inline">\(X_i\)</span> an indicator variable that is 1 if child <span class="math inline">\(i\)</span> is in the experimental condition and 0 for the control conditions. Multiplying <span class="math inline">\(\beta_{exp}\)</span> by this indicator means that the predictor has the value 0 for participants in the control condition and <span class="math inline">\(\beta_{exp}\)</span> for those in the experimental condition.</li>
<li><span class="math inline">\(\beta_{age*exp} * age_i * X_i\)</span> is the interaction between age and experimental condition. <span class="math inline">\(\beta_{age*exp}\)</span> represents the change in log odds associated with being one year older <em>and</em> in the experimental condition. This term is multiplied by both each child‚Äôs age <em>and</em> the condition indicator <span class="math inline">\(X_i\)</span>.</li>
<li><span class="math inline">\(\gamma_i\)</span> is the random intercept for individual participants, capturing individual variation in the odds of success across trials.</li>
<li><span class="math inline">\(\delta_t\)</span> is the random intercept for individual experimental items, capturing variation in the odds of success across the four different items.</li>
</ul>
<caption>
<span id="tab:models-sgf-model-print">Table 7.2: </span>
</caption>
<div custom-style="Table Caption">
<em>Estimated effects for our generalized linear mixed effects model on data from Stiller et al.¬†(2015).</em>
</div>
<table>
<thead>
<tr class="header">
<th align="left">Term</th>
<th align="left"><span class="math inline">\(\hat{\beta}\)</span></th>
<th align="left">95% CI</th>
<th align="left"><span class="math inline">\(z\)</span></th>
<th align="left"><span class="math inline">\(p\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Control condition</td>
<td align="left">-1.46</td>
<td align="left">[-1.88, -1.04]</td>
<td align="left">-6.76</td>
<td align="left">&lt; .001</td>
</tr>
<tr class="even">
<td align="left">Age (years)</td>
<td align="left">-0.38</td>
<td align="left">[-0.75, -0.01]</td>
<td align="left">-1.99</td>
<td align="left">.046</td>
</tr>
<tr class="odd">
<td align="left">Experimental condition</td>
<td align="left">2.26</td>
<td align="left">[1.82, 2.70]</td>
<td align="left">10.07</td>
<td align="left">&lt; .001</td>
</tr>
<tr class="even">
<td align="left">Age (years) * Experimental condition</td>
<td align="left">0.92</td>
<td align="left">[0.42, 1.43]</td>
<td align="left">3.60</td>
<td align="left">&lt; .001</td>
</tr>
</tbody>
</table>
<p>Let‚Äôs estimate this model and see how it looks. We‚Äôll focus here on interpretation of the so-called <strong>fixed effects</strong> (the main predictors), as opposed to the participant and item random effects.<label for="tufte-sn-113" class="margin-toggle sidenote-number">113</label><input type="checkbox" id="tufte-sn-113" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">113</span> Participant means are estimated to have a standard deviation of 0.23 (in log odds) while items have a standard deviation of <code>r</code>round(as.data.frame(lme4::VarCorr(mod))$sdcor[2],2)`. These indicate that both of our random effects capture meaningful variation.</span> Table <a href="7-models.html#tab:models-sgf-model-print">7.2</a> shows the coefficients. Again, let‚Äôs walk through each.</p>
<ul>
<li><p>The control condition estimate (intercept) is <span class="math inline">\(\hat{\beta} = -1.46\)</span>, 95% CI <span class="math inline">\([-1.88, -1.04]\)</span>, <span class="math inline">\(z = -6.76\)</span>, <span class="math inline">\(p &lt; .001\)</span>. This estimate reflects that the log odds of a correct response for an average-age participant in the control condition is -1.46, which corresponds to a probability of 0.19. If we look at Figure <a href="7-models.html#fig:models-sgf-plot">7.5</a>, that estimate makes sense: 0.19 seems close to the average for the control condition.</p></li>
<li><p>The age effect estimate is <span class="math inline">\(\hat{\beta} = -0.38\)</span>, 95% CI <span class="math inline">\([-0.75, -0.01]\)</span>, <span class="math inline">\(z = -1.99\)</span>, <span class="math inline">\(p = .046\)</span>. There is a slight decrease in the probability of a correct response for older children in the control condition. Again, looking at Figure <a href="7-models.html#fig:models-sgf-plot">7.5</a>, this estimate is interpretable: we see a small decline in the probability of a correct response for the oldest age group.</p></li>
<li><p>The key experimental condition estimate then is <span class="math inline">\(\hat{\beta} = 2.26\)</span>, 95% CI <span class="math inline">\([1.82, 2.70]\)</span>, <span class="math inline">\(z = 10.07\)</span>, <span class="math inline">\(p &lt; .001\)</span>. This estimate means that the log odds of a correct response for an average-age participant in the experimental condition is the sum of the estimates for the control (intercept) and the experimental conditions: -1.46 + 2.26, which corresponds to a probability of 0.69. Again, grounding our interpretation in Figure <a href="7-models.html#fig:models-sgf-plot">7.5</a>, this estimate corresponds to the average value for the experimental condition.</p></li>
<li><p>Finally, the interaction of age and condition is <span class="math inline">\(\hat{\beta} = 0.92\)</span>, 95% CI <span class="math inline">\([0.42, 1.43]\)</span>, <span class="math inline">\(z = 3.60\)</span>, <span class="math inline">\(p &lt; .001\)</span>. This positive coefficient reflects that with every year of age, the difference between control and experimental conditions grows.</p></li>
</ul>
<p>In sum, this model suggests that there was a substantial difference in performance between experimental and control conditions, in turn supporting the hypothesis that children in the sampled age group can perform pragmatic inferences. This example illustrates the ‚Äúdefault model‚Äù framework that we recommend ‚Äì the idea that a single regression model corresponding to the design of the experiment can yield an interpretable estimate of the causal effect of interest, even in the presence of several other sources of variation.</p>
</div>
<div id="when-does-it-makes-sense-to-include-covariates-in-a-model" class="section level3" number="7.6.3">
<h3><span class="header-section-number">7.6.3</span> When does it makes sense to include covariates in a model?</h3>
<p>Let‚Äôs come back to one piece of advice that we gave above about making a ‚Äúdefault‚Äù model of an experiment: not including covariates. This advice can seem surprising. Many demographic factors are of interest to psychologists and other behavioral scientists, and in observational studies these factors will almost always be related to important life outcomes. So why not put them into our experimental models? After all, we did include age in our worked example above!</p>
<p>Well, if you have one or at most a small handful of covariates that you believe are meaningfully related to the outcome, you <em>can</em> plan in advance to put them in your model. If you think that your effect is likely to be <strong>moderated</strong> a specific demographic characteristic ‚Äì as we did with age and pragmatic inference in our example above ‚Äì then this inclusion can be quite useful.</p>
<p>Further, including covariates can increase the precision of your estimates by reducing ‚Äúnoise‚Äù in your outcome, and if you hypothesize that they interact. What‚Äôs surprising though is how <em>little</em> this adjustment does to increase your overall precision unless the correlation between covariate and outcome is very big. Figure <a href="7-models.html#fig:models-precision">7.6</a> shows this relationship in a simple simulation. Only when the correlation between covariate and outcome (e.g., age and tea rating) is greater than .6 ‚Äì .8 does this adjustment really help.<label for="tufte-sn-114" class="margin-toggle sidenote-number">114</label><input type="checkbox" id="tufte-sn-114" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">114</span> A relationship that strong is very unusual for most experiments in psychology, not least because our outcome measures are very unlikely to even be reliable enough to correlate with <em>themselves</em> at <span class="math inline">\(r &gt; .8\)</span> let alone with another variable; see Chapter <a href="8-measurement.html#measurement">8</a> for more details on this point.</span></p>
<div class="figure"><span id="fig:models-precision"></span>
<p class="caption marginnote shownote">
Figure 7.6: Decreases in estimation error (root mean squared error) due to adjusting for covariates, plotted by the N participants in each group and the correlation between the covariate and the outcome.
</p>
<img src="experimentology_files/figure-html/models-precision-1.png" alt="Decreases in estimation error (root mean squared error) due to adjusting for covariates, plotted by the N participants in each group and the correlation between the covariate and the outcome." width="\linewidth"  />
</div>
<p>That said, there are quite a few reasons not to include covariates ‚Äì motivating our recommendation to skip them in your default model unless you have very strong expectations for either A) a correlation with the outcome or B) a strong moderation relationship.</p>
<p>The first reason is simply because we don‚Äôt need to. Because randomization cuts causal links, our experimental estimate is an unbiased estimate of the causal effect of interest. We are guaranteed that, in the limit of many different experiments, even though people with different ages will be in the different tea tasting conditions, this source of variation will be averaged out.</p>
<p>The second reason is that, on average, including more covariates into models actually (slightly) decreases the probability that the model can detect a true effect. Just by chance covariates can ‚Äúsoak up‚Äù variation in the outcome, leaving less to be accounted for by the true effect!</p>
<p>The third reason is that you can actually compromise your causal inference by including some covariates, particularly those that are collected <em>after</em> randomization. The logic of randomization is that you cut all causal links between features of the sample and the condition manipulation. But you can ‚Äúuncut‚Äù these links by accident by adding variables into your model that are related to group status. This problem is generically called <strong>conditioning on post-treatment variables</strong> and a full discussion of is out of the scope of this book, but it‚Äôs something to avoid [and read up on if you‚Äôre worried about it; <span class="citation">Montgomery et al. (<a href="#ref-montgomery2018" role="doc-biblioref">2018</a>)</span>].</p>
<p>Finally, one of the standard justifications for controlling is actually ill-founded as well: adding covariates because your groups are unbalanced. People often talk about ‚Äúunhappy randomization‚Äù: you randomize to the different tea-tasting groups, for example, but then it turns out the mean age is a bit different between groups. Then you do a t-test or some other statistical test and find out that you actually have a significant age difference.<label for="tufte-sn-115" class="margin-toggle sidenote-number">115</label><input type="checkbox" id="tufte-sn-115" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">115</span> Incidentally, this practice makes no sense: because you randomized, you know that the difference in ages occurred by chance, so why are you using a t-test to test if the variation is due to chance?</span> But incidental demographic differences between groups are unlikely to be important unless that characteristic is highly correlated with the outcome (see above).</p>
<p>So these are are options: if a covariate is known to be very strongly related to our outcome, we can include it in our default model. Otherwise, we avoid a lot of trouble by leaving covariates out.</p>
</div>
</div>
<div id="chapter-summary-models" class="section level2" number="7.7">
<h2><span class="header-section-number">7.7</span> Chapter summary: Models</h2>
<p>In the last three chapters, we have spelled out a framework for data analysis that focuses on the key experimental goal: a measurement of a particular causal effect. We began with basic techniques for estimating effects and making inferences about how these effects estimated from a sample can be generalized to a population. This chapter showed how these ideas naturally give rise to the idea of making models of data, which allow estimation of effects in more complex designs. Simple regression models, which are formally identical to other inference methods in the most basic case, can be extended with the generalized linear model as well as with mixed effects models. Finally, we ended with some guidance on how to build a ‚Äúdefault model‚Äù ‚Äì an (often pre-registered) regression model that maps onto your experimental design and provides the primary estimate of your key causal effect.</p>
<!-- ## Connection to Bayesian approaches -->
<!-- * Say that what we have covered so far is essentially just the likelihood; formalize the intuition from "Estimation" about Bayesian inference to show how a prior affects the model specification -->
<!-- structure in your data so that you can better estimate the particular effects of interest. -->
<!-- *** The advice not to model covariates that aren't very correlated with your outcome is very frequentist, with the idea being that you lose power when you condition on too many things. In contrast, Gelman & Hill (2006) give more Bayesian advice: if you think a variable matters to your outcome, keep it in the model. This advice is consistent with the idea of modeling experimental covariates, even if they don't have a big correlation with the outcome. In the Bayesian framework, including this extra information should (maybe only marginally) improve your precision but you aren't "spending degrees of freedom" in the same way. -->
<!-- ## FROM PREVIOUS STRUCTURE: Inference and estimation for two-group designs -->
<!-- Throughout this book we've taken the position that the goal of experiments is to estimate a causal effect of interest, ideally as part of some theory of how different constructs relate to one another. All this talk of hypotheses and inferences above is only indirectly related to that goal.  -->
<!-- - Intuition builder: For very large n, or flat prior, Bayes and frequentist coincide.  -->
<!-- ### Simple models of between-group differences -->
<!-- Introducing simple inference models: -->
<!-- - The chi-squared test for inferring whether two samples come from the same distribution -->
<!-- - The t-test for inferring whether a single group‚Äôs effect differs from 0 -->
<!-- - The t-test for inferring whether two groups differ from one another -->
<!-- - The paired t-test as a first glimpse at how we might account for participant-level random effects (see Chapter 7). -->
<!-- ::: {.accident-report} -->
<!-- ‚ö†Ô∏è Accident report: Once you have the basic t-test under your belt, it might feel natural to compare each group to 0 and conclude that one group is different from 0 and the other one isn‚Äôt. But ‚Äúthe difference between significant and not significant is not necessarily itself statistically significant‚Äù (Nieuwenhuis, Forstmann, and Wagenmakers 2011).  -->
<!-- ::: -->
<!-- How to go from theory to hypotheses to statistical model -->
<!-- Re-casting the t-test as a regression model  -->
<!-- ::: {.interactive} -->
<!-- ‚å®Ô∏è Interactive box: Visualizing how different tests are variants of linear models. -->
<!-- ::: -->
<!-- - Discrete data and logistic regression. Same thing, different linking function. (lead-in to GLM: probit, Poisson, beta, etc.). -->
<!-- Multilevel regression models of a difference between two groups, controlling for experimental items and subject -->
<!--   - Sidebar: what should you control for? Different subcultures in psychology either post-hoc control for or look for moderation by demographic factors. We discuss the consequences of these decisions for both precision and causal inference.  -->
<!-- Causality revisited: what can and can‚Äôt be concluded from an experiment -->
<!-- ::: {.accident-report} -->
<!-- ‚ö†Ô∏è Accident report: Mediation going wrong: even when you have a randomized experiment, you can still mess up your causal inference (Montgomery, Nyhan, and Torres 2018).  -->
<!-- ::: -->
<!-- - Dropping subjects who fail a manipulation check can be problematic (Aronow, Baron, and Pinson 2019). -->
<!-- - Mediation requires more confounding assumptions than causal inference about total effects, and these assumptions may be violated even in randomized experiments.  -->
<!-- ### Homeless -->
<!-- ‚å®Ô∏è Interactive box: non-parametric simulations where you can shuffle data across groups a bunch of times and see what kind of distribution it produces by chance -->
<!-- ::: {.interactive} -->
<!-- ‚å®Ô∏è Interactive: Nonparameteric resampling under the null -->
<!-- As we've seen above, hypothesis testing hinges on approximating the null distribution of the statistical estimate. In the examples above, it was easy to use statistical theory to work out the null distribution: for example, in Figure \@ref(fig:inference-null-model), we knew that the null distribution must be binomial since we had a binary outcome, and the binomial distribution parameter $p$ must be 0.5, because that is what the null says.  -->
<!-- But sometimes we don't know what the null distribution would look like. Suppose we want to estimate group differences in a highly skewed continuous outcome, like salary, but we had a small sample size (e.g., $n=10$ per group): -->
<!-- ```{r inference-permutation-1, eval=FALSE} -->
<!-- print(d) -->
<!-- set.seed(451) -->
<!-- n.per.group = 10 -->
<!-- d = data.frame( Y = c( rexp( n = n.per.group, rate = 4 ), -->
<!--                        rexp( n = n.per.group, rate = 4 ) + 0.3 ), -->
<!--                 Group = c( rep( "Control", n.per.group ), -->
<!--                            rep ( "Treatment", n.per.group ) ) ) -->
<!-- colors = c("black", "orange") -->
<!-- ggplot( data = d, -->
<!--         aes(x = Y, group = Group ) ) + -->
<!--   #geom_histogram( aes(fill = Group), alpha = 0.4 ) + -->
<!--   geom_dotplot( aes( fill=Group ), alpha=0.4, binwidth = .05 ) + -->
<!--   scale_y_continuous(NULL, breaks = NULL) + -->
<!--   theme_bw() + xlab("Y") + -->
<!--   ylab("")  -->
<!-- ``` -->
<!-- We can't proceed with a t-test in good conscience because, with only $n=20$, we can't necessarily trust that the Central Limit Theorem has "kicked in" sufficiently for the test to work despite the skewness. Stated otherwise, we can't be sure that the null distribution is normal in this case.  -->
<!-- When we can't rely on theory, another way to approximate a null distribution is through nonparameteric resampling. "Resampling" means that we're going to cleverly draw new samples *from our existing sample*, and "nonparametric" means that we will do this in a way that obviates assumptions about the shape of the null distribution (in contrast to parameteric approaches that do rely on such assumptions). -->
<!-- The central idea is that, if the treatment truly had no effect on the outcome, then the observations would be *exchangeable* between the treatment and control groups. That is, there would not be systematic differences between the treatment and control groups. This may or may not be true in our observed sample (after all, that's why we're doing a hypothesis test in the first place), but we could draw new samples from our existing sample in a manner that forces exchangeability. In this case, we could randomly permute the column of outcomes in our dataset while leaving the column of treatment assignments fixed: -->
<!-- ```{r inference-permutation-2, eval=FALSE} -->
<!-- # show construction of a single permuted dataset -->
<!-- # library(modelr) -->
<!-- # library(purrr) -->
<!-- perm.reps = 100 -->
<!-- perms = permute(data = d, n = perm.reps, Y) -->
<!-- # show the first permuted dataset -->
<!-- print( perms$perm[[1]]$data ) -->
<!-- #@WHY ARE THESE ALL THE SAME?? -->
<!-- cbind( perms$perm[[1]]$data$Y, perms$perm[[2]]$data$Y ) -->
<!-- # c.f. package example for debugging -->
<!-- perms2 <- permute(mtcars,  1000, mpg) -->
<!-- cbind( perms2$perm[[1]]$data$mpg, perms2$perm[[2]]$data$mpg ) -->
<!-- # also the same??  -->
<!-- # get group mean differences for each permutation via OLS -->
<!-- ols = map(perms$perm, ~ lm(Y ~ 1, data = .)) -->
<!-- ATEs = lapply( ols, function(.ols) as.numeric( coef(.ols)["(Intercept)"] ) ) -->
<!-- # now plot the ATEs to show the null sampling distribution -->
<!-- ``` -->
<!-- ::: -->

</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-baddeley1975" class="csl-entry">
Baddeley, A. D., Thomson, N., &amp; Buchanan, M. (1975). Word length and the structure of short-term memory. <em>Journal of Verbal Learning and Verbal Behavior</em>, <em>14</em>(6), 575‚Äì589.
</div>
<div id="ref-barr2013" class="csl-entry">
Barr, D. J., Levy, R., Scheepers, C., &amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. <em>Journal of Memory and Language</em>, <em>68</em>(3), 255‚Äì278.
</div>
<div id="ref-bates2014" class="csl-entry">
Bates, D., M√§chler, M., Bolker, B., &amp; Walker, S. (2014). Fitting linear mixed-effects models using lme4. <em>arXiv Preprint arXiv:1406.5823</em>.
</div>
<div id="ref-bie2021fitting" class="csl-entry">
Bie, R., Haneuse, S., Huey, N., Schildcrout, J., &amp; McGee, G. (2021). Fitting marginal models in small samples: A simulation study of marginalized multilevel models and generalized estimating equations. <em>Statistics in Medicine</em>, <em>40</em>(24), 5298‚Äì5312.
</div>
<div id="ref-clark1973" class="csl-entry">
Clark, H. H. (1973). The language-as-fixed-effect fallacy: A critique of language statistics in psychological research. <em>Journal of Verbal Learning and Verbal Behavior</em>, <em>12</em>(4), 335‚Äì359.
</div>
<div id="ref-lovatt2000" class="csl-entry">
Lovatt, P., Avons, S. E., &amp; Masterson, J. (2000). The word-length effect and disyllabic words. <em>The Quarterly Journal of Experimental Psychology: Section A</em>, <em>53</em>(1), 1‚Äì22.
</div>
<div id="ref-montgomery2018" class="csl-entry">
Montgomery, J. M., Nyhan, B., &amp; Torres, M. (2018). How conditioning on posttreatment variables can ruin your experiment and what to do about it. <em>Am. J. Pol. Sci.</em>, <em>62</em>(3), 760‚Äì775.
</div>
<div id="ref-stiller2015" class="csl-entry">
Stiller, A. J., Goodman, N. D., &amp; Frank, M. C. (2015). Ad-hoc implicature in preschool children. <em>Language Learning and Development</em>, <em>11</em>(2), 176‚Äì190.
</div>
<div id="ref-westfall2015" class="csl-entry">
Westfall, J., Judd, C. M., &amp; Kenny, D. A. (2015). Replicating studies in which samples of participants respond to samples of stimuli. <em>Perspectives on Psychological Science</em>, <em>10</em>(3), 390‚Äì399.
</div>
</div>
<p style="text-align: center;">
<a href="6-inference.html"><button class="btn btn-default">Previous</button></a>
<a href="8-measurement.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<link href="www/global.css" rel="stylesheet">
<script src="www/global.js"></script>


</body>
</html>
