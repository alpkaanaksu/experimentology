<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 8 Design of experiments | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />



<meta name="description" content="Chapter 8 Design of experiments | Experimentology">

<title>Chapter 8 Design of experiments | Experimentology</title>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.6.1/grViz.js"></script>




<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="toc/toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-intro.html#intro"><span class="toc-section-number">1</span> Experiments and theories</a></li>
<li><a href="2-replication.html#replication"><span class="toc-section-number">2</span> Replication and reproducibility</a></li>
<li><a href="3-ethics.html#ethics"><span class="toc-section-number">3</span> Ethics</a></li>
<li class="part"><span><b>II Statistics</b></span></li>
<li><a href="4-estimation.html#estimation"><span class="toc-section-number">4</span> Estimation</a></li>
<li><a href="5-inference.html#inference"><span class="toc-section-number">5</span> Inference</a></li>
<li><a href="6-models.html#models"><span class="toc-section-number">6</span> Models</a></li>
<li class="part"><span><b>III Design and Planning</b></span></li>
<li><a href="7-measurement.html#measurement"><span class="toc-section-number">7</span> Measurement</a></li>
<li><a href="8-design.html#design"><span class="toc-section-number">8</span> Design of experiments</a></li>
<li><a href="9-sampling.html#sampling"><span class="toc-section-number">9</span> Sampling</a></li>
<li><a href="10-prereg.html#prereg"><span class="toc-section-number">10</span> Preregistration</a></li>
<li class="part"><span><b>IV Execution</b></span></li>
<li><a href="11-selection.html#selection"><span class="toc-section-number">11</span> Experimental strategy</a></li>
<li><a href="12-collection.html#collection"><span class="toc-section-number">12</span> Data collection</a></li>
<li><a href="13-management.html#management"><span class="toc-section-number">13</span> Project management</a></li>
<li class="part"><span><b>V Analysis and Reporting</b></span></li>
<li><a href="14-viz.html#viz"><span class="toc-section-number">14</span> Visualization</a></li>
<li><a href="15-eda.html#eda"><span class="toc-section-number">15</span> Exploratory data analysis</a></li>
<li><a href="16-writing.html#writing"><span class="toc-section-number">16</span> Reproducible writing</a></li>
<li><a href="17-meta.html#meta"><span class="toc-section-number">17</span> Meta-analysis</a></li>
<li><a href="18-conclusions.html#conclusions"><span class="toc-section-number">18</span> Conclusions</a></li>
<li class="part"><span><b>VI Appendices</b></span></li>
<li><a href="19-git.html#git"><span class="toc-section-number">19</span> GitHub Tutorial</a></li>
<li><a href="20-rmarkdown.html#rmarkdown"><span class="toc-section-number">20</span> R Markdown Tutorial</a></li>
<li><a href="21-tidyverse.html#tidyverse"><span class="toc-section-number">21</span> Tidyverse Tutorial</a></li>
<li><a href="22-ggplot.html#ggplot"><span class="toc-section-number">22</span> ggplot Tutorial</a></li>
<li><a href="23-instructors.html#instructors"><span class="toc-section-number">23</span> Instructor‚Äôs guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="design" class="section level1" number="8">
<h1><span class="header-section-number">Chapter 8</span> Design of experiments</h1>
<div class="learning-goals">
<p>üçé Learning goals:</p>
<ul>
<li>Describe key elements to designing a manipulation</li>
<li>Define randomization and counterbalancing strategies for removing confounds</li>
<li>Discuss strategies to design experiments that are appropriate to the populations of interest</li>
</ul>
</div>
<p>The key thesis of our book is that experiments should be designed to yield precise and unbiased measurements of a causal effect of interest. But the causal effect of what? The manipulation, in a word. In an experiment we intervene on the world and measure the effects of that manipulation. We then compare that measurement to a case where the intervention has not occurred. The previous chapter covered the topic of measurement; here we discuss manipulations.</p>
<p>We refer to different intervention states as <strong>conditions</strong> of the experiment. The most common experimental design is the comparison between a <strong>control</strong> condition, in which the intervention is not performed, and an <strong>experimental</strong> (or sometimes, <strong>treatment</strong>) condition in which the intervention is performed. But many other experimental designs are possible. The goal of this chapter is to introduce some of these and give you some tools for considering their tradeoffs. In the first part of the chapter, we‚Äôll introduce some common experimental designs and the vocabulary for describing them.</p>
<p>To be useful, a measure must be a valid measure of a construct of interest. The same is true for a manipulation ‚Äì it must validly relate to the causal effect of interest. In the next part of the chapter, we‚Äôll discuss issues of <strong>manipulation validity</strong>, including both issues of ecological validity and <strong>confounding</strong>. We‚Äôll talk about how practices like <strong>randomization</strong> and <strong>counterbalancing</strong> can help remove nuisance confounds.<label for="tufte-sn-1" class="margin-toggle sidenote-number">1</label><input type="checkbox" id="tufte-sn-1" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">1</span> This section will draw on our introduction of causal inference in Chapter <a href="1-intro.html#intro">1</a>, so if you haven‚Äôt read that, now‚Äôs the time.</span></p>
<p>We‚Äôll end the chapter by discussing some aspects of strategy in experimental design. How do you design an experiment to test a theory? What sorts of experimental designs are maximally efficient?</p>
<div id="experimental-designs" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Experimental designs</h2>
<p>Experimental designs are so fundamental to so many fields that they are discussed in many different ways. As a result, the terminology can get quite confusing. Here we‚Äôll try to stay consistent by describing an experiment as a relationship between some <strong>manipulation</strong> in which participants are randomly assigned to an experimental condition to evaluate its effects on some <strong>measure</strong>.<label for="tufte-sn-2" class="margin-toggle sidenote-number">2</label><input type="checkbox" id="tufte-sn-2" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">2</span> The alternative terminology used in psychology is that of an <strong>independent variable</strong> (the manipulation, which is causally prior and hence ‚Äúindependent‚Äù of other causal influences) and a <strong>dependent variable</strong> (the measure, which causally depends on the manipulation, or so we hypothesize). This terminology seems transparently terrible. An alternative is the terms that are often used in econometrics of the <strong>treatment</strong> or <strong>intervention</strong> (manipulation) and the <strong>outcome</strong> (measure). This terminology is clearer, but also has some fairly medical connotations ‚Äì it sounds like the treatment is something substantial and lasting, and the outcome is meaningful. That‚Äôs not always the case in experiments that investigate psychological mechanisms. For example, in a cognitive psychology context, it sounds a bit weird to us to say that the ‚Äútreatment‚Äù was reading scrambled words and the ‚Äúoutcome‚Äù was lexical decision reaction times.</span></p>
</div>
<div id="factorial-experiments" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Factorial experiments</h2>
<p>How should we decide what conditions to have in our experiment? The classical ‚Äúdesign of experiments‚Äù paradigm has as its goal to</p>
<p>Crossed designs</p>
</div>
<div id="between--vs.-within-participant-designs" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Between- vs.¬†within-participant designs</h2>
<div class="figure"><span id="fig:unnamed-chunk-2"></span>
<div id="htmlwidget-30e4409849e39179307f" style="width:50%;height:110%;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-30e4409849e39179307f">{"x":{"diagram":"digraph {\n  graph [layout = dot, rankdir = LR]\n  \n  node [shape = rectangle, style = filled, fillcolor = white]        \n  a [label = \"Participant 1\"]\n  b [label = \"Participant 2\"]\n  c [label = \"Participant 3\"]\n  d [label = \"Participant 4\"]\n  \n  node [fillcolor = pink]\n  c1a [label = \"Experimental Manipulation\"]\n  c1b [label = \"Experimental Manipulation\"]\n  \n  node [fillcolor = lightblue]\n  c2a [label = \"Control Manipulation\"]\n  c2b [label = \"Control Manipulation\"]\n  \n  node [fillcolor = white]\n  m1 [label =  \"Measure\"]\n  m2 [label =  \"Measure\"]\n  m3 [label =  \"Measure\"]\n  m4 [label =  \"Measure\"]\n  \n  # edge definitions with the node IDs\n  a -> c1a -> m1\n  b -> c1b -> m2\n  c -> c2b -> m4\n  d -> c2a -> m3\n  }","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p class="caption marginnote shownote">
Figure 8.1: A between-participants design.
</p>
</div>
<p>between
Main advantage
No contamination by other exposure to experimental materials
Disadvantages
Requires many participants
Individual differences create a lot of variability in groups
Potential for assignment bias: need to control for differences between groups
Other environmental group differences</p>
<p>Main advantage
Eliminates subject variability
Relatively few participants needed, because of this lack of variability
Disadvantages
Carryover effects mean that ordering of conditions can be problematic
Not always possible: imagine trying a within-subjects design for surgery
General contention: preferable when possible</p>
</div>
<div id="discrete-and-continuous-experimental-manipulations" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> Discrete and continuous experimental manipulations</h2>
<p>Continuous and discrete variables</p>
<p>Dose response relationships</p>
<p>Multi-experiment design strategy: start simple?</p>
<!-- ::: {.case-study} -->
<!-- üî¨ Case study: Still suspicious? -->
<!-- The ‚Äúsuspicious coincidence‚Äù effect (Xu and Tenenbaum 2007) with non-replication by Spencer et al. (2011) resolved by Lewis & Frank (2018) ‚Äústill suspicious‚Äù paper.  -->
<!-- ::: -->
</div>
<div id="manipulation-validity" class="section level2" number="8.5">
<h2><span class="header-section-number">8.5</span> Manipulation validity</h2>
<div id="threats-to-manipulation-validity" class="section level3" number="8.5.1">
<h3><span class="header-section-number">8.5.1</span> Threats to manipulation validity</h3>
<ul>
<li><p>Manipulations must correspond to the construct whose causal effect is being estimated.</p></li>
<li><p>Demand characteristics. How did concerns about demand characteristics emerge? What proposed mechanisms cause demand characteristics to influence participant behavior? What evidence do we have that demand characteristics impact participant behavior? And what strategies can we use to mitigate demand characteristics?</p></li>
</ul>
<p>Ecological validity</p>
</div>
<div id="confounding" class="section level3" number="8.5.2">
<h3><span class="header-section-number">8.5.2</span> Confounding</h3>
<p>Let‚Äôs consider an alternate experiment now. Suppose we did the same basic procedure, but now with a ‚Äúwithin-subjects‚Äù design where participants do both the Dylan treatment and the control, in that order. This experiment is flawed, of course. If you observe a Dylan effect, you can‚Äôt rule out the idea that participants got tired and wrote worse in the control condition because it always came second.</p>
<p>Order (Dylan first vs.¬†control first; notated X‚Äô) is an experimental confound: a variable that is created in the course of the experiment that is both causally related to the predictor and potentially also related to the outcome. Here‚Äôs how the causal model now looks:</p>
<p>We‚Äôve reconstructed the same kind of confounding relationship we had with age, where we had a variable (X‚Äô) that was correlated both with our predictor (X) and our outcome (Y)! So‚Ä¶</p>
<p>In the causal language we have been using, counterbalancing allows us to snip out the causal dependency between order and Dylan. Now they are unconfounded (uncorrelated) with one another. We‚Äôve ‚Äúsolved‚Äù a confound in our experimental design. Here‚Äôs the picture:</p>
<p>These are not covariates! Covariates are related but don‚Äôt have causal force in this design because of randomization. We can use them in our analysis to make our estimates more precise (see Chapter <a href="6-models.html#models">6</a>), but we won‚Äôt worry about them here. If someone says to you, ‚Äúparticipant gender is a confound in your experiment,‚Äù if you‚Äôve done random assignment to condition appropriately (acoss genders), you should say ‚Äúno it‚Äôs not.‚Äù</p>
</div>
<div id="removing-nuisance-confounds" class="section level3" number="8.5.3">
<h3><span class="header-section-number">8.5.3</span> Removing nuisance confounds</h3>
<p>What should we do with our experimental confounds?</p>
<p>Option 1. Randomize. Increasingly, this is my go-to method for dealing with any confound. Is the correct answer on my survey confounded with response side? Randomize what side the response shows up on! Is order confounded with condition? Randomize the order you present in! Randomization is much easier now that we program many of our experiments using software like Qualtrics or code them from scratch in JavaScript.</p>
<p>The only time you really get in trouble with randomization is when you have a large number of options, a small number of participants, or some combination of the two. In this case, you can end up with unbalanced levels of the randomized factors (for example, ten answers on the right side and two on the left). Averaging across many experiments, this lack of balance will come out in the wash. But in a single experiment, it can really mess up your data ‚Äì especially if your participants notice and start choosing one side more than the other because it‚Äôs right more often. For that reason, when balance is critical, you want option 2.</p>
<p>Option 2. Counterbalance. If you think a particular confound might have a significant effect on your measure, balancing it across participants and across trials is a very safe choice. That way, you are guaranteed to have no effect of the confound on your average effect. In a simple counterbalance of order for our Dylan experiment, we manipulate condition order between subjects. Some participants hear Dylan first and others hear Dylan second. Although technically we might call order a second ‚Äúfactor‚Äù in the experiment, in practice it‚Äôs really just a nuisance variable, so we don‚Äôt talk about it as a factor and we often don‚Äôt analyze it (but see Option 3 below).</p>
<p>Counterbalancing doesn‚Äôt always work, though. It gets trickier when you have too many levels on a variable (too many Dylan songs!) or multiple confounding variables. For example, if you have lots of different nuisance variables ‚Äì say, condition order, what writing prompt you use for each order, which Dylan song you play ‚Äì it may not be possible to do a fully-crossed counterbalance so that all combinations of these factors are seen by equal numbers of participants. In these kinds of cases, you may have to rely on partial counterbalancing schemes or latin squares designs, or you may have to fall back on randomization.</p>
<p>Option 3. Do Options 1 and 2 and then model the variation. This option was never part of my training, but it‚Äôs an interesting third option that I‚Äôm increasingly considering.** That is, we are often faced with the choice between A) a noisy between-participants design and B) a lower-noise within-participants design that nevertheless adds noise back in via some obvious order effect that you have to randomize or counterbalance. In a recent talk by Andrew Gelman, he suggested that we try to model these as covariates, to reduce noise. This seems like a pretty interesting suggestion, especially if the correlation between them and the outcome is substantial.***</p>
</div>
</div>
<div id="strategy" class="section level2" number="8.6">
<h2><span class="header-section-number">8.6</span> Strategy</h2>
<div id="how-to-design-a-manipulation-to-test-a-theory." class="section level3" number="8.6.1">
<h3><span class="header-section-number">8.6.1</span> How to design a manipulation to test a theory.</h3>
<p>Simplicity as a key design principle</p>
<ul>
<li>E. O. Wilson‚Äôs advice: iteration on a repeatable measurement.</li>
<li>Statistical and interpretability concerns for complex interaction designs.</li>
<li>Nuisance variables: counterbalancing and randomization.</li>
</ul>
<p>The temptation to manipulate lots of things</p>
<p>Again, we advocate for simplicity.</p>
<p>The advice is out there to</p>
</div>
<div id="connecting-with-theory" class="section level3" number="8.6.2">
<h3><span class="header-section-number">8.6.2</span> Connecting with theory</h3>
<ul>
<li>The ethics of the ‚Äúdead on arrival‚Äù experiment ‚Äì why appropriate experimental design is an ethical imperative (we ‚Äúwaste‚Äù participant contributions otherwise).
‚Äúrisky tests‚Äù: those that will best help adjudicate between theories. <span class="citation"><a href="#ref-meehl1978" role="doc-biblioref">Meehl</a> (<a href="#ref-meehl1978" role="doc-biblioref">1978</a>)</span></li>
</ul>
<p>Optimal experiment design in psychophysics and beyond ‚Äì how to use quantitative models to select the stimulus that maximizes your chances of a theory-informing result.</p>
<div class="ethics-box">
<p>üåø Ethics box: Including the population being sampled in the design process.</p>
</div>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-meehl1978" class="csl-entry">
Meehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir karl, sir ronald, and the slow progress of soft psychology. <em>J. Consult. Clin. Psychol.</em>, <em>46</em>(4), 806‚Äì834.
</div>
</div>
<p style="text-align: center;">
<a href="7-measurement.html"><button class="btn btn-default">Previous</button></a>
<a href="9-sampling.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="toc/toc.js"></script>


</body>
</html>
