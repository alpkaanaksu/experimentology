<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 8 Design of experiments | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 8 Design of experiments | Experimentology">

<title>Chapter 8 Design of experiments | Experimentology</title>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.6.1/grViz.js"></script>




<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="toc/toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-intro.html#intro"><span class="toc-section-number">1</span> Experiments and theories</a></li>
<li><a href="2-replication.html#replication"><span class="toc-section-number">2</span> Replication and reproducibility</a></li>
<li><a href="3-ethics.html#ethics"><span class="toc-section-number">3</span> Ethics</a></li>
<li class="part"><span><b>II Statistics</b></span></li>
<li><a href="4-estimation.html#estimation"><span class="toc-section-number">4</span> Estimation</a></li>
<li><a href="5-inference.html#inference"><span class="toc-section-number">5</span> Inference</a></li>
<li><a href="6-models.html#models"><span class="toc-section-number">6</span> Models</a></li>
<li class="part"><span><b>III Design and Planning</b></span></li>
<li><a href="7-measurement.html#measurement"><span class="toc-section-number">7</span> Measurement</a></li>
<li><a href="8-design.html#design"><span class="toc-section-number">8</span> Design of experiments</a></li>
<li><a href="9-sampling.html#sampling"><span class="toc-section-number">9</span> Sampling</a></li>
<li><a href="10-prereg.html#prereg"><span class="toc-section-number">10</span> Preregistration</a></li>
<li class="part"><span><b>IV Execution</b></span></li>
<li><a href="11-selection.html#selection"><span class="toc-section-number">11</span> Experimental strategy</a></li>
<li><a href="12-collection.html#collection"><span class="toc-section-number">12</span> Data collection</a></li>
<li><a href="13-management.html#management"><span class="toc-section-number">13</span> Project management</a></li>
<li class="part"><span><b>V Analysis and Reporting</b></span></li>
<li><a href="14-viz.html#viz"><span class="toc-section-number">14</span> Visualization</a></li>
<li><a href="15-eda.html#eda"><span class="toc-section-number">15</span> Exploratory data analysis</a></li>
<li><a href="16-writing.html#writing"><span class="toc-section-number">16</span> Reproducible writing</a></li>
<li><a href="17-meta.html#meta"><span class="toc-section-number">17</span> Meta-analysis</a></li>
<li><a href="18-conclusions.html#conclusions"><span class="toc-section-number">18</span> Conclusions</a></li>
<li class="part"><span><b>VI Appendices</b></span></li>
<li><a href="19-git.html#git"><span class="toc-section-number">19</span> GitHub Tutorial</a></li>
<li><a href="20-rmarkdown.html#rmarkdown"><span class="toc-section-number">20</span> R Markdown Tutorial</a></li>
<li><a href="21-tidyverse.html#tidyverse"><span class="toc-section-number">21</span> Tidyverse Tutorial</a></li>
<li><a href="22-ggplot.html#ggplot"><span class="toc-section-number">22</span> ggplot Tutorial</a></li>
<li><a href="23-instructors.html#instructors"><span class="toc-section-number">23</span> Instructor‚Äôs guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="design" class="section level1" number="8">
<h1><span class="header-section-number">Chapter 8</span> Design of experiments</h1>
<p><img src="images/design/meme.jpg" width="\linewidth"  /></p>
<div class="learning-goals">
<p>üçé Learning goals:</p>
<ul>
<li>Describe key elements to designing a manipulation</li>
<li>Define randomization and counterbalancing strategies for removing confounds</li>
<li>Discuss strategies to design experiments that are appropriate to the populations of interest</li>
</ul>
</div>
<p>The key thesis of our book is that experiments should be designed to yield precise and unbiased measurements of a causal effect. But the causal effect of what? The manipulation, in a word. In an experiment we intervene on the world and measure the effects of that manipulation. We then compare that measurement to a case where the intervention has not occurred. The previous chapter covered the topic of measurement; here we discuss manipulations.</p>
<p>We refer to different intervention states as <strong>conditions</strong> of the experiment. These conditions instantiate specific <strong>factors</strong> of interest. The most common experimental design is the comparison between a <strong>control</strong> condition, in which the intervention is not performed, and an <strong>experimental</strong> (or sometimes, <strong>treatment</strong>) condition in which the intervention is performed. But many other experimental designs are possible. The goal of this chapter is to introduce some of these and give you some tools for considering their tradeoffs. In the first part of the chapter, we‚Äôll introduce some common experimental designs and the vocabulary for describing them.</p>
<p>To be useful, a measure must be a valid measure of a construct of interest. The same is true for a manipulation ‚Äì it must validly relate to the causal effect of interest. In the next part of the chapter, we‚Äôll discuss issues of <strong>manipulation validity</strong>, including both issues of ecological validity and <strong>confounding</strong>. We‚Äôll talk about how practices like <strong>randomization</strong> and <strong>counterbalancing</strong> can help remove nuisance confounds.<label for="tufte-sn-1" class="margin-toggle sidenote-number">1</label><input type="checkbox" id="tufte-sn-1" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">1</span> This section will draw on our introduction of causal inference in Chapter <a href="1-intro.html#intro">1</a>, so if you haven‚Äôt read that, now‚Äôs the time.</span></p>
<p>We‚Äôll end the chapter by discussing some aspects of strategy in experimental design. How do you design an experiment to test a theory? What sorts of experimental designs are maximally efficient? To preview our general approach: we think that your default experiment should have one or a maximum of two factors and should manipulate those factors continuously and within-subjects. This strategy is most likely to yield precise estimates of a particular effect that can be used to constrain future theorizing.</p>
<div id="experimental-designs" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Experimental designs</h2>
<p>Experimental designs are so fundamental to so many fields that they are discussed in many different ways. As a result, the terminology can get quite confusing. Here we‚Äôll try to stay consistent by describing an experiment as a relationship between some <strong>manipulation</strong> in which participants are randomly assigned to an experimental condition to evaluate its effects on some <strong>measure</strong>.<label for="tufte-sn-2" class="margin-toggle sidenote-number">2</label><input type="checkbox" id="tufte-sn-2" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">2</span> The alternative terminology used in psychology is that of an <strong>independent variable</strong> (the manipulation, which is causally prior and hence ‚Äúindependent‚Äù of other causal influences) and a <strong>dependent variable</strong> (the measure, which causally depends on the manipulation, or so we hypothesize). This terminology seems transparently terrible.</span> An alternative is the terms that are often used in econometrics: the <strong>treatment</strong> (manipulation) and the <strong>outcome</strong> (measure).<label for="tufte-sn-3" class="margin-toggle sidenote-number">3</label><input type="checkbox" id="tufte-sn-3" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">3</span> This terminology has some fairly medical connotations ‚Äì it sounds like the treatment is something substantial and lasting, and the outcome is meaningful. That‚Äôs not always the case in experiments that investigate psychological mechanisms. For example, in a cognitive psychology context, it sounds a bit weird to us to say that the ‚Äútreatment‚Äù was reading scrambled words and the ‚Äúoutcome‚Äù was lexical decision reaction times.</span> We‚Äôll sometimes use ‚Äútreatment‚Äù language here as well.</p>
<p>In this section, we‚Äôll discuss a number of dimensions on which experiments vary. First, they vary in how many factors they incorporate and how these factors are crossed ‚Äì we begin with the two-factor experiment and then discuss generalizations. Second, they vary in how many conditions and how many measures are given to each participant. Third, their manipulations can be discrete or continuous.</p>
<div id="a-two-factor-experiment" class="section level3" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> A two-factor experiment</h3>
<p>The classical ‚Äúdesign of experiments‚Äù framework has as its goal to separate observed variability in the dependent measure into 1) variability due to the manipulation(s) and (2) other variability, including measurement error and participant-level variation. This framework maps nicely onto the statistical framework described in Chapters <a href="4-estimation.html#estimation">4</a> ‚Äì <a href="6-models.html#models">6</a>. We are modeling the distribution of our measure using information about the condition structure of our experiment as our predictors.</p>
<p>Different experimental designs will allow us to estimate condition effects more and less effectively. Recall in Chapter <a href="4-estimation.html#estimation">4</a>, we estimated the effect of our manipulation by a simple subtraction: <span class="math inline">\(\Delta = \theta_{treatment} - \theta_{control}\)</span> (where <span class="math inline">\(\Delta\)</span> is the effect estimate, and <span class="math inline">\(\theta\)</span>s indicate the estimates for each condition). This logic works just fine also if there are two distinct treatments in a three condition experiment: each treatment can be compared to control separately. For treatment 1, <span class="math inline">\(\Delta_{T_1} = \theta_{T_2} - \theta_{control}\)</span> and <span class="math inline">\(\Delta_{T_1} = \theta_{T_2} - \theta_{control}\)</span>.</p>
<p>That logic is going to get more complicated if we have more than one distinct factor of interest, though. Let‚Äôs look at a simple example. <span class="citation"><a href="#ref-young2007" role="doc-biblioref">Young et al.</a> (<a href="#ref-young2007" role="doc-biblioref">2007</a>)</span> were interested in how moral judgments depend on both the beliefs of actors and the outcomes of their actions. They presented participants with vignettes in which they learned, for example, that Grace visits a chemical factory with her friend and goes to the coffee break room where she sees a white powder that she puts in her friend‚Äôs coffee. They then manipulated both Grace‚Äôs beliefs and the outcomes of her actions following the schema in Figure <a href="8-design.html#fig:design-young-design">8.1</a>. Participants (N=10) used a four-point Likert scale to rate whether the actions were morally forbidden (1) or permissible (4).</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:design-young-design"></span>
<img src="images/design/young2007-design.png" alt="The 2x2 crossed design used in Young et al. (2007)." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 8.1: The 2x2 crossed design used in Young et al.¬†(2007).<!--</p>-->
<!--</div>--></span>
</p>
<p>Young et al.‚Äôs design has two factors ‚Äì belief and outcome ‚Äì each with two levels (negative and neutral).<label for="tufte-sn-4" class="margin-toggle sidenote-number">4</label><input type="checkbox" id="tufte-sn-4" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">4</span> Note that neither of these is necessarily a ‚Äúcontrol‚Äù condition: the goal is simply to compare these two levels of the factor ‚Äì negative and neutral ‚Äì to estimate the effect due to the factor.</span> These factors are <strong>fully crossed</strong>: each level of each factor is combined with each level of each other. That means that we can estimate a number of effects of interest. The experimental data are shown in Figure <a href="8-design.html#fig:design-young-data">8.2</a>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:design-young-data"></span>
<img src="images/design/young2007-data.png" alt="Moral permissability as a function of belief and outcome. Results from Young et al. (2007)." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 8.2: Moral permissability as a function of belief and outcome. Results from Young et al.¬†(2007).<!--</p>-->
<!--</div>--></span>
</p>
<p>This fully-crossed design makes it easy for us to estimate quantities of interest. Let‚Äôs say that our <strong>reference</strong> group (equivalent to the control group for now) is neutral belief, neutral outcome, which we‚Äôll notate <span class="math inline">\(B,O\)</span>. Now it‚Äôs easy to use the same kind of subtraction we did before to estimate a variety of effects. For example, we can look at the effect of negative belief in the case of a neutral outcome: <span class="math inline">\(\Delta_{-B,O} = \theta_{-B,O} - \theta_{B,O}\)</span>. The effect of a negative outcome is computed similarly as <span class="math inline">\(\Delta_{B,-O} = \theta_{B,-O} - \theta_{B,O}\)</span>.</p>
<p>But now there is a complexity: these two <strong>simple effects</strong> (effects of one variable at a particular level of another variable) make a prediction. They predict that the combined effect <span class="math inline">\(\Delta_{-B,-O}\)</span> should be equal to the sum of <span class="math inline">\(\Delta_{-B,O}\)</span> and <span class="math inline">\(\Delta_{B,-O}\)</span>.<label for="tufte-sn-5" class="margin-toggle sidenote-number">5</label><input type="checkbox" id="tufte-sn-5" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">5</span> If you‚Äôre interested, you can also compute the <strong>average</strong> or <strong>main</strong> effect of a particular factor via the same subtractive logic. For example, the average effect of negative belief (<span class="math inline">\(-B\)</span>) vs.¬†a neutral belief (<span class="math inline">\(B\)</span>) can be computed as <span class="math inline">\(\Delta_{-B} = \frac{(\theta_{-O, -B} + \theta_{O, -B}) - (\theta_{-O, B} + \theta_{O, B})}{2}\)</span>.</span> As we can see from the graph, that‚Äôs not right: if it were, the negative belief, negative outcome condition would be below the minimum possible rating. Instead, we observe an <strong>interaction</strong> effect (sometimes called a <strong>two-way interaction</strong> when there are two factors): The effect when both factors are present is different than the sum of the two simple effects.<label for="tufte-sn-6" class="margin-toggle sidenote-number">6</label><input type="checkbox" id="tufte-sn-6" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">6</span> If you‚Äôre reading carefully, you might be thinking that this all sounds like we‚Äôre talking about the analysis of variance (ANOVA), not about experimental design per se. But these two topics are actually the same topic under the hood: the question is how to design an experiment so that these statistical models can be used to estimate particular effects ‚Äì and combinations of effects ‚Äì that we care about.</span> Critically, without a fully-crossed design, we can‚Äôt estimate this interaction and we would have made an incorrect prediction.</p>
</div>
<div id="generalized-factorial-designs" class="section level3" number="8.1.2">
<h3><span class="header-section-number">8.1.2</span> Generalized factorial designs</h3>
<p>We can refer to Young et al.‚Äôs design, in which there are two factors with two levels each as a <strong>2x2 design</strong> (pronounced ‚Äútwo by two‚Äù). 2x2 designs are incredibly common and useful, but they are only one of an infinite variety of such designs that can be constructed.</p>
<p>Say we added a third factor to Young et al.‚Äôs design such that Grace either feels neutral towards her friend or is angry on that day. If we fully crossed this third affective factor with the other two (belief and outcome), we‚Äôd have a 2x2x2 design. This design would have eight conditions: <span class="math inline">\((A, B, O)\)</span>, <span class="math inline">\((A, B, -O)\)</span>, <span class="math inline">\((A, -B, O)\)</span>, <span class="math inline">\((-A, B, O)\)</span>, <span class="math inline">\((A, -B, -O)\)</span>, <span class="math inline">\((-A, B, -O)\)</span>, <span class="math inline">\((-A, -B, O)\)</span>, <span class="math inline">\((-A, -B, -O)\)</span>. These conditions would in turn allow us to estimate both two-way and three-way interactions, enumerated in Table <a href="8-design.html#tab:design-three-way">8.1</a>.</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:design-three-way">Table 8.1: </span>Possible effects in a hypothetical 2x2x2 experimental design with affect, belief, and outcome as factors.</span><!--</caption>--></p>
<table>


<thead>
<tr>
<th style="text-align:left;">
Effect
</th>
<th style="text-align:left;">
Term Type
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Affect
</td>
<td style="text-align:left;">
Main effect
</td>
</tr>
<tr>
<td style="text-align:left;">
Belief
</td>
<td style="text-align:left;">
Main effect
</td>
</tr>
<tr>
<td style="text-align:left;">
Outcome
</td>
<td style="text-align:left;">
Main effect
</td>
</tr>
<tr>
<td style="text-align:left;">
Affect X Belief
</td>
<td style="text-align:left;">
2-way interaction
</td>
</tr>
<tr>
<td style="text-align:left;">
Affect X Outcome
</td>
<td style="text-align:left;">
2-way interaction
</td>
</tr>
<tr>
<td style="text-align:left;">
Belief X Outcome
</td>
<td style="text-align:left;">
2-way interaction
</td>
</tr>
<tr>
<td style="text-align:left;">
Affect X Belief X Outcome
</td>
<td style="text-align:left;">
3-way interaction
</td>
</tr>
</tbody>
</table>
<p>Three-way interactions are hard to think about! The affect X belief X outcome interaction tells you about the difference in moral permissibility that‚Äôs due to all three factors being present as opposed to what you‚Äôd predict on the basis of your estimates of the two-way interactions. In addition to being hard to think about, higher order interactions tend to be hard to estimate, because estimating them accurately requires you to have a stable estimate of all of the lower-order interactions <span class="citation">(<a href="#ref-mcclelland1993" role="doc-biblioref">McClelland &amp; Judd, 1993</a>)</span>. For this reason, we recommend against experimental designs that rely on higher-order interactions unless you are in a situation where you both have strong predictions about these interactions and are confident in your ability to estimate them appropriately.<label for="tufte-sn-7" class="margin-toggle sidenote-number">7</label><input type="checkbox" id="tufte-sn-7" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">7</span> We‚Äôll talk about how to understand sample size requirements of this type next in Chapter <a href="9-sampling.html#sampling">9</a>.</span></p>
<p>Three-way interactions are just the beginning, though. If you have three factors with two levels each, you can estimate 7 total effects of interest, as in Table <a href="8-design.html#tab:design-three-way">8.1</a>. If you have four factors with two levels each, you get 15. Four factors with three levels each gets you a horrifying 80 different effects!<label for="tufte-sn-8" class="margin-toggle sidenote-number">8</label><input type="checkbox" id="tufte-sn-8" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">8</span> The general formula for <span class="math inline">\(N\)</span> factors with <span class="math inline">\(M\)</span> levels each is <span class="math inline">\(M^N-1\)</span>.</span> This way lies madness, at least from the perspective of estimating and interpreting individual effects in a reasonable sample.</p>
<p>So what should you do if you really do care about four or more factors ‚Äì in the sense that you want to estimate their effects and include them in your theory? The simplest strategy is to start your research off by measuring them independently by running a series of single-factor experiments. These experiments can yield a basis for judging which factors are most important for your outcome and hence which should be prioritized for experiments to estimate interactions.<label for="tufte-sn-9" class="margin-toggle sidenote-number">9</label><input type="checkbox" id="tufte-sn-9" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">9</span> Another strategy is to use fancier methods. For example, the literature on optimal experiment design contains methods for choosing the most informative sequence of experiments to run in order to estimate the parameters in a model <span class="citation">(e.g., <a href="#ref-myung2009" role="doc-biblioref">Myung &amp; Pitt, 2009</a>)</span>. Going down this road typically means having an implemented computational theory of your domain, but it can be a very productive strategy for exploring a complex experimental space with many factors.</span></p>
</div>
<div id="between--vs.-within-participant-designs" class="section level3" number="8.1.3">
<h3><span class="header-section-number">8.1.3</span> Between- vs.¬†within-participant designs</h3>
<p>Once you have a sense of the factor or factors you would like to manipulate in your experiment, the next step is to consider how these will be presented to participants, and how that presentation will interact with your measurements. The biggest decision to be made is whether each participant will experience only one level of a factor ‚Äì a <strong>between-participants</strong> design ‚Äì or whether they will experience multiple levels ‚Äì a <strong>within-participants</strong> design. Figure <a href="8-design.html#fig:design-between">8.3</a> shows a very simple example of between-participants design with four participants (two assigned to each condition), while Figure <a href="#fig:design-within"><strong>??</strong></a> shows a within-participants version of the same design.<label for="tufte-sn-10" class="margin-toggle sidenote-number">10</label><input type="checkbox" id="tufte-sn-10" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">10</span> The within-participants design is counterbalanced for the order of the conditions; we cover the issue of counterbalancing below.</span></p>
<div class="figure"><span id="fig:design-between"></span>
<div id="htmlwidget-30e4409849e39179307f" style="width:50%;height:100%;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-30e4409849e39179307f">{"x":{"diagram":"digraph {\n  graph [layout = dot, rankdir = LR]\n  \n  node [shape = rectangle, style = filled, fillcolor = white]        \n  a [label = \"Participant 1\"]\n  b [label = \"Participant 2\"]\n  c [label = \"Participant 3\"]\n  d [label = \"Participant 4\"]\n  \n  node [fillcolor = pink]\n  c1a [label = \"Experimental Manipulation\"]\n  c1b [label = \"Experimental Manipulation\"]\n  \n  node [fillcolor = lightblue]\n  c2a [label = \"Control Manipulation\"]\n  c2b [label = \"Control Manipulation\"]\n  \n  node [fillcolor = white]\n  m1 [label =  \"Measure\"]\n  m2 [label =  \"Measure\"]\n  m3 [label =  \"Measure\"]\n  m4 [label =  \"Measure\"]\n  \n  # edge definitions with the node IDs\n  a -> c1a -> m1\n  b -> c1b -> m2\n  c -> c2b -> m4\n  d -> c2a -> m3\n  }","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p class="caption marginnote shownote">
Figure 8.3: A between-participants design.
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-2"></span>
<div id="htmlwidget-2e17466358a46d139971" style="width:50%;height:100%;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-2e17466358a46d139971">{"x":{"diagram":"digraph {\n  graph [layout = dot, rankdir = LR]\n  \n  node [shape = rectangle, style = filled, fillcolor = white]        \n  a [label = \"Participant 1\"]\n  b [label = \"Participant 2\"]\n  c [label = \"Participant 3\"]\n  d [label = \"Participant 4\"]\n  \n  node [fillcolor = pink]\n  c1a [label = \"Experimental Manipulation\"]\n  c2a [label = \"Experimental Manipulation\"]\n  c3a [label = \"Experimental Manipulation\"]\n  c4a [label = \"Experimental Manipulation\"]\n  \n  node [fillcolor = lightblue]\n  c1b [label = \"Experimental Manipulation\"]\n  c2b [label = \"Experimental Manipulation\"]\n  c3b [label = \"Experimental Manipulation\"]\n  c4b [label = \"Experimental Manipulation\"]\n  \n  node [fillcolor = white]\n  m1a [label =  \"Measure\"]\n  m2a [label =  \"Measure\"]\n  m3a [label =  \"Measure\"]\n  m4a [label =  \"Measure\"]\n  m1b [label =  \"Measure\"]\n  m2b [label =  \"Measure\"]\n  m3b [label =  \"Measure\"]\n  m4b [label =  \"Measure\"]\n  \n  # edge definitions with the node IDs\n  a -> c1a -> m1a -> c1b -> m1b\n  b -> c2a -> m2a -> c2b -> m2b\n  c -> c3b -> m3a -> c3a -> m3b\n  d -> c4b -> m4a -> c4a -> m4b\n  }","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p class="caption marginnote shownote">
Figure 8.4: A within-participants design, counterbalanced for order.
</p>
</div>
<p>The decision whether to measure a particular factor between- or within-participants is consequential because people vary. Imagine we‚Äôre estimating our treatment effect as before, simply by computing <span class="math inline">\(\widehat{\Delta} = \widehat{\theta}_{T} - \widehat{\theta}_{C}\)</span> with each of these estimates from different populations of participants. In this scenario, our estimate <span class="math inline">\(\widehat{\Delta}\)</span> contains three components: 1) the true differences between <span class="math inline">\(\theta_{T}\)</span> and <span class="math inline">\(\theta_{C}\)</span>, 2) sampling-related variation in which participants from the population ended up in the samples for the two conditions, and 3) measurement error. Component #2 is present because any two samples of participants from a population will differ in their average on a measure ‚Äì this is precisely the kind of sampling variation we saw in the null distributions in Chapter <a href="5-inference.html#inference">5</a>.</p>
<p>When our experimental design is within-participants, component #2 is not present, because participants in both conditions are sampled from the <em>same</em> population. If we get unlucky and all of our participants are lower than the population mean on our measure, that unluckiness affects our conditions equally. We discuss the specific consequences for sample size calculations in the next chapter but the consequences are fairly extreme. Between-participants designs typically require between two and eight times as many participants as within-participants designs!</p>
<p>Given these advantages, why would you consider using a between-participants design? A within-participants design is simply not possible for all experiments. For example, consider a medical intervention like an experimental surgical procedure. Patients likely cannot receive both two procedures, and so no within-participant comparison of procedures is possible.</p>
<p>Most treatment conditions in the behavioral sciences are not so extreme, but it may be impractical or inadvisable to deliver multiple conditions. <span class="citation">(<a href="#ref-greenwald1976" role="doc-biblioref"><strong>greenwald1976?</strong></a>)</span> distinguishes three types of undesirable effects: <strong>practice</strong>, <strong>sensitization</strong>, and <strong>carry-over</strong> effects<label for="tufte-sn-11" class="margin-toggle sidenote-number">11</label><input type="checkbox" id="tufte-sn-11" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">11</span> We tend to think of all of these as being forms of carry-over effect, and sometimes use this as a catch-all description. Some people also use the picturesque description <a href="https://statmodeling.stat.columbia.edu/2017/11/25/poisoning-well-within-person-design-whats-risk/">‚Äúpoisoning the well‚Äù</a> ‚Äì earlier conditions ‚Äúruin‚Äù the data for later conditions.</span>:</p>
<ul>
<li>Practice effects occur when administering the measure or the treatment will lead to improvement. Imagine a curriculum intervention for teaching a math concept via a novel presentation ‚Äì it would be hard to convince participating schools to teach the same topic to students twice, and the effect of the second round of teaching would likely be quite different than the first!</li>
<li>Sensitization effects occur when seeing two versions of an intervention mean that you might respond differently to the second than the first because you have compared them and noticed the contrast. Greenwald‚Äôs example is of a study on room lighting ‚Äì if the experimenters are constantly changing the lighting, participants may become aware that this is the point of the study.</li>
<li>Carry-over effects refer to the case where one treatment might have a longer-lasting effect than the measurement period. For example, imagine a study in which one treatment was to make participants frustrated with an impossible puzzle; if a second condition were given after this first one, participants might still be frustrated, leading to spill-over.</li>
</ul>
<p>All of these issues can lead to real concerns with respect to within-subject designs. On the other hand, following <a href="https://statmodeling.stat.columbia.edu/2017/11/25/poisoning-well-within-person-design-whats-risk/">Gelman‚Äôs</a> guidance, we worry that the desire for effect estimates that are completely unbiased by these concerns may lead to the overuse of between-participant designs. As we mentioned above, these designs come at a major cost in terms of power and precision. An alternative approach is simply to acknowledge the possibility of carry-over type effects and plan to analyze these within your statistical model (for example by estimating the interaction of condition and order).</p>
<p>We summarize the state of affairs from our perspective in Figure <a href="#design-between-within"><strong>??</strong></a>. From our perspective, within-participant designs should be preferred whenever possible.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:design-between-within"></span>
<img src="images/design/between-within.png" alt="Pros and cons of between- vs. within-participant designs. We recommend within-participant designs when possible." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 8.5: Pros and cons of between- vs.¬†within-participant designs. We recommend within-participant designs when possible.<!--</p>-->
<!--</div>--></span>
</p>
</div>
<div id="repeated-measurements-and-experimental-items" class="section level3" number="8.1.4">
<h3><span class="header-section-number">8.1.4</span> Repeated measurements and experimental items</h3>
<p>We just discussed decision-making about whether to administer multiple <em>manipulations</em> to a single participant. The exactly analogous decision comes up for <em>measures</em>! And our take-home will be similar: unless there are specific difficulties that come up, it‚Äôs usually a very good idea to take multiple measurements from each participant, in what is called ‚Äì sensibly ‚Äì a <strong>repeated measures</strong> design.<label for="tufte-sn-12" class="margin-toggle sidenote-number">12</label><input type="checkbox" id="tufte-sn-12" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">12</span> We‚Äôre of course talking about taking mutliple measurements of the same construct! This is different from taking multiple measures of different constructs. As we discussed in Chapter <a href="7-measurement.html#measurement">7</a>, we tend to be against measuring lots of different things in a single experiment ‚Äì in part because of the concerns that we‚Äôre articulating in this chapter: if you have time, it‚Äôs better to make more precise measures of the one construct you care about most. Measuring one thing well is hard enough. Much better to do that than to measure many constructs badly.</span></p>
<p>In the last subsection, we described how variability in our estimates in a between-participants design depend on three components: 1) true condition differences, 2) sampling variation between conditions, and 3) measurement error. (The within-participants design is good because it doesn‚Äôt have #2). Repeated measures designs help with measurement error. The more times you measure, the lower your measurement error. This is the the same thing as reliability, which we covered in Chapter <a href="7-measurement.html#measurement">7</a>. If you have a perfectly reliable measure, there is no measurement error. Repeated measurements increase reliability.</p>
<p>The simplest way you can do a repeated measures design is by administering your treatment and then administering your measure ‚Äì multiple times. This scenario is pictured in a between-participants design in Figure <a href="8-design.html#fig:design-rm-between">8.6</a>. Somtimes this works quite well. For example, imagine a transcranial magnetic stimulation (TMS) experiment: participants receive neural stimulation for a period of time, targeted at a particular region. Then they perform some measurement task repeatedly until it wears off. The more times they perform it, the better the estimate of whatever effet (when compared to a control of TMS to another region, say).</p>
<div class="figure"><span id="fig:design-rm-between"></span>
<div id="htmlwidget-30e4409849e39179307f" style="width:50%;height:100%;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-30e4409849e39179307f">{"x":{"diagram":"digraph {\n  graph [layout = dot, rankdir = LR]\n  \n  node [shape = rectangle, style = filled, fillcolor = white]        \n  a [label = \"Participant 1\"]\n  b [label = \"Participant 2\"]\n  c [label = \"Participant 3\"]\n  d [label = \"Participant 4\"]\n  \n  node [fillcolor = pink]\n  c1a [label = \"Experimental Manipulation\"]\n  c1b [label = \"Experimental Manipulation\"]\n  \n  node [fillcolor = lightblue]\n  c2a [label = \"Control Manipulation\"]\n  c2b [label = \"Control Manipulation\"]\n  \n  node [fillcolor = white]\n  m1a [label =  \"Measure\"]\n  m2a [label =  \"Measure\"]\n  m3a [label =  \"Measure\"]\n  m4a [label =  \"Measure\"]\n  m1b [label =  \"Measure\"]\n  m2b [label =  \"Measure\"]\n  m3b [label =  \"Measure\"]\n  m4b [label =  \"Measure\"]\n  m1c [label =  \"Measure\"]\n  m2c [label =  \"Measure\"]\n  m3c [label =  \"Measure\"]\n  m4c [label =  \"Measure\"]\n  \n  # edge definitions with the node IDs\n  a -> c1a -> m1b -> m1a -> m1c\n  b -> c1b -> m2b -> m2a -> m2c\n  c -> c2b -> m3b -> m3a -> m3c\n  d -> c2a -> m4b -> m4a -> m4c\n  }","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p class="caption marginnote shownote">
Figure 8.6: A between-participants, repeated-measures design.
</p>
</div>
<p>The catch is exactly analogous to the between-participants design: some measures can‚Äôt be repeated without altering the response. To take an obvious example, we can‚Äôt give the same math problem twice! The general solution to this problem that is typically used is the <strong>experimental item</strong>. In the case of a math assessment, you create multiple problems that you believe test the same concept but have different numbers or other superficial characteristics. This practice is widespread, and we will have a lot to say about it in the next chapter, because the use of multiple experimental items can license generalizations across a population of items in the same way that the use of multiple participants can ideally license generalizations across a population of people <span class="citation">(<a href="#ref-clark1973" role="doc-biblioref">Clark, 1973</a>)</span>.</p>
<p>One variation on the repeated measures, between-participants design is a specific version where the measure is administered both before (pre-) and after (post-) intervention, as in Figure <a href="8-design.html#fig:design-pre-post">8.7</a>. This design is sometimes known as a <strong>pre-post</strong> design. It is extremely common in cases where the intervention is larger-scale and harder to give within-participants, such as in a field experiment where a policy or curriculum is given to one sample and not to another. The pre measurements can be used to subtract participant-level variability out and recover a more precise estimate of the treatment effect. Recall that our treatment effect in a pure between participants design is <span class="math inline">\(\widehat{\Delta} = \widehat{\theta}_{T} - \widehat{\theta}_{C}\)</span>. In a pre-post design, we can do better by computing <span class="math inline">\(\widehat{\Delta} = (\widehat{\theta_{T_{post}}} - \widehat{\theta_{T_{pre}}}) - (\widehat{\theta_{C_{post}}} - \widehat{\theta_{C_{pre}}})\)</span>. We could rephrase this as "how much more did the treatment group go up than the control group?<label for="tufte-sn-13" class="margin-toggle sidenote-number">13</label><input type="checkbox" id="tufte-sn-13" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">13</span> This estimate is sometimes called a ‚Äúdifference in differences‚Äù and is very widely used in the field of econometrics, both in experimental and quasi-experimental cases <span class="citation">(<a href="#ref-cunningham21" role="doc-biblioref"><strong>cunningham21?</strong></a>)</span>.</span></p>
<div class="figure"><span id="fig:design-pre-post"></span>
<div id="htmlwidget-2e17466358a46d139971" style="width:50%;height:100%;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-2e17466358a46d139971">{"x":{"diagram":"digraph {\n  graph [layout = dot, rankdir = LR]\n  \n  node [shape = rectangle, style = filled, fillcolor = white]        \n  a [label = \"Participant 1\"]\n  b [label = \"Participant 2\"]\n  c [label = \"Participant 3\"]\n  d [label = \"Participant 4\"]\n  \n  node [fillcolor = pink]\n  c1a [label = \"Experimental Manipulation\"]\n  c1b [label = \"Experimental Manipulation\"]\n  \n  node [fillcolor = lightblue]\n  c2a [label = \"Control Manipulation\"]\n  c2b [label = \"Control Manipulation\"]\n  \n  node [fillcolor = white]\n  m1a [label =  \"Measure\"]\n  m2a [label =  \"Measure\"]\n  m3a [label =  \"Measure\"]\n  m4a [label =  \"Measure\"]\n  m1b [label =  \"Measure\"]\n  m2b [label =  \"Measure\"]\n  m3b [label =  \"Measure\"]\n  m4b [label =  \"Measure\"]\n  \n  # edge definitions with the node IDs\n  a -> m1a -> c1a -> m1b\n  b -> m2a -> c1b -> m2b\n  c -> m3a -> c2b -> m3b\n  d -> m4a -> c2a -> m4b\n  }","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p class="caption marginnote shownote">
Figure 8.7: A between-participants, pre-post design.
</p>
</div>
<p>Of course, repeated measurements are not limited to between-participants designs! The bread and butter of perception, psychophysics, and cognitive psychology is the type of within-participants repeated measure design shown in Figure <a href="8-design.html#fig:design-rm-within">8.8</a>.</p>
<div class="figure"><span id="fig:design-rm-within"></span>
<div id="htmlwidget-6e8228581aa3e4de6c04" style="width:50%;height:100%;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-6e8228581aa3e4de6c04">{"x":{"diagram":"digraph {\n  graph [layout = dot, rankdir = LR]\n  \n  node [shape = rectangle, style = filled, fillcolor = white]        \n  a [label = \"Participant 1\"]\n  b [label = \"Participant 2\"]\n  c [label = \"Participant 3\"]\n  d [label = \"Participant 4\"]\n  \n  node [fillcolor = pink]\n  c1a [label = \"Experimental Manipulation\"]\n  c1b [label = \"Experimental Manipulation\"]\n  c3a [label = \"Experimental Manipulation\"]\n  c3b [label = \"Experimental Manipulation\"]\n  \n  node [fillcolor = lightblue]\n  c2a [label = \"Control Manipulation\"]\n  c2b [label = \"Control Manipulation\"]\n  c4a [label = \"Control Manipulation\"]\n  c4b [label = \"Control Manipulation\"]\n  \n  node [fillcolor = white]\n  m1a [label =  \"Measure\"]\n  m2a [label =  \"Measure\"]\n  m3a [label =  \"Measure\"]\n  m4a [label =  \"Measure\"]\n  m1b [label =  \"Measure\"]\n  m2b [label =  \"Measure\"]\n  m3b [label =  \"Measure\"]\n  m4b [label =  \"Measure\"]\n  \n  # edge definitions with the node IDs\n  a -> c1a -> m1a -> c4a -> m1b\n  b -> c1b -> m2a -> c4b -> m2b\n  c -> c2b -> m3a -> c3b -> m3b\n  d -> c2a -> m4a -> c3a -> m4b\n  }","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p class="caption marginnote shownote">
Figure 8.8: A within-participants repeated-measures design with multiple trials.
</p>
</div>
</div>
<div id="discrete-and-continuous-experimental-manipulations" class="section level3" number="8.1.5">
<h3><span class="header-section-number">8.1.5</span> Discrete and continuous experimental manipulations</h3>
<p>Most experimental designs in most subfields of psychology use discrete condition manipulations: treatment vs.¬†control. In our view, this decision is often a lost opportunity. In our framework, the goal of an experiment is to estimate a causal effect; ideally, this estimate can be generalized to other contexts and used as a basis for theory. Measuring not just one effect but instead a <strong>dose-response</strong> relationship ‚Äì how the measure changes as the strength of the manipulation is changed ‚Äì has a number of benefits in helping to achieve this goal.</p>
<p>Many manipulations can be <strong>titrated</strong> ‚Äì that is, their strength can be varied continuously ‚Äì with a little creativity on the part of an experimenter. A curriculum intervention can be applied at different levels of intensity, perhaps by changing the number of sessions in which it is taught. For a priming manipulation, the frequency or duration of prime stimuli can be varied. Two stimuli can be morphed continuously so that categorization boundaries can be examined.<label for="tufte-sn-14" class="margin-toggle sidenote-number">14</label><input type="checkbox" id="tufte-sn-14" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">14</span> These methods are extremely common in perception and psychophysics research, in part because the dimensions being studied are often continuous in nature. For exmaple, imagine trying to estimate a subject‚Äôs visual contrast sensitivity <em>without</em> continuously manipulating the contrast of the stimulus, eliciting judgments at many different levels.</span></p>
<div class="figure"><span id="fig:design-dose-schema"></span>
<p class="caption marginnote shownote">
Figure 8.9: Three schematic designs. (A) Control and treatment are two levels of a nominal variable. (B) Control is compared to ordered levels of a treatment. (C) Treatment level is an interval or ratio variable such that points can be connected and a parametric curve can be extrapolated.
</p>
<img src="images/design/dose-response.png" alt="Three schematic designs. (A) Control and treatment are two levels of a nominal variable. (B) Control is compared to ordered levels of a treatment. (C) Treatment level is an interval or ratio variable such that points can be connected and a parametric curve can be extrapolated." width="\linewidth"  />
</div>
<p>Dose-response designs are useful because they provide insight into the shape of the function mapping your manipulation to your measure. Knowing this shape can inform your theoretical understanding! Consider the examples given in Figure <a href="#design-dose-schema"><strong>??</strong></a>. If you only have two conditions in your experiment, then the most you can say about the relationship between your manipulation and your measure is that it produces an effect of a particular magnitude; in essence, you are assuming that condition is a nominal variable. If you have multiple ordered levels of treatment, you can start to speculate about the nature of the relationship between treatment and effect magnitude. But if you can measure the strength of your treatment, then you can start to describe the nature of the relationship between the strength of treatment and strength of effect via a parametric function (e.g., a linear regression, a sigmoid, or other function).<label for="tufte-sn-15" class="margin-toggle sidenote-number">15</label><input type="checkbox" id="tufte-sn-15" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">15</span> These assumptions are theory-laden, of course ‚Äì the choice of a linear function or a sigmoid is not necessary: nothing guarantees that simple, smooth, or monotonic functions are the right ones. The important point from our perspective is that choosing a function makes explicit your assumptions about the nature of the treatment-effect relationship.</span> These parametric functions can in turn allow you to generalize from your experiment, making predictions about what would happen under intervention conditions that you didn‚Äôt measure directly!</p>
<p>This all can feel a bit abstract, so let‚Äôs consider an example. <span class="citation">(<a href="#ref-brennan1966" role="doc-biblioref"><strong>brennan1966?</strong></a>)</span> were interested in the relationship between visual complexity and infants‚Äô looking preferences. Do infants uniformly prefer complex stimuli, or do they search for stimuli at an appropriate level of complexity for their processing abilities. To test this hypothesis, they exposed infants in three different age groups (3, 8, and 14 weeks, N=30) to black and white checkerboard stimuli with three different levels of complexity (2x2, 8x8, and 24x24). Their findings are plotted in Figure <a href="#design-dose-ex"><strong>??</strong></a>: the youngest infants preferred the simplest stimuli, while infants at an intermediate age preferred stimuli of intermediate complexity, and the oldest infants preferred the most complex stimuli. These findings help to motivate the theory that infants attend preferentially to stimuli that provide appropriate learning input for their processing ability <span class="citation">(<a href="#ref-kidd2012" role="doc-biblioref">Kidd et al., 2012</a>)</span>.<label for="tufte-sn-16" class="margin-toggle sidenote-number">16</label><input type="checkbox" id="tufte-sn-16" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">16</span> <span class="citation">(<a href="#ref-brennan1966" role="doc-biblioref"><strong>brennan1966?</strong></a>)</span>‚Äôs experiment uses a quantitative manipulation of complexity (checkerboard density). But they treat this manipulation as an ordinal ‚Äì rather than an interval ‚Äì variable, presumably because they do not know precisely how checkerboard density translates into changes in the psychological construct they care about, namely complexity. Does doubling the number of squares on the board double complexity, or do you have to double the number of squares on each side? Does complexity saturate, such that a 128x128 checkerboard is not much more complex? These are just a few of the questions that the dose-response approach raises.</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:design-dose-ex"></span>
<img src="experimentology_files/figure-html/design-dose-ex-1.png" alt="Infants' looking time, plotted by stimulus complexity and infant age. Data from Brennan et al., 1966." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 8.10: Infants‚Äô looking time, plotted by stimulus complexity and infant age. Data from Brennan et al., 1966.<!--</p>-->
<!--</div>--></span>
</p>
<p>If your goal is simply to detect whether an effect is zero or non-zero, then dose-response designs do not achieve the maximum statistical power. For example, if <span class="citation">(<a href="#ref-brennan1966" role="doc-biblioref"><strong>brennan1966?</strong></a>)</span> simply wanted to achieve maximal statistical power, they probably should have only tested two age groups and two levels of complexity (say, 3 and 14 week infants and 2x2 and 24x24 checkerboards). That would have been enough to show an interaction of complexity and age, and their greater resources devoted to these four (as opposed to nine) conditions would mean more precise estimates of each (simulated in Figure <a href="8-design.html#fig:design-dose-ex-2">8.11</a>). But their findings would be less clearly supportive of the view that infants prefer stimuli that are appropriate to their processing ability. By seeking to measure intermediate conditions, they provided a stronger constraint on their theory.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:design-dose-ex-2"></span>
<img src="experimentology_files/figure-html/design-dose-ex-2-1.png" alt="Imagining the data from Brennan et al. 1966 if they had omitted intermediate conditions in search of the most extreme effect." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 8.11: Imagining the data from Brennan et al.¬†1966 if they had omitted intermediate conditions in search of the most extreme effect.<!--</p>-->
<!--</div>--></span>
</p>
<!-- Continuous and discrete variables -->
<!-- Dose response relationships  -->
<!-- ::: {.case-study} -->
<!-- üî¨ Case study: Still suspicious? -->
<!-- The ‚Äúsuspicious coincidence‚Äù effect (Xu and Tenenbaum 2007) with non-replication by Spencer et al. (2011) resolved by Lewis & Frank (2018) ‚Äústill suspicious‚Äù paper.  -->
<!-- ::: -->
</div>
</div>
<div id="manipulation-validity" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Manipulation validity</h2>
<div id="threats-to-manipulation-validity" class="section level3" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Threats to manipulation validity</h3>
<ul>
<li><p>Manipulations must correspond to the construct whose causal effect is being estimated.</p></li>
<li><p>Demand characteristics. How did concerns about demand characteristics emerge? What proposed mechanisms cause demand characteristics to influence participant behavior? What evidence do we have that demand characteristics impact participant behavior? And what strategies can we use to mitigate demand characteristics?</p></li>
</ul>
<p>Ecological validity</p>
</div>
<div id="confounding" class="section level3" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Confounding</h3>
<p>Let‚Äôs consider an alternate experiment now. Suppose we did the same basic procedure, but now with a ‚Äúwithin-subjects‚Äù design where participants do both the Dylan treatment and the control, in that order. This experiment is flawed, of course. If you observe a Dylan effect, you can‚Äôt rule out the idea that participants got tired and wrote worse in the control condition because it always came second.</p>
<p>Order (Dylan first vs.¬†control first; notated X‚Äô) is an experimental confound: a variable that is created in the course of the experiment that is both causally related to the predictor and potentially also related to the outcome. Here‚Äôs how the causal model now looks:</p>
<p>We‚Äôve reconstructed the same kind of confounding relationship we had with age, where we had a variable (X‚Äô) that was correlated both with our predictor (X) and our outcome (Y)! So‚Ä¶</p>
<p>In the causal language we have been using, counterbalancing allows us to snip out the causal dependency between order and Dylan. Now they are unconfounded (uncorrelated) with one another. We‚Äôve ‚Äúsolved‚Äù a confound in our experimental design. Here‚Äôs the picture:</p>
<p>These are not covariates! Covariates are related but don‚Äôt have causal force in this design because of randomization. We can use them in our analysis to make our estimates more precise (see Chapter <a href="6-models.html#models">6</a>), but we won‚Äôt worry about them here. If someone says to you, ‚Äúparticipant gender is a confound in your experiment,‚Äù if you‚Äôve done random assignment to condition appropriately (acoss genders), you should say ‚Äúno it‚Äôs not.‚Äù</p>
</div>
<div id="removing-nuisance-confounds" class="section level3" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> Removing nuisance confounds</h3>
<p>What should we do with our experimental confounds?</p>
<p>Option 1. Randomize. Increasingly, this is my go-to method for dealing with any confound. Is the correct answer on my survey confounded with response side? Randomize what side the response shows up on! Is order confounded with condition? Randomize the order you present in! Randomization is much easier now that we program many of our experiments using software like Qualtrics or code them from scratch in JavaScript.</p>
<p>The only time you really get in trouble with randomization is when you have a large number of options, a small number of participants, or some combination of the two. In this case, you can end up with unbalanced levels of the randomized factors (for example, ten answers on the right side and two on the left). Averaging across many experiments, this lack of balance will come out in the wash. But in a single experiment, it can really mess up your data ‚Äì especially if your participants notice and start choosing one side more than the other because it‚Äôs right more often. For that reason, when balance is critical, you want option 2.</p>
<p>Option 2. Counterbalance. If you think a particular confound might have a significant effect on your measure, balancing it across participants and across trials is a very safe choice. That way, you are guaranteed to have no effect of the confound on your average effect. In a simple counterbalance of order for our Dylan experiment, we manipulate condition order between subjects. Some participants hear Dylan first and others hear Dylan second. Although technically we might call order a second ‚Äúfactor‚Äù in the experiment, in practice it‚Äôs really just a nuisance variable, so we don‚Äôt talk about it as a factor and we often don‚Äôt analyze it (but see Option 3 below).</p>
<p>Counterbalancing doesn‚Äôt always work, though. It gets trickier when you have too many levels on a variable (too many Dylan songs!) or multiple confounding variables. For example, if you have lots of different nuisance variables ‚Äì say, condition order, what writing prompt you use for each order, which Dylan song you play ‚Äì it may not be possible to do a fully-crossed counterbalance so that all combinations of these factors are seen by equal numbers of participants. In these kinds of cases, you may have to rely on partial counterbalancing schemes or latin squares designs, or you may have to fall back on randomization.</p>
<p>Option 3. Do Options 1 and 2 and then model the variation. This option was never part of my training, but it‚Äôs an interesting third option that I‚Äôm increasingly considering.** That is, we are often faced with the choice between A) a noisy between-participants design and B) a lower-noise within-participants design that nevertheless adds noise back in via some obvious order effect that you have to randomize or counterbalance. In a recent talk by Andrew Gelman, he suggested that we try to model these as covariates, to reduce noise. This seems like a pretty interesting suggestion, especially if the correlation between them and the outcome is substantial.***</p>
</div>
</div>
<div id="strategy" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Strategy</h2>
<div id="how-to-design-a-manipulation-to-test-a-theory." class="section level3" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> How to design a manipulation to test a theory.</h3>
<p>Simplicity as a key design principle</p>
<ul>
<li>E. O. Wilson‚Äôs advice: iteration on a repeatable measurement.</li>
<li>Statistical and interpretability concerns for complex interaction designs.</li>
<li>Nuisance variables: counterbalancing and randomization.</li>
</ul>
<p>The temptation to manipulate lots of things</p>
<p>Again, we advocate for simplicity.</p>
<p>The advice is out there to</p>
</div>
<div id="connecting-with-theory" class="section level3" number="8.3.2">
<h3><span class="header-section-number">8.3.2</span> Connecting with theory</h3>
<ul>
<li>The ethics of the ‚Äúdead on arrival‚Äù experiment ‚Äì why appropriate experimental design is an ethical imperative (we ‚Äúwaste‚Äù participant contributions otherwise).
‚Äúrisky tests‚Äù: those that will best help adjudicate between theories. <span class="citation"><a href="#ref-meehl1978" role="doc-biblioref">Meehl</a> (<a href="#ref-meehl1978" role="doc-biblioref">1978</a>)</span></li>
</ul>
<p>Optimal experiment design in psychophysics and beyond ‚Äì how to use quantitative models to select the stimulus that maximizes your chances of a theory-informing result.</p>
<div class="ethics-box">
<p>üåø Ethics box: Including the population being sampled in the design process.</p>
</div>
<p>Multi-experiment design strategy: start simple?</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-clark1973" class="csl-entry">
Clark, H. H. (1973). The language-as-fixed-effect fallacy: A critique of language statistics in psychological research. <em>Journal of Verbal Learning and Verbal Behavior</em>, <em>12</em>(4), 335‚Äì359.
</div>
<div id="ref-kidd2012" class="csl-entry">
Kidd, C., Piantadosi, S. T., &amp; Aslin, R. N. (2012). The goldilocks effect: Human infants allocate attention to visual sequences that are neither too simple nor too complex. <em>PloS One</em>, <em>7</em>(5), e36399.
</div>
<div id="ref-mcclelland1993" class="csl-entry">
McClelland, G. H., &amp; Judd, C. M. (1993). Statistical difficulties of detecting interactions and moderator effects. <em>Psychological Bulletin</em>, <em>114</em>(2), 376.
</div>
<div id="ref-meehl1978" class="csl-entry">
Meehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir karl, sir ronald, and the slow progress of soft psychology. <em>J. Consult. Clin. Psychol.</em>, <em>46</em>(4), 806‚Äì834.
</div>
<div id="ref-myung2009" class="csl-entry">
Myung, J. I., &amp; Pitt, M. A. (2009). Optimal experimental design for model discrimination. <em>Psychological Review</em>, <em>116</em>(3), 499.
</div>
<div id="ref-young2007" class="csl-entry">
Young, L., Cushman, F., Hauser, M., &amp; Saxe, R. (2007). The neural basis of the interaction between theory of mind and moral judgment. <em>Proceedings of the National Academy of Sciences</em>, <em>104</em>(20), 8235‚Äì8240.
</div>
</div>
<p style="text-align: center;">
<a href="7-measurement.html"><button class="btn btn-default">Previous</button></a>
<a href="9-sampling.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="toc/toc.js"></script>


</body>
</html>
