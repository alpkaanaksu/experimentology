<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 3 Ethics | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 3 Ethics | Experimentology">

<title>Chapter 3 Ethics | Experimentology</title>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.18/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="toc/toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-intro.html#intro"><span class="toc-section-number">1</span> Experiments and theories</a></li>
<li><a href="2-replication.html#replication"><span class="toc-section-number">2</span> Replication and reproducibility</a></li>
<li><a href="3-ethics.html#ethics"><span class="toc-section-number">3</span> Ethics</a></li>
<li class="part"><span><b>II Statistics</b></span></li>
<li><a href="4-estimation.html#estimation"><span class="toc-section-number">4</span> Estimation</a></li>
<li><a href="5-inference.html#inference"><span class="toc-section-number">5</span> Inference</a></li>
<li><a href="17-meta.html#meta"><span class="toc-section-number">17</span> Meta-analysis</a></li>
<li><a href="18-conclusions.html#conclusions"><span class="toc-section-number">18</span> Conclusions</a></li>
<li class="part"><span><b>III Appendices</b></span></li>
<li><a href="19-git.html#git"><span class="toc-section-number">19</span> GitHub Tutorial</a></li>
<li><a href="20-rmarkdown.html#rmarkdown"><span class="toc-section-number">20</span> R Markdown Tutorial</a></li>
<li><a href="21-tidyverse.html#tidyverse"><span class="toc-section-number">21</span> Tidyverse Tutorial</a></li>
<li><a href="22-ggplot.html#ggplot"><span class="toc-section-number">22</span> ggplot Tutorial</a></li>
<li><a href="23-instructors.html#instructors"><span class="toc-section-number">23</span> Instructor‚Äôs guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="ethics" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Ethics</h1>
<div class="learning-goals">
<p>üçé Learning goals:</p>
<ul>
<li>Distinguish between deontological, utilitarian, and virtue ethics approaches to research</li>
<li>Identify key tenets of experimental vs.¬†publication ethics</li>
<li>Describe the ethical duty for transparency</li>
</ul>
</div>
<div class="case-study">
<p>üî¨ Case study: Diederick Stapel
Dutch social psychologist, Diederick Stapel, was a research powerhouse; he authored and co-authored well over 200 articles on social comparison, stereotype threat, and discrimination and published his findings in dozens of prestigious journals <strong>[<a href="https://www.apa.org/science/about/psa/2011/12/diederik-stapel" class="uri">https://www.apa.org/science/about/psa/2011/12/diederik-stapel</a>]</strong>. It was Stapel who reported that affirming positive personal qualities buffered against dangerous social comparison, that product advertisements related to a person‚Äôs attractiveness changed the manner and frequency with which they evaluated their own sense of self, and that exposure to intelligent in-group members boosted a person‚Äôs performance on an upcoming task <span class="citation">(<a href="#ref-stapel2012" role="doc-biblioref">Diederik A. Stapel &amp; Linde, 2012</a>; <a href="#ref-trampe2011" role="doc-biblioref">Trampe et al., 2011</a>; <a href="#ref-gordijin2012" role="doc-biblioref"><strong>gordijin2012?</strong></a>)</span>. These findings were fresh and noteworthy at the time of publication; Stapel‚Äôs papers were cited more than 11,000 times. The only problem? Much of Stapel‚Äôs data were fraudulent.</p>
<p>When Stapel first began fabricating his data, he admitted to making small tweaks to a few cells in his data <span class="citation">(<a href="#ref-stapel2012b" role="doc-biblioref">Diederik Alexander Stapel, 2012</a>)</span>. Changing a single number here and there would turn a flat study into an impressive one. Having achieved comfortable success (and garnering little suspicion from journal editors and others in the scientific community), Stapel eventually began altering entire data sets and passing them off as his own. But in 2011, several colleagues began to grow skeptical of his overwhelming success and brought their concerns to the Psychology Department at Tilburg University. Sure enough, his findings were, in fact, too good to be true. Stapel‚Äôs dirty little secret had quickly become front-page news in the scientific community and his entire career was teetering on the brink of collapse. At the time of this textbook‚Äôs release, 58 of Diederick Stapel‚Äôs papers were <strong>retracted</strong>, or withdrawn from the journals they were originally published.</p>
<p>It might be tempting to conclude that Diederick Stapel‚Äôs case was a highly unusual phenomenon, one that could never be duplicated. Unfortunately, scientific misconduct is more common than you might think. In fact, one recent study revealed that 1 in 12 Dutch scientists admitted to committing one or more serious forms of research misconduct <span class="citation">(<a href="#ref-devrieze2021" role="doc-biblioref">Vrieze, 2021</a>)</span>. What‚Äôs more, nearly half of assistant and associate Dutch professors who completed a survey on research misconduct reported tampering with their data in the last three years. This is alarming for our field and illuminates the pervasiveness of the problem. Fortunately, you can decide to conduct honest and ethically-centered research. Don‚Äôt fabricate your work. Do make an ethical commitment to research. In this chapter, we‚Äôll start discussing how.</p>
</div>
<div id="ethical-frameworks" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Ethical Frameworks</h2>
<p>In this section, we will discuss three ethical frameworks to guide our research pursuits. This is not meant to be an exhaustive list, and you are encouraged to consider other frameworks not mentioned here.</p>
<div id="the-deontological-approach" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> The Deontological Approach</h3>
<p>If you were ever instructed as a child to ‚Äúrespect your elders,‚Äù congratulations! You have been introduced to <strong>deontological ethics</strong>. Also known as duty-based ethics, deontology emphasizes the importance of morality in deed, regardless of the outcome <span class="citation">(<a href="#ref-Biagetti2020" role="doc-biblioref"><strong>Biagetti2020?</strong></a>)</span>. 16th century philosopher and Deontology‚Äôs most cited figure, Immanuel Kant, believed that there exists universal moral laws everyone must obey, and that following these laws elevates us to rational agents. For example, The Ten Commandments, a biblical account in which the Jewish God (? is this the right way to word this) instructed his servant, Moses, to write down ten rules for the Israelite (? Israelite in the Bible, but better to say Israelis?) people to obey, is a prime example of deontological ethics <strong><span class="citation">(<a href="#ref-English" role="doc-biblioref"><strong>English?</strong></a> Standard Version Bible. (2001). ESV Online. <a href="https://esv.literalword.com/" class="uri" role="doc-biblioref">https://esv.literalword.com/</a>)</span></strong>. Commands such as ‚ÄúThou shalt not steal‚Äù and ‚ÄúThou shalt not kill‚Äù offered clear instructions for the Israelite people to obey. To Kant, deontology provided a means of establishing order without the confusion of evaluating individual outcomes. This line of thinking is akin to prioritizing ‚Äúintent‚Äù over ‚Äúimpact.‚Äù It is important to remember that deontological ethics is unconcerned with individual needs, wants, desires, and goals, which means that morality is a function of both duty and action. Thinking back to our initial example that opened this paragraph, if we asked why we needed to respect our elders, we may have been told it was ‚Äúthe right thing to do.‚Äù</p>
<p>Deontology is primarily concerned with four ethical tenets:</p>
<ol style="list-style-type: decimal">
<li><strong>Respect for autonomy</strong>. This means that people participating in research studies can make their own decisions about their participation, and that those with diminished autonomy (children, neuro-divergent people, etc.) should receive equal protections <span class="citation">(<a href="#ref-beauchamp2001" role="doc-biblioref">Beauchamp et al., 2001</a>)</span>. Respecting someone‚Äôs autonomy also means providing them with all the information they need to make an informed decision about their participation in a research study without coercion. In short, we respect that people can make the best decision for themselves when provided with the appropriate information related to that decision.</li>
<li><strong>Beneficence</strong>. This means that researchers are obligated (not simply suggested) to protect the well-being of participants for the duration of the study. Beneficence is broken down into two actions based on writing from Greek physician, Hippocrates. The first is to do no harm. Researchers must take steps to minimize the risks to participants and to disclose any known risks at the onset. If risks are discovered during participation, researchers must notify participants of their discovery and make reasonable efforts to mitigate these risks, even if that means stopping the study altogether. The second is to maximize potential benefits. This doesn‚Äôt mean compensating participants with exorbitant amounts of money or gifts, it just means identifying all possible benefits and making them available where possible. For example, a study that explores the impact of daily journaling on depressive symptoms in adolescents could maximize benefits by inviting participants in the control or placebo conditions to also try journaling at the end of the study if it was shown to be effective at reducing depressive symptoms.</li>
<li><strong>Nonmaleficence</strong>. This principle is similar to beneficence (in fact, beneficence and nonmaleficence were a single principle when they were first introduced in the <strong>Belmont Report</strong>, which we‚Äôll discuss later) but differs in it‚Äôs emphasis on doing/causing no harm. But remember, deontology is about intent, not impact, so harm is sometimes warranted when the intent is morally good. For example, administering a vaccine may cause some discomfort and pain, but the intent is to protect the patient from developing a deadly virus in the future. The harm is justifiable under this framework. In social science research, we might ask participants to think about a painful memory or experience to understand how emotions can be better regulated, which may temporarily bring discomfort and sadness but ultimately improve emotion research.</li>
<li><strong>Justice</strong>. This means that both the benefits and risks of a study should be equally distributed among all participants. Justice can be based on multiple criteria, including need, effort, contribution, and merit. In human subjects research we do not base justice on effort, contribution, or merit because doing so would violate equal access to participation in research. This is also true in how participants are assigned to study conditions. In general, participants should not be systematically assigned to one condition over another due to features they arrive to the study with, like socioeconomic status, race and ethnicity, or gender. The caveat here is when there is sufficient evidence or presumption that these factors affect people in systematic ways. Even then, distributive justice is of the utmost importance.</li>
</ol>
<p>This approach to ethics works well until it doesn‚Äôt. Imagine you have a grandparent who says that people who don‚Äôt wear seat belts should not receive medical attention if they are involved in an auto accident. What‚Äôs more, your grandparent is on the local city council board and has decided to pass a city ordinance to prevent paramedics from assisting anyone suspected of not wearing their seat belt. Suddenly, you are faced with incompatible goals; although you have learned to respect your elders, you also believe that people who are involved in auto accidents deserve medical attention, regardless of whether or not they were wearing a seat belt at the time of impact. Deontological ethics deals only in absolutist terms, meaning that there exists one set of standards to follow and that there can be no gray area. Deontology cannot solve your complicated moral dilemma, and you are no closer to arriving at a solution.</p>
</div>
<div id="the-utilitarian-approach" class="section level3" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> The Utilitarian Approach</h3>
<p>Now imagine you feel so strongly about this issue of who should receive medical attention during an auto accident that you cannot help but speak up to your grandparent and attempt to change their mind. While you respect their opinion, you are disappointed by their decision and plan to express this disappointment. How do you reconcile a moral obligation to respect your elders with a separate moral obligation to support the well-being of others? In this case, you might consider a utilitarian approach. <strong>Utilitarian ethics</strong> emphasizes an action‚Äôs outcome as the marker for whether or not to engage in that action <span class="citation">(<a href="#ref-flinders1992" role="doc-biblioref">Flinders, 1992</a>)</span>. Particularly, utilitarianism asks whether the greatest number of people receive the greatest benefit from some action. If yes, proceed. If no, consider alternative solutions that maximize positive outcomes for more people.</p>
<p>Utilitarian ethics as we have come to understand it today was heavily influenced from work by John Stuart Mill, a 19th century British philosopher and economist. Mill likened utilitarianism to something called the <strong>Greatest Happiness Principle</strong>, or the idea that an action should be considered morally good based on the degree of happiness or pleasure people experience because of it, and that an action should be considered morally bad based on the degree of unhappiness or pain people experience by the same action <span class="citation">(<a href="#ref-mill1859" role="doc-biblioref">Mill, 1859</a>)</span>. When Mill wrote the book, <em>Utilitarianism</em>, many of his critics challenged this idea that pleasure could adequately define morality because it was considered subjective and, by extension, impractical. In response, Mill (thoroughly irritated by his colleagues‚Äô ‚Äòsimple-mindedness‚Äô) concluded that most humans would converge on the same maximally pleasurable and ‚Äúnoble‚Äù actions, even if some community members would seek pleasure in actions that offer the potential of harm to others. To these critics, he writes, ‚Äúutilitarianism‚Ä¶could only attain it‚Äôs end by the general cultivation of nobleness of character, even if each individual were only benefited by the nobleness of others, and his own, so far, as happiness is concerned, were a sheer deduction from the benefit.‚Äù In other words, utilitarianism works in societies where most people seek pleasure in actions that cause little or no harm to others. In the case of your grandparent, utilitarian ethics would support your decision to speak out against an ordinance that harms some victims of auto accidents.</p>
<p>By this point, you might have some healthy doubts about pursuing a utilitarian framework, even in our imaginary example about your grandparent and victims of auto accidents. You might be thinking, ‚ÄúWhat if I live in a community of people with more mixed or, worse, more harmful views?‚Äù Utilitarianism is based on an assumption that people are generally good and that actions which bring about the most pleasure are generally positive. Essentially, it relies on a ‚Äúmajority rules‚Äù policy to function successfully. This is a reasonable approach when a community is relatively homogeneous in thinking or when the action is popular among most people. But as you may have gathered by now, a utilitarian framework is ineffective when actions bring about mixed feelings or when there is significant disagreement of individual goals. Although Mill was confident that societies would converge on moral outcomes, we must also take into account what effects the minority community members may face under utilitarianism. ‚ÄúCasualties‚Äù are inevitable when the focus is on the greater society, but the means justify the ends <span class="citation">(<a href="#ref-garbutt2011" role="doc-biblioref">Garbutt &amp; Davies, 2011</a>)</span>. Given what we‚Äôve discussed so far, what viable alternatives are out there?</p>
</div>
<div id="the-virtue-approach" class="section level3" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> The Virtue Approach</h3>
<p>Another way we can approach this ethical dilemma is through a virtue framework. You have probably heard the phrase, ‚Äúpatience is a virtue‚Äù more than you care to remember, and that is probably because we have cared a lot about virtues for a very long time. From Aristotle to Voltaire, Churchill to Baldwin, and many others in between, we place a premium on virtuous actions. Before we dive any further, let‚Äôs first discuss what we mean by virtue. A <strong>virtue</strong> is a trait, a disposition, or a quality that is thought to be a moral foundation <span class="citation">(<a href="#ref-annas2006" role="doc-biblioref">Annas, 2006</a>)</span>. Someone who regularly collects and hands out coats to refugees in their community may be thought of as having a virtue of compassion while someone else who always seeks to tell the truth no matter the consequences may be thought of as having a virtue of honesty. These virtues are both learned from and revered by a society, which only reinforces their importance.</p>
<p>One essential feature of virtue ethics is that people can learn to be virtuous by observing those actions in others they admire <span class="citation">(<a href="#ref-morris2016" role="doc-biblioref">Morris &amp; Morris, 2016</a>)</span>. Proponents of virtue ethics say this works for two reasons: (1) people are generally good at recognizing morally good traits in others and (2) people receive some fulfillment in living virtuously. This is different from utilitarianism because it focuses on the actions and character of the person rather than on the consequences the action brings to the majority of society. Let‚Äôs return to the auto accident example we discussed earlier to illustrate this idea. How would you approach your grandparent about their views of victims of auto accidents under this ethical framework? If you want to think like a virtue theorist, you might consider the question, ‚ÄúWhat do I need to do to lead a good life?‚Äù If what matters to you are virtues of compassion and kindness, you will share your disappointment with your grandparent‚Äôs decision and will challenge the city ordinance they put in place. But if you value honor and respect above all else, you might decide not to speak up in this case. Are you seeing the difference now? Virtue ethics emphasize the goals of the individual and relies on peoples‚Äô desires to do the ‚Äúright‚Äù thing as they see it.</p>
<p>It might be easy to see the potential downsides to this framework when we think about the variety of goals that drive our decisions. No clearer do we see these differences than in the American political context. American politics is primarily divided into two parties- Democratic (typically liberals) and Republican (typically conservatives). These two political parties have become more polarized over the last four decades, not because one party is a bad apple but because the virtues that drive decision-making within these parties have become more distinct. Political debates are primarily concerned with one or more of five moral foundations- care (where the goal is to reduce suffering), fairness (where the goal is to improve equality), loyalty (where the goal is to support in-group members), authority (where the goal is to respect leadership and tradition), and sanctity (where the goal is to promote purity) <span class="citation">(<a href="#ref-feinberg2019" role="doc-biblioref">Feinberg &amp; Willer, 2019</a>; <a href="#ref-graham2009" role="doc-biblioref">Graham et al., 2009</a>)</span>. Whereas conservatives tend to value loyalty, authority, and sanctity, liberals have been shown to value care and fairness virtues. Because both parties have placed importance on differing virtues, party members approach policy decisions from the lens of the virtues they value. So which side is right? According to virtue ethics, they both are. So long as party members uphold values most important to them, all is right with the world.</p>
</div>
<div id="optimal-ethical-framework-for-human-subjects-research" class="section level3" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Optimal Ethical Framework for Human Subjects Research</h3>
</div>
</div>
<div id="experimental-ethics" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Experimental Ethics</h2>
<div id="the-irb" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> The IRB</h3>
<p>Over one decade after surviving prisoners were liberated from the last concentration camp of the Holocaust, Adolf Eichmann, one of the Holocaust‚Äôs primary organizers and leaders, was tried for the role he played in this ghastly extermination crusade <span class="citation">(<a href="#ref-baade1961" role="doc-biblioref">Baade, 1961</a>)</span>. Eventually known as the Eichmann Trial, this 8-month-long investigation led to 15 counts of criminal offenses, including crimes against the Jewish people and crimes against humanity. While reflecting on his rationale for forcibly removing, torturing, and eventually murdering millions of Jews under the direction of Adolf Hitler, an unrepentant Eichmann claimed that he was ‚Äúmerely a cog in the machinery that carried out the directives of the German Reich‚Äù <span class="citation">(<a href="#ref-kilham1974" role="doc-biblioref">Kilham &amp; Mann, 1974</a>)</span>, that he was not directly responsible for the millions of lives taken. This startling admission gave a young assistant professor at Yale University an interesting idea. He thought to himself, ‚ÄúCould it be that Eichmann and his million accomplices in the Holocaust were just following orders? Could we call them all accomplices?‚Äù <span class="citation">(<a href="#ref-milgram1974" role="doc-biblioref">Milgram, 1974</a>)</span>. That young professor was Stanley Milgram, and the (in)famous Milgram experiment was born.</p>
<p>To test whether people would comply under the direction of an authority figure no matter how uncomfortable or harmful the outcome, Milgram invited participants into the laboratory to serve as a teacher for an activity <span class="citation">(<a href="#ref-milgram1963" role="doc-biblioref">Milgram, 1963</a>)</span>. Participants were told that they were to administer electric shocks of increasing voltage to another participant, the student, in a nearby room whenever the student provided an incorrect response. In reality, the student was a confederate who was in on the experiment and only pretended to be in pain when they received the shocks. Participants were encouraged to continue administering shocks despite clearly audible pleas from students to stop the electric shocks. Milgram predicted that only about 1% of participants would continue until they had delivered the maximum voltage. The results were shocking; 65% of participants administered the maximum voltage to the student, and displayed anxiety symptoms, nervousness, and some even developed seizures. Although experimenters debriefed participants at the end of the study, the damage had already been done. Milgram‚Äôs shocking shock experiment convinced many people in the scientific community to adopt stricter policies that protect study participants from unnecessary or incongruent harm as had been observed in this experiment.</p>
<p>Unfortunately, Milgram‚Äôs shock experiment was just one of dozens of unethical human subjects studies that garnered the attention (and anger) of the public. Something had to be done. In 1978, the National Commission for the Protection of Human Services of Biomedical and Behavioral Research released <strong>The Belmont Report</strong>, a 20-page document outlining protections for the rights of human subjects participating in research studies <span class="citation">(<a href="#ref-adashi2018" role="doc-biblioref">Adashi et al., 2018</a>)</span>. Perhaps the most important message found in the Report was the notion that ‚Äúinvestigators should not have sole responsibility for determining whether research involving human subjects fulfills ethical standards. Others, who are independent of the research, must share the responsibility.‚Äù In other words, ethical research required both transparency and external oversight. Whereas much of the research being done before the release of the Belmont Report modeled a utilitarian framework, commissioners were adamant that this approach was both unethical and self-serving. The days of exploitative and rogue human subjects research were dwindling quickly.</p>
<p>A by-product of the Belmont Report, and of calls to reform biomedical and behavioral research involving human subjects, was the creation of the <strong>Institutional Review Board</strong>, or the <strong>IRB</strong>. The IRB is a committee of people who review, evaluate, and monitor human subjects research to make sure that participants‚Äô rights and agency are protected when engaging in research <span class="citation">(<a href="#ref-oakes2002" role="doc-biblioref">Oakes, 2002</a>)</span>. You can think of the IRB as a local government, and every organization that conducts human subjects or animal research is required to have an IRB. Your university likely has an IRB, and its members are probably a mix of scientists, doctors, professors, and community residents. When a group of researchers have a research question they are interested in pursuing with human subjects, they must receive approval from their local IRB before beginning any data collection. The IRB reviews each study to make sure:</p>
<ol style="list-style-type: decimal">
<li>A study poses <strong>minimal risk</strong> to participants. This means the anticipated harm or discomfort to the participant is not greater than what would be experienced in everyday life. Because participants in Milgram‚Äôs experiment experienced heightened anxiety and seizures, it would not have passed the minimal risk assessment.</li>
<li>Researchers obtain <strong>informed consent</strong> from participants before collecting any data. This means experimenters must disclose all potential risks and benefits so that participants can make an informed (Get it?) decision about whether or not to participate in the study. Importantly, informed consent does not stop after participants sign a consent form. If researchers discover any new potential risks or benefits along the way, they must disclose these discoveries to all participants.</li>
<li>All collected information remains confidential. <strong>Confidentiality</strong> is critical, especially when collecting protected health information (or <strong>PHI</strong>). For the most part, what happens in the lab stays in the lab. Researchers have an obligation to their participants to protect their names, addresses, and social security numbers at minimum. Data that is anonymized and encrypted is harder to land in the wrong hands. While researchers cannot guarantee confidentiality, we can (and should) minimize the risk wherever possible. We‚Äôll talk more about this in a later section.</li>
<li>Participants are recruited equitably and without coercion. Before the IRB became a standard oversight, researchers often utilized marginalized and vulnerable populations to test their research questions, and these questions sometimes required extremely invasive procedures. We‚Äôll discuss what we mean by marginalized and vulnerable populations in the Special Considerations for Vulnerable Populations section. For now, you should know that participation in research studies should always be made voluntary and that everyone eligible to for a study should have equal access to participate.</li>
</ol>
<p>Did you know that the IRB operates under one of the three ethical frameworks we discussed in the last section? Based on what you‚Äôve learned so far, take a moment to guess which framework the IRB has adopted (no peeking!). If you guessed deontological, you‚Äôre right. Remember that deontologists are concerned with the intent over the impact of an action. We do not steal because stealing is wrong. We can see a deontological IRB in action when we consider how they make decisions. One of the IRB‚Äôs primary oversights is ensuring the autonomy of participants, and they do this by following a strict set of guidelines. This rule-based approach to ethics is paramount to deontology</p>
<p>
Proposed framework: virtue
</p>
<p>
Navigation
</p>
</div>
<div id="debriefing-participants" class="section level3" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Debriefing Participants</h3>
<p>So far, we‚Äôve discussed one research study that prompted external oversight and additional scrutiny of human subjects research. But before the shock experiments was the Tuskegee Syphilis Study, and the results were even more devastating and egregious. In 1929, The United States Public Health Service (USPHS) was perplexed by the effects of a particular disease in its latent form that seemed to find its local epicenter in Macon County, Alabama, with an overwhelmingly Black population <span class="citation">(<a href="#ref-brandt1978" role="doc-biblioref">Brandt, 1978</a>)</span>. Syphilis is a sexually transmitted bacterial infection that can either be in a visible and active stage or in a latent stage <strong><span class="citation">(<a href="#ref-cite" role="doc-biblioref"><strong>cite?</strong></a>)</span></strong>. At the time of the study‚Äôs inception, roughly 36% of Tuskegee‚Äôs adult population had developed some form of syphilis, one of the highest infection rates in America <span class="citation">(<a href="#ref-white2006" role="doc-biblioref">White, 2006</a>)</span>. Thanks to a grant from the Julius Rosenwald Fund and leadership from Dr.¬†Taliaferro Clark and Surgeon General H.S. Cumming, the USPHS recruited 400 Black males from 25-60 years of age with latent syphilis and 200 Black males without the infection to serve as a control group to participate in what would become one of the most exploitative research studies ever done on American soil <span class="citation">(<a href="#ref-brandt1978" role="doc-biblioref">Brandt, 1978</a>)</span>. The USPHS sought the help of the Macon County Board of Health to recruit participants with the promise that they would provide treatment for community members with syphilis. The researchers sought poor, illiterate Blacks and, instead of telling them that they were being recruited for a research study, they merely informed them that they would be treated for ‚Äúbad blood,‚Äù a phrase used in that time to refer to syphilis. But because the study was only interested in tracking the natural course of latent syphilis without any medical intervention, the USPHS had no intention of providing any care to its participants. To assuage participants, the USPHS distributed an ointment not been shown to be effective in the treatment of syphilis, and only small doses of a medication actually used to treat the infection. In addition, participants underwent a spinal tap which was presented to them as another form of therapy and their ‚Äúlast chance for free treatment.‚Äù By 1955, just over 30% of the original participants died from syphilis complications. Dr.¬†O.C. Wenger of the disease clinic in Hot Springs, Alabama, admitted, ‚Äúwe now know, where we could only surmise before, that we have contributed to [the participants‚Äô] ailments and shortened their lives.‚Äù But even this admission did not mark the end of the experiment. In fact, it would be another 22 years before the final report was released and (lack of) treatment ended. In total, 128 participants died of syphilis or complications from the infection, 40 wives became infected, and 19 children were born with the infection <span class="citation">(<a href="#ref-katz2011" role="doc-biblioref">Katz &amp; Warren, 2011</a>)</span>. The damage rippled through two generations, and many never actually learned what had been done to them.</p>
<p>Think critically about what went wrong with the Tuskegee Syphilis Study. In what ways did investigators violate the rights of participants? Let‚Äôs start from the beginning. Investigators did not obtain informed consent. Participants were not made aware of all known risks and benefits involved with their participation. Instead, they were led to believe that diagnostic and invasive exams were directly related to their treatment. In human subjects research, this is referred to as <strong>deception</strong>. A study is deceptive when (1) experimenters withold any information about its goals or intentions, (2) experimenters hide their true identity (such as when using a confederate), (3) some aspects of the research are under- or overstated to conceal certain information, or (4) participants receive any false or misleading information <span class="citation">(<a href="#ref-tai2011" role="doc-biblioref"><strong>tai2011?</strong></a>)</span>. Deception is harmful to participants because it prevents them from making an informed decision about their involvement in a research study. In the case of the Tuskegee Syphilis Study, participants were unable to decline participation on the basis that they would not actually receive treatment. Had the men involved in the study known this fact, it is unlikely all 299 with the infection would have remained enrolled. Additionally, participants were denied appropriate treatment following a 1943 discovery that penicillin was effective at treating syphilis <span class="citation">(<a href="#ref-mahoney1943" role="doc-biblioref">Mahoney et al., 1943</a>)</span>. In fact, the USPHS requested that medical professionals overseeing their care outside of the research study not offer treatment to participants so as to perserve the study‚Äôs integrity. This intervention violated participants‚Äô rights to equal access to care, which should have taken precedence over the results of the study. Another problem with this research study was that recruitment was both imbalanced and coercive. Not only were participants selected from the poorest of neighborhoods in the hopes of finding vulnerable populations with little agency, but they were also bribed with empty promises of treatment and a monetary incentive to pay for burial fees, a financial obstacle for many sharecroppers and tenant farmers at the time.</p>
<p>Importantly, the USPHS failed to <strong>debrief</strong> the men involved in their study. What does it mean to debrief participants about a research study? In general, deception in scientific research is considered unethical because it misleads the participant on their assumptions about how they will be involved in a study. But sometimes deception is necessary. Consider a recent study led by researchers at Northwestern University and The University of California at Berkeley who were interested in understanding how people regulate emotions after experiencing loss <span class="citation">(<a href="#ref-rompilla2021" role="doc-biblioref">Rompilla Jr et al., 2021</a>)</span>. Rather than finding people who had actually experienced recent loss, researchers instead asked older adults to watch short clips that depicted some type of loss in order to artificially induce these feelings. Once researchers invoked these feelings, they then exposed participants to different means of reappraising these emotions. Do you remember what the goal of the study was? It certainly wasn‚Äôt to make people sad, but instead to determine how different forms of emotional reappraisal alleviate the negative emotions associated with loss. The researchers needed these feelings to be readily accessible to participants in order to adequately evaluate the reappraisal strategies of interest. They also needed to debrief, or to inform participants about the deception and reveal the true study goals <span class="citation">(<a href="#ref-tai2012" role="doc-biblioref">Tai, 2012</a>)</span>. A critical piece of the debriefing process is soliciting participant thoughts and attitudes about any deception involved. Why might it be important to gather this information? We should be cognizant of the potential effects that our deception has on our participants. For Rompilla and colleagues (2021), it would be important to know whether participants had any lingering and unresolved negative emotions at the conclusion of the study and to talk through these feelings before releasing participants back to their regular lives.</p>
<p>Debriefing should happen at the end of any research study, even if the level of deception appears low. This hard-and-fast rule removes researcher-interpretation about the distinctions between high and low deception. Imagine running a study in which you are curious about the relationship between children‚Äôs developmental language trajectory and how often parents talk to their children. To do this, you record parent-child interactions in a lab setting and then have children complete a word recognition task over several time periods. To preserve parents‚Äô natural communication tendencies with their children, you may not want to reveal exactly what you are studying at the first appointment. It would be reasonable to wait until the last interaction to share your intentions with parents so long as (1) doing so does not pose additional risks or harm to participants and (2) you have received expressed approval from the IRB <span class="citation">(<a href="#ref-tai2012" role="doc-biblioref">Tai, 2012</a>)</span>. This also means you may not skip this step or assume participants know your intentions. Remember the ethical principle of justice we discussed earlier in this chapter? It also applies to debriefing! Every participant should have equal access to study information regardless of how the experimenter views the deception.</p>
<p>Now that we have established both the function and importance of debriefing, let‚Äôs discuss strategies on how to effectively debrief participants. In general, debriefing is composed of four parts: (1) participation gratitude, (2) discussion of goals, (3) explanation of deception, and (4) questions and clarification <span class="citation">(<a href="#ref-allen2017" role="doc-biblioref">Allen, 2017</a>)</span>. In the following sections, we will unpack each component in further detail.</p>
<div id="participation-gratitude" class="section level4" number="3.2.2.1">
<h4><span class="header-section-number">3.2.2.1</span> Participation Gratitude</h4>
<p>It is generally a good idea to thank participants for their involvement in the research study, especially because research participation is completely voluntary. Sharing your gratefulness for participation can be as short and as simple as, ‚ÄúThank you so much for participating in the research study today.‚Äù Some studies also include an additional token of appreciation such as monetary compensation or course credit (this is typically used in undergraduate settings). If monetary or other physical compensation will be implemented, it should be commensurate with the amount of time and effort required for participation. A study that includes 2 hours of fMRI scanning and a battery of reaction time measures is probably undervaluing participant involvement when offering $5 at the conclusion of the study, but another study that is offering $ 100 for completing a 5-item questionnaire is probably overvaluing participant involvement. If you‚Äôre having trouble deciding how to compensate participants in your research study, try gathering data about compensation structures for similar studies. Your institution‚Äôs IRB office may also have insights about how best to compensate research participants. Remember, the most important point of this component is that you‚Äôve appropriately communicated your gratitude to your research participants.</p>
</div>
<div id="discussion-of-goals" class="section level4" number="3.2.2.2">
<h4><span class="header-section-number">3.2.2.2</span> Discussion of Goals</h4>
<p>Next, researchers should briefly share the purpose of the research study with participants. Why were participants recruited for this study in the first place? What are the researchers hoping to learn by conducting this study? It is important to ensure that participants fully understand the goals of the study, so avoiding technical jargon or confusing language is critical. One question you might have at this point is how much information is necessary to disclose to participants. There is no exact formula but participants should generally know about the research questions and hypotheses. As an added bonus, you might also consider sharing any preliminary findings or where to find the completed write-up at the study‚Äôs conclusion. This latter point may be useful when some aspect of the study appears evaluative and participants are interested in knowing how well they preformed against their peers. For example, a parent whose child completed a word-recognition task may request information about their child‚Äôs performance. Importantly, many studies cannot evaluate individual data points because they are only meant be informative in the aggregate so providing information about a single data point is futile at best and risky at worst. Instead of offering information about the individual child, a researcher might choose to share preliminary results about the entire data set or agree to follow up with the parent when the data has been published.</p>
</div>
<div id="explanation-of-deception" class="section level4" number="3.2.2.3">
<h4><span class="header-section-number">3.2.2.3</span> Explanation of Deception</h4>
<p>Before concluding involvement in the research study, participants should be aware of whether and how any deception was utilized, regardless of how minor the deception seems to the researcher. David Holmes referred to this component of the debriefing process as <strong>dehoaxing</strong> because it is meant to illuminate any aspects of the study that were previously misleading or inaccurate <span class="citation">(<a href="#ref-holmes1976" role="doc-biblioref">Holmes, 1976</a>)</span>. The goal here is two-part: (1) to reveal the true intent of the study and (2) to alleviate any potential anxiety associated with the deception. After identifying where the deception occurred, it is useful to explain why the deception was necessary for the study‚Äôs success. Then, researchers should clarify any misconceptions brought about by the false information. For example, researchers who administer a pill meant to increase confidence before participants are told they will give a speech in front of a large crowd should reveal that the pill was merely a placebo which has no effect on confidence levels or on performance. In this case, participants should not leave with the impression that taking a pill will improve their public speaking skills. Although it is still possible that participants could have some residual uncertainty about whether or not a study was truly deceptive, it is important that researchers clearly communicate points of deception and attempt to correct any misinformation at the study‚Äôs conclusion.</p>
</div>
<div id="questions-and-clarification" class="section level4" number="3.2.2.4">
<h4><span class="header-section-number">3.2.2.4</span> Questions and Clarification</h4>
<p>Finally, researchers should answer any questions or address any concerns raised by participants. Many researchers use this opportunity to first ask participants about their interpretation of the study, what they thought were the study goals. This not only illuminates aspects of the study design that may have been unclear to or hidden from participants, but it also begins a discussion where both researchers and participants can communicate about this joint experience. This step is also helpful in identifying negative emotions or feelings resulting from the study <span class="citation">(<a href="#ref-allen2017" role="doc-biblioref">Allen, 2017</a>)</span>. When participants do express negative emotions, researchers are responsible for sharing resources participants can utilize to work though the discomfort. Resources may include information about mental health services, hotlines, or contact information for the study‚Äôs principal investigator or the office of the IRB that approved the study. This also means that participants maintain the right to report any concerns they have about the study beyond the research team, and this right does not disappear once individual participation has concluded. Thus, it is important that researchers reiterate that participation in the study is voluntary and that participants have the right to withdraw at any time. If researchers cannot adequately answer participant questions, they should refer participants to entities who can.</p>
</div>
<div id="a-note-on-debriefing-in-online-studies" class="section level4" number="3.2.2.5">
<h4><span class="header-section-number">3.2.2.5</span> A Note on Debriefing in Online Studies</h4>
<p>With the widespread proliferation of data collection sites such as Amazon Mechanical Turk and Prolific, many researchers have elected to conduct some or all of their studies online. Online studies can be advantageous to the research landscape because they increase sample diversity and access to science. A research lab located in the western United States who recruits participants for in-person testing may achieve some impressive diversity when advertising in libraries and grocery stores, but can almost certainly increase its scope when recruiting participants across the country. This also means that people who are unable to travel to research sites but have reasonable access to a computer with internet capabilities are suddenly able to contribute to the scientific community in ways previously unimaginable. But with this new landscape comes new considerations for researchers, especially in the ethical domain. So far in this chapter, we have discussed strategies for conducting ethically mindful research in in-person settings, but what is still outstanding is how to transfer these new skills online, namely when debriefing participants. Fortunately, this process is easier than you might think. Importantly, debriefing is required regardless of the study‚Äôs presentation method. When a study is fully automated and participants do not interact with experimenters, researchers can make use of <strong>debriefing statements</strong>. Debriefing statements are documents that summarize all four components of the debriefing process (participation gratitude, discussion of goals, explanation of deception, and questions and clarification). Because experimenters are not present at the end of the study to answer participant questions, the statement typically provides the contact information for both the principal investigator and the IRB office. These communication channels should be clearly conveyed should participants need to follow up about the study for any reason. It might also be worth considering whether a study is best suited for the online space given its content. Studies that have high deception levels or induce negative emotions/are triggering in nature may benefit from having an experimenter present to verbally alleviate concerns and adress points of deception rather than relying on a written statement. If researchers cannot be reasonably sure that participants will not receive any additional benefit in speaking with an experimenter following their involvement in the study, it is worth reconsidering whether the study can be conducted online in the first place.</p>
</div>
</div>
<div id="special-considerations-for-vulnerable-populations" class="section level3" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Special Considerations for Vulnerable Populations</h3>
<p>Regardless of who is participating in research, investigators have an obligation to protect the rights and well-being of all participants. However, some populations have been designated as vulnerable and should receive additional considerations or oversight when involving them in research studies. In this section, we will consider four of a handful of vulnerable populations and discuss whether and how to include them in research studies.</p>
<div id="people-with-disabilities" class="section level4" number="3.2.3.1">
<h4><span class="header-section-number">3.2.3.1</span> People with Disabilities</h4>
<p>There are thousands of disabilities that affect cognition, development, motor ability, communication, and decision-making with varying degrees of interference, so it is first important to remember that considerations for this population will be just as diverse as its members. Officially, a disability is a diagnosed mental or physical condition that limits or restricts a person‚Äôs involvement in daily activities <span class="citation">(<a href="#ref-stineman2001" role="doc-biblioref"><strong>stineman2001?</strong></a>)</span>. Roughly 8% of the US population is disabled, which makes it likely that, in the context of a research study, researchers may come into contact with someone who is disabled. Assuming all general rules and regulations are followed, there are no laws that preclude people with disabilities from participating in research. However, those with cognitive disabilities who are unable to make their own decisions (importantly, this also applies to children with or without disabilities) may only participant with written consent from a legal guardian and with their individual assent (if applicable). This means that even if the person provides assent, researchers may not enroll them in the research study without also obtaining consent from their guardian. Those retaining full cognitive capacity but who have other disabilities that make it challenging to participate normally in the research study should receive appropriate accommodations to access the material, including the study‚Äôs risks and benefits. Imagine</p>
<p>
Crowd workers
</p>
<p>
Low-income families
</p>
<p>
Prison population
</p>
<div class="ethics-box">
<p>üåø Ethics box: Countering institutional injustice in the inclusion of marginalized populations (Walsh and Ross 2003; Noguera 2003). Example: representation of Black boys in research on aggression in childhood.</p>
</div>
</div>
</div>
</div>
<div id="publication-ethics" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Publication Ethics</h2>
<div id="authorship" class="section level3" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Authorship</h3>
<p>
Why it matters/saving yourself the trouble
</p>
<p>
When to have the conversation
</p>
<p>
When to re-evaluate
</p>
</div>
<div id="disclosures" class="section level3" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Disclosures</h3>
<p>
Why it matters
</p>
<p>
Financial statements
</p>
<p>
Conflicts of interest
</p>
</div>
<div id="generalizability-statements" class="section level3" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Generalizability Statements</h3>
<p>
Purpose and why it matters
</p>
<p>
Important elements
</p>
<p>
Example statement
</p>
</div>
<div id="post-publication-errors" class="section level3" number="3.3.4">
<h3><span class="header-section-number">3.3.4</span> Post-publication Errors</h3>
<p>
Don‚Äôt ignore it!
</p>
<p>
How to remedy it
</p>
<p>
Why it matters
</p>
</div>
<div id="intellectual-humility" class="section level3" number="3.3.5">
<h3><span class="header-section-number">3.3.5</span> Intellectual Humility</h3>
<div class="accident-report">
<p>‚ö†Ô∏è Accident report: Is it ethical to ‚Äúcherry pick‚Äù research? Simmons, Nelson, &amp; Simonsohn (2011) experiment shows that listening to the Beatles actually makes you younger!</p>
</div>
</div>
</div>
<div id="the-ethical-duty-for-transparency" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> The Ethical Duty for Transparency</h2>
<p>Openness is definitional to the scientific enterprise. <span class="citation"><a href="#ref-merton1979" role="doc-biblioref">Merton</a> (<a href="#ref-merton1979" role="doc-biblioref">1979</a>)</span> described a set of norms that science is assumed to follow: communism ‚Äì that scientific knowledge belongs to the community; universalism ‚Äì that the validity of scientific results is independent of the identity of the scientists; disinterestedness ‚Äì that scientists and scientific institutions act for the benefit of the overall enterprise; and organized skepticism ‚Äì that scientific findings must be critically evaluated prior to acceptance. The choice to be a scientist constitutes acceptance of these norms.</p>
<p>For individual scientists to adhere to these norms, the products of research must be open. To contribute to the communal good, papers must be available so they can be read, evaluated, and extended. And to be subject to skeptical inquiry, experimental materials, research data, analytic code, and software must be all available so that analytic calculations can be verified and experiments can be reproduced. Otherwise, evaluators must accept arguments on the authority of the reporter rather than by virtue of the materials and data, an alternative that is inimical to the norm of universalism. For many scientists, the situation is neatly summarized by the motto of the Royal Society: ‚ÄúNullius in verba,‚Äù often loosely translated as ‚Äúon no one‚Äôs word.‚Äù</p>
<p>Beyond its centrality to science, openness also carries benefits, both to science and to scientists. Open access to the scientific literature increases the impact of publications, which in turn increases the pace of discovery. Openly accessible data increases the potential for citation and reuse, and maximizes the chances that errors are found and corrected. These benefits accrue not just to the scientific ecosystem at large but also to individual scientists, who gain via citations, media impact, collaborations, and funding opportunities.</p>
<p>Some responsibilities follow from these benefits. Because openness maximizes the impact of research and its products, researchers have a responsibility to their funders to pursue open practices so as to seek the maximal return on funders‚Äô investments. And by the same logic, if research participants contribute their time to scientific projects, the researchers also owe it to these participants to maximize the impact of their contributions <span class="citation">(<a href="#ref-brakewood2013" role="doc-biblioref">Brakewood &amp; Poldrack, 2013</a>)</span>.</p>
<p>For all of these reasons, individual scientists have a duty to be open ‚Äì and scientific institutions have a duty to promote transparency in the science they support and publish.</p>
<div id="the-negatives-of-openness" class="section level3" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> The negatives of openness</h3>
<p>Scientists have many other ethical duties beyond openness, however. They have obligations to their collaborators and trainees. They have committed to funders to complete specific studies. And in biomedical and social science fields, they have duties to preserve the welfare of their research participants as well. Conflicts with these duties are often the source of researchers‚Äô hesitance to embrace openness.</p>
<p>Transparency policies also carry costs in terms of time and effort. For example, some routes to open access publication require authors to pay substantial publication costs (i.e., author processing charges). Organizing materials and data for sharing as well as providing support to dataset users can also be time-consuming, especially for larger datasets.</p>
<p>Maintaining participant confidentiality is a major source of both cost and risk for biomedical and other human subjects research. Loss of confidentiality by research participants can have big negative consequences for health, employment, and well-being. While ensuring that tabular data does not contain identifying information is often relatively straightforward, other types of data can be tricky and expensive to anonymize. For example, removing identifying information from video data requires considerable time and expertise. And certain types of dense or narrative data simply may not be de-identifiable due to aspects of the data or the participants‚Äô identities.</p>
<p>Transparency can even be a source of risk ‚Äì actual or perceived ‚Äì to researchers themselves. Effort spent pursuing open practices may not be seen as compatible with other career incentives. For example, learning technical tools to facilitate code and data sharing could take away from time to pursue new research. Disclosure of high value datasets prior to publication could in principle lead to opportunities for ‚Äúscooping‚Äù ‚Äì though it turns out that there are very few documented cases of pre-emption as a result of data sharing. Finally, open sharing of research products prior to and during peer review might carry greater risk for junior researchers and for researchers from disadvantaged groups, because of their greater vulnerability to critiques or negative attention.</p>
</div>
<div id="individuals-should-consider-openness-as-a-default" class="section level3" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Individuals should consider openness as a default</h3>
<p>In the face of competing duties as well as potential negatives to openness, what should individual researchers do? First, because of the ethical duty to openness for every scientist, open practices should be a default in cases where risks and costs are limited. For example, the vast majority of journals allow authors to post accepted manuscripts in their untypset form to an open repository. This route to ‚Äúgreen‚Äù open access is easy, cost free, and ‚Äì because it comes only after articles are accepted for publication ‚Äì confers essentially no risks of scooping. As a second example, the vast majority of analytic code can be posted as an explicit record of exactly how analyses were conducted, even if posting data is sometimes more fraught. These kinds of ‚Äúincentive compatible‚Äù actions towards openness can bring researchers much of the way to a fully transparent workflow, and there is no excuse not to take them.</p>
<p>For some researchers, however, there will be real negatives associated with one or more open practices. If they are not aware of the positive benefits of transparency and sharing for their work and the work of their trainees, they may consider open practices only as a necessary evil, rather than as opportunities to increase citations or build a reputation. But if they recognize the potential benefits of openness, researchers can ask whether there are steps that can be taken to realize some of those benefits while mitigating risks ‚Äì for example, releasing only summary, tabular data rather than raw media data, or making use of a data sharing repository with robust access control.</p>
<p>In some cases, researchers might decide not to share. One example of this kind of situation came up in my own work, when I was studying dense audio-video recordings of the private life of a single identified family; these data are both sensitive and impossible to de-identify. The family decided not to share these data, and I support this decision, having seen how much the data would have compromised their family‚Äôs privacy ‚Äì though we did make tabular data available so that statistical results could be reproduced. A second more general case is archival data without consent for sharing where recontacting participants may be impossible or impractical. These cases are relatively rare, however; it is more common that sharing simply presents some potentially mitigable costs. It is precisely in these cases that institutions should step in.</p>
</div>
<div id="institutions-can-mitigate-the-risks-and-costs-of-openness" class="section level3" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Institutions can mitigate the risks and costs of openness</h3>
<p>Given the ethical imperative towards openness, institutions like funders, journals, and societies need to use their role to promote open practices and to mitigate potential negatives. Scholarly societies have an important role to play in educating scientists about the benefits of openness and providing resources to steer their members towards best practices for sharing their publication and other research products. Similarly, journals can set good defaults, for example by requiring data and code sharing except in cases where a strong justification is given (equivalent to adopting the second highest level in the Transparency and Openness Promotion guidelines). I don‚Äôt think the TOP guidelines are perfect, but I‚Äôm not sure why in this case we‚Äôd let the perfect be the enemy of the good.</p>
<p>Departments and research institutes can also signal their interest in open practices in job advertisements and tenure/promotion guidelines. We did this the last time we had a search at Stanford Psych and it signaled our department‚Äôs general interest in these practices, leading to some good conversations with candidates (and letting us notice explicitly if candidates weren‚Äôt as interested as we were). In addition, by structuring graduate programs to provide training in tools and methods for data and code sharing, departments can educate grad students about producing reproducible and replicable research ‚Äì this has been my hobby horse for quite a while (see here and here).</p>
<p>Institutional funders of research play the most important role, however. Most funders already signal an interest in openness through a required data management plan or similar document, and some (like the US NIH) mandate data sharing to the extent permissible given other regulatory constraints (e.g., institutional review, health or data privacy laws). These requirements, though laudable, don‚Äôt really change the scientific incentives at play. Data sharing should not just be required: It should also be treated as part of the scientific merit of an application. Creating a sufficiently high value dataset should be itself meritorious enough to warrant funding. And on the opposite side of the calculus, funders should signal their willingness to support the effort required to mitigate data sharing costs. For example, this could take the form of extra budget supplements explicitly tied to sharing activities.</p>
<p>More generally, funders and other institutional stakeholders need to act to change the incentive structure for individuals. For example, funding agencies could make it a priority to invest in creating technical tools and practice guidelines for human subject data anonymization. A small RFP for these could create huge value, making it much more straightforward to participate in data sharing.</p>
</div>
<div id="conclusion" class="section level3" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Conclusion</h3>
<p>Both advocates and critics of open practices often appear to be arguing about the merits of radical transparency, but this goal is often not achievable. Instead, individual researchers and institutions should proceed from both an understanding of the benefits of openness and an appreciation of the ethical duty to be open. These starting points lead naturally to a set of practices that are open by default, with exceptions in case of specific risks.</p>
<p>When individual researchers can‚Äôt mitigate the costs associated with openness, responsibility falls to institutional actors in the scientific ecosystem to help. We can all do our part in this by lobbying our journals scientific societies, institutions, and funders to support researchers in making the right decisions around transparency.</p>

</div>
</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-adashi2018" class="csl-entry">
Adashi, E. Y., Walters, L. B., &amp; Menikoff, J. A. (2018). The belmont report at 40: Reckoning with time. <em>American Journal of Public Health</em>, <em>108</em>(10), 1345‚Äì1348.
</div>
<div id="ref-allen2017" class="csl-entry">
Allen, M. (2017). Debriefing of participants. In <em>The sage encyclopedia of communication research methods</em> (Vols. 1‚Äì4). Sage Publications.
</div>
<div id="ref-annas2006" class="csl-entry">
Annas, J. (2006). Virtue ethics. <em>The Oxford Handbook of Ethical Theory</em>, 515‚Äì536.
</div>
<div id="ref-baade1961" class="csl-entry">
Baade, H. W. (1961). The eichmann trial: Some legal aspects. <em>Duke LJ</em>, 400.
</div>
<div id="ref-beauchamp2001" class="csl-entry">
Beauchamp, T. L., Childress, J. F., &amp; others. (2001). <em>Principles of biomedical ethics</em>. Oxford University Press, USA.
</div>
<div id="ref-brakewood2013" class="csl-entry">
Brakewood, B., &amp; Poldrack, R. A. (2013). The ethics of secondary data analysis: Considering the application of belmont principles to the sharing of neuroimaging data. <em>Neuroimage</em>, <em>82</em>, 671‚Äì676.
</div>
<div id="ref-brandt1978" class="csl-entry">
Brandt, A. M. (1978). Racism and research: The case of the tuskegee syphilis study. <em>Hastings Center Report</em>, 21‚Äì29.
</div>
<div id="ref-feinberg2019" class="csl-entry">
Feinberg, M., &amp; Willer, R. (2019). Moral reframing: A technique for effective and persuasive communication across political divides. <em>Social and Personality Psychology Compass</em>, <em>13</em>(12), e12501.
</div>
<div id="ref-flinders1992" class="csl-entry">
Flinders, D. J. (1992). In search of ethical guidance: Constructing a basis for dialogue. <em>International Journal of Qualitative Studies in Education</em>, <em>5</em>(2), 101‚Äì115.
</div>
<div id="ref-garbutt2011" class="csl-entry">
Garbutt, G., &amp; Davies, P. (2011). Should the practice of medicine be a deontological or utilitarian enterprise? <em>Journal of Medical Ethics</em>, <em>37</em>(5), 267‚Äì270.
</div>
<div id="ref-graham2009" class="csl-entry">
Graham, J., Haidt, J., &amp; Nosek, B. A. (2009). Liberals and conservatives rely on different sets of moral foundations. <em>Journal of Personality and Social Psychology</em>, <em>96</em>(5), 1029.
</div>
<div id="ref-holmes1976" class="csl-entry">
Holmes, D. S. (1976). Debriefing after psychological experiments: I. Effectiveness of postdeception dehoaxing. <em>American Psychologist</em>, <em>31</em>(12), 858.
</div>
<div id="ref-katz2011" class="csl-entry">
Katz, R. V., &amp; Warren, R. C. (2011). <em>The search for the legacy of the USPHS syphilis study at tuskegee</em>. Lexington Books.
</div>
<div id="ref-kilham1974" class="csl-entry">
Kilham, W., &amp; Mann, L. (1974). Level of destructive obedience as a function of transmitter and executant roles in the milgram obedience paradigm. <em>Journal of Personality and Social Psychology</em>, <em>29</em>(5), 696.
</div>
<div id="ref-mahoney1943" class="csl-entry">
Mahoney, J. F., Arnold, R., &amp; Harris, A. (1943). Penicillin treatment of early syphilis‚Äîa preliminary report. <em>American Journal of Public Health and the Nations Health</em>, <em>33</em>(12), 1387‚Äì1391.
</div>
<div id="ref-merton1979" class="csl-entry">
Merton, R. K. (1979). The normative structure of science. <em>The Sociology of Science: Theoretical and Empirical Investigations</em>, 267‚Äì278.
</div>
<div id="ref-milgram1963" class="csl-entry">
Milgram, S. (1963). Behavioral study of obedience. <em>The Journal of Abnormal and Social Psychology</em>, <em>67</em>(4), 371.
</div>
<div id="ref-milgram1974" class="csl-entry">
Milgram, S. (1974). <em>Obedience to authority: An experimental view</em>. Harper &amp; Row.
</div>
<div id="ref-mill1859" class="csl-entry">
Mill, J. S. (1859). Utilitarianism (1863). <em>Utilitarianism, Liberty, Representative Government</em>, 7‚Äì9.
</div>
<div id="ref-morris2016" class="csl-entry">
Morris, M. C., &amp; Morris, J. Z. (2016). The importance of virtue ethics in the IRB. <em>Research Ethics</em>, <em>12</em>(4), 201‚Äì216.
</div>
<div id="ref-oakes2002" class="csl-entry">
Oakes, J. M. (2002). Risks and wrongs in social science research: An evaluator‚Äôs guide to the IRB. <em>Evaluation Review</em>, <em>26</em>(5), 443‚Äì479.
</div>
<div id="ref-rompilla2021" class="csl-entry">
Rompilla Jr, D. B., Hittner, E. F., Stephens, J. E., Mauss, I., &amp; Haase, C. M. (2021). Emotion regulation in the face of loss: How detachment, positive reappraisal, and acceptance shape experiences, physiology, and perceptions in late life. <em>Emotion</em>.
</div>
<div id="ref-stapel2012b" class="csl-entry">
Stapel, Diederik Alexander. (2012). <em>Ontsporing</em>. Prometheus Amsterdam.
</div>
<div id="ref-stapel2012" class="csl-entry">
Stapel, Diederik A., &amp; Linde, L. A. van der. (2012). <em>" what drives self-affirmation effects? On the importance of differentiating value affirmation and attribute affirmation": Retraction of stapel and van der linde (2011).</em>
</div>
<div id="ref-tai2012" class="csl-entry">
Tai, M. C.-T. (2012). Deception and informed consent in social, behavioral, and educational research (SBER). <em>Tzu Chi Medical Journal</em>, <em>24</em>(4), 218‚Äì222.
</div>
<div id="ref-trampe2011" class="csl-entry">
Trampe, D., A. Stapel, D., &amp; W. Siero, F. (2011). Retracted: The self-activation effect of advertisements: Ads can affect whether and how consumers think about the self. <em>Journal of Consumer Research</em>, <em>37</em>(6), 1030‚Äì1045.
</div>
<div id="ref-devrieze2021" class="csl-entry">
Vrieze, J. de. (2021). Large survey finds questionable research practices are common. <em>Large Survey Finds Questionable Research Practices Are Common</em>.
</div>
<div id="ref-white2006" class="csl-entry">
White, R. M. (2006). Effects of untreated syphilis in the negro male, 1932 to 1972: A closure comes to the tuskegee study, 2004. <em>Urology</em>, <em>67</em>(3), 654.
</div>
</div>
<p style="text-align: center;">
<a href="2-replication.html"><button class="btn btn-default">Previous</button></a>
<a href="4-estimation.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="toc/toc.js"></script>


</body>
</html>
