<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 1 Experiments | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 1 Experiments | Experimentology">

<title>Chapter 1 Experiments | Experimentology</title>

<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.18/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-experiments.html#experiments"><span class="toc-section-number">1</span> Experiments</a></li>
<li><a href="2-theories.html#theories"><span class="toc-section-number">2</span> Theories</a></li>
<li><a href="3-replication.html#replication"><span class="toc-section-number">3</span> Replication and reproducibility</a></li>
<li><a href="4-ethics.html#ethics"><span class="toc-section-number">4</span> Ethics</a></li>
<li class="part"><span><b>II Statistics</b></span></li>
<li><a href="5-estimation.html#estimation"><span class="toc-section-number">5</span> Estimation</a></li>
<li><a href="6-inference.html#inference"><span class="toc-section-number">6</span> Inference</a></li>
<li><a href="7-models.html#models"><span class="toc-section-number">7</span> Models</a></li>
<li class="part"><span><b>III Design and Planning</b></span></li>
<li><a href="8-measurement.html#measurement"><span class="toc-section-number">8</span> Measurement</a></li>
<li><a href="9-design.html#design"><span class="toc-section-number">9</span> Design of experiments</a></li>
<li><a href="10-sampling.html#sampling"><span class="toc-section-number">10</span> Sampling</a></li>
<li class="part"><span><b>IV Execution</b></span></li>
<li><a href="11-prereg.html#prereg"><span class="toc-section-number">11</span> Preregistration</a></li>
<li><a href="12-collection.html#collection"><span class="toc-section-number">12</span> Data collection</a></li>
<li><a href="13-management.html#management"><span class="toc-section-number">13</span> Project management</a></li>
<li class="part"><span><b>V Analysis and Reporting</b></span></li>
<li><a href="14-viz.html#viz"><span class="toc-section-number">14</span> Visualization</a></li>
<li><a href="15-writing.html#writing"><span class="toc-section-number">15</span> Writing</a></li>
<li><a href="16-meta.html#meta"><span class="toc-section-number">16</span> Meta-analysis</a></li>
<li><a href="17-conclusions.html#conclusions"><span class="toc-section-number">17</span> Conclusions</a></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li><a href="A-git.html#git"><span class="toc-section-number">A</span> GitHub Tutorial</a></li>
<li><a href="B-rmarkdown.html#rmarkdown"><span class="toc-section-number">B</span> R Markdown Tutorial</a></li>
<li><a href="C-tidyverse.html#tidyverse"><span class="toc-section-number">C</span> Tidyverse Tutorial</a></li>
<li><a href="D-ggplot.html#ggplot"><span class="toc-section-number">D</span> ggplot Tutorial</a></li>
<li><a href="E-instructors.html#instructors"><span class="toc-section-number">E</span> Instructor‚Äôs guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="experiments" class="section level1" number="1">
<h1><span class="header-section-number">Chapter 1</span> Experiments</h1>
<div class="learning-goals">
<p>üçé Learning goals</p>
<ul>
<li>Define ‚Äúexperiment‚Äù</li>
<li>Reason about the relationship between the experimental method and causal inference</li>
<li>Articulate the critical role of randomization in causal inference</li>
<li>Consider constraints on generalizability for experimental estimates</li>
</ul>
</div>
<p>Welcome to Experimentology. This is a book about how to do psychology experiments! Much of what we cover in the book is about the nitty gritty of how to design your study, how to analyze your data, or even how to name your files! But before we can get into all that, we‚Äôre going to need to have a conversation about what an experiment is. And that in turn will lead us pretty quickly to talk about <strong>causality</strong>, since the unique contribution of experiments is to help us measure causal effects.</p>
<p>The guiding mantra of this book is that experiments are for estimating causal effects, and that good experiments do so in a maximally precise and unbiased way, leading to strong generalizations. Much of our advice about how to navigate decision-making with respect to measurement, design, and sampling comes directly from this mantra.</p>
<div id="what-is-an-experiment-and-why-would-you-do-one" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> What is an experiment, and why would you do one?</h2>
<!-- - On one hand, experiments are ‚Äúthe worst way to learn about the world‚Äù (in the words of one of our mentors). ‚ÄúYou can't play 20 questions with nature and win‚Äù (Newell 1973). But experiments are also one of our best tools for making strong causal inferences about the hidden structure of the world. They allow us to not just observe the world but to systematically intervene on it. -->
<!-- - We used to just poke things or people and measure what happened (see Hacking, 1990 on the 19th century craze of just measuring everything for fun.) Now we typically want our experiments to resolve deeper questions or test hypotheses. -->
<p>When you do an experiment, you change the world in order to learn something new. This common-sense definition has two parts to it: the <strong>manipulation</strong> and the <strong>measure</strong>. The manipulation is the thing you are doing to the world, and the measure is the way you quantify the effects that your actions had. The manipulation licenses <strong>causal inference</strong>, which is our first topic. A second ingredient, <strong>randomization</strong>, licenses inferences about the locus of the causal effect.
<!-- The result is some **generalization** that can be made about unseen observations.  -->
<!-- We'll talk about each of these in turn. --></p>
<p>Let‚Äôs think through an example. If you‚Äôve ever tried to write a paper or even a tricky email while listening to vocal music with lyrics, you might have had the feeling that the lyrics of the music interfered with your own writing. Let‚Äôs call this the ‚ÄúDylan Hypothesis‚Äù ‚Äì listening to music like Bob Dylan‚Äôs lyrically rich songs decreases writing skill in the moment while you‚Äôre listening to its.</p>
<p>The Dylan Hypothesis is a <strong>causal hypothesis</strong> ‚Äì meaning that we ascribe responsibility for the decrease in writing skill to this factor particularly.<label for="tufte-sn-2" class="margin-toggle sidenote-number">2</label><input type="checkbox" id="tufte-sn-2" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">2</span> Defining causality is one of the trickiest and oldest problems in philosophy, and we won‚Äôt attempt to solve it here! But from a psychological perspective, we‚Äôre fond of <span class="citation">D. Lewis (<a href="#ref-lewis1973" role="doc-biblioref">1973</a>)</span>‚Äôs ‚Äúcounterfactual‚Äù analysis of causality. On this view, the Dylan Hypothesis amounts to the claim that, in some situation, if we <em>hadn‚Äôt</em> played Dylan, we <em>wouldn‚Äôt</em> have experienced a decrement in writing ability.</span> In what follows, we‚Äôll try to be precise about the causal inferences we‚Äôre discussing.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:intro-dylan1"></span>
<img src="images/intro/dylan1.png" alt="The hypothesized causal relationship of the Dylan Hypothesis." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 1.1: The hypothesized causal relationship of the Dylan Hypothesis.<!--</p>-->
<!--</div>--></span>
</p>
<p>In Figure <a href="1-experiments.html#fig:intro-dylan1">1.1</a>, we show the Dylan Hypothesis using a kind of diagram called a <strong>causal graphical model</strong> <span class="citation">(<a href="#ref-pearl1998" role="doc-biblioref">Pearl, 1998</a>)</span>. Our outcome is writing skill (<span class="math inline">\(Y\)</span>) and our predictor is Dylan listening (<span class="math inline">\(X\)</span>). The edge between them represents a hypothesized causal relationship. Dylan listening is hypothesized have to a causal effect on writing skill, and not vice versa. Now let‚Äôs talk about how experiments allow us to make these causal inferences.</p>
</div>
<div id="causal-inference" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Causal inference</h2>
<p>Most science textbooks don‚Äôt start a definition of what an experiment is; psychology textbooks are an exception <span class="citation">(<a href="#ref-winston1996" role="doc-biblioref">Winston &amp; Blais, 1996</a>)</span>. Perhaps this is because experiments are a critical method for making strong causal inferences, which are otherwise in short supply in psychology, and so the contrast between experimental and non-experimental research is very salient for psychologists (and economists too). In contrast, the role of causality is much more straightforward in the physical sciences ‚Äì so straightforward that they don‚Äôt talk about it at all.</p>
<p>Causal inference is a central issue in any field that deals with human beings. People are very complex systems. Even from an intuitive perspective, there are many obvious factors ‚Äì personality, values, culture, physiology, genes ‚Äì that influence any individual choice. Further, scientists typically have relatively limited opportunities to intervene on complex social systems. Although we can carry out some kinds of experiments within ethical and practical limits, we‚Äôre not just allowed to go around doing what we want to the people around us, just to see what happens! This situation stands in stark contrast to the physical sciences: you can do pretty much anything you want with a chemical solution.</p>
<p>Returning to the Dylan Hypothesis, suppose we did an observational study where we measured our variables ‚Äì Dylan listening and writing quality ‚Äì in a large population.<label for="tufte-sn-3" class="margin-toggle sidenote-number">3</label><input type="checkbox" id="tufte-sn-3" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">3</span> This sounds impractical, but let‚Äôs imagine some kind of dystopian, Google Docs+Spotify surveillance in which the companies team up to monitor your writing and listening habits and provide quantitative estimates of writing quality and lyric density. Big data!</span> We might compute a correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and find some non-zero relationship between them. If we did this study, you might predict that listening to Dylan would be related to better writing.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:intro-dylan2"></span>
<img src="images/intro/dylan2.png" alt="Cofounding with age in the Dylan Hypothesis." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 1.2: Cofounding with age in the Dylan Hypothesis.<!--</p>-->
<!--</div>--></span>
</p>
<p>Can we make a causal inference? No.¬†Correlation doesn‚Äôt equal causation here. There is (at least one) confounding third variable: age (<span class="math inline">\(Z\)</span>). Age is positively related to both Dylan listening and writing skill in our population of interest. Older people tend to be good writers and also tend to be more into folk rockers.<label for="tufte-sn-4" class="margin-toggle sidenote-number">4</label><input type="checkbox" id="tufte-sn-4" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">4</span> We won‚Äôt even put a question mark on this edge because it seems likely to be true.</span></p>
<p>The causal relationship of age to our other two variables means that variation in <span class="math inline">\(Z\)</span> can induce a correlation in <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, even in the absence of a true causal link. When <span class="math inline">\(Z\)</span> is higher, so are <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> on average, so they are correlated. This gives us a slightly more precise definition of <strong>confounding</strong>, a concept that everyone learns to use and recognize in discussing experimental design. In our observational experiment, age is <strong>confounded</strong> with Dylan listening because it has a causal relationship with both the predictor and the outcome.<label for="tufte-sn-5" class="margin-toggle sidenote-number">5</label><input type="checkbox" id="tufte-sn-5" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">5</span> There‚Äôs a more technical definition that‚Äôs more precise here: that is, age provides a ‚Äúbackdoor‚Äù from Dylan listening into writing skill. The ‚Äúbackdoor criterion‚Äù (it‚Äôs really called that!) is a way to define confounding in more complex causal graphs.</span></p>
<p>Experiments are when we intervene on the world and measure the consequences. Here, this means forcing some people to listen to Dylan. In the language of graphical models, if we control the Dylan listening, variable <span class="math inline">\(X\)</span> is causally exogenous ‚Äì not caused by anything else in the system). We ‚Äúsnipped‚Äù the causal link between age and Dylan listening, shown by the scissors icon in Figure <a href="1-experiments.html#fig:intro-dylan3">1.3</a>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:intro-dylan3"></span>
<img src="images/intro/dylan3.png" alt="Experimental intervention snips the causal link between the confounded variable and the outcome." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 1.3: Experimental intervention snips the causal link between the confounded variable and the outcome.<!--</p>-->
<!--</div>--></span>
</p>
<p>In sum, experiments allow us to make strong causal inferences because they allow us to wield the causal scissors, severing the links that would ordinarily allow ‚Äúupstream‚Äù variables like age to confound our inferences.<label for="tufte-sn-6" class="margin-toggle sidenote-number">6</label><input type="checkbox" id="tufte-sn-6" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">6</span> There is a whole wonderful field of causal inference methods for non-experimental and pseudo-experimental designs. Much of this work happens in econometrics and not psychology, and the methods are extremely sophisticated. We recommend <span class="citation">Cunningham (<a href="#ref-cunningham2021" role="doc-biblioref">2021</a>)</span> as an open access introductory text.</span></p>
</div>
<div id="randomization" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Randomization</h2>
<p>When we talk about experiments in psychology, we are talking about experiments that are designed based on the logic of <strong>comparison</strong>. In the physical sciences, we can intervene on a system, comparing before and after the intervention. An example of a simple experiment would be to measure the temperature of some water, heat it, and then measure the temperature again. We feel relatively warranted in making the inference that the heat caused the temperature to rise.</p>
<p>This logic feels sound because we believe that our action heating the water is the only thing that‚Äôs different between our initial measurement (which we might call a <strong>control</strong> measurement<label for="tufte-sn-7" class="margin-toggle sidenote-number">7</label><input type="checkbox" id="tufte-sn-7" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">7</span> Interestingly, this term is not much more than a hundred years old; it starts appearing in the medical literature in the 1890s and the psychological literature soon after that <span class="citation">(<a href="#ref-boring1954" role="doc-biblioref">Boring, 1954</a>)</span>.</span>) and the measurement after our experimental intervention. It feels like we ‚Äúheld everything constant‚Äù between the two observations. This is <span class="citation">Mill (<a href="#ref-mill1869" role="doc-biblioref">1869</a>)</span>‚Äôs ‚Äúmethod of differences‚Äù:</p>
<blockquote>
<p>If an instance in which the phenomenon under investigation occurs, and an instance in which it does not occur, have every circumstance in common save one, that one occurring only in the former; the circumstance in which alone the two instances differ, is the effect, or the cause, or an indispensable part of the cause, of the phenomenon.‚Äù</p>
</blockquote>
<p>In other words, comparison allows for causal inferences.</p>
<p>The trouble is that it‚Äôs not always as easy to ‚Äúhold everything constant.‚Äù Let‚Äôs do two different analogously-designed Dylan Hypothesis experiments. The first will be a <strong>within-participants</strong> design. We ask someone to produce two writing samples for us to rate; the first is written in silence and the second while listening to <em>Blood on the Tracks</em>. Now we have a problem: our two measurements occur at different times. Being the second observation as opposed to the first might have a causal effect on writing skill ‚Äì perhaps the participant practiced, perhaps they feel more comfortable in our experimental setup. Perhaps they feel more tired. In other words, we have introduced another confound ‚Äì this time between parts of the experiment and Dylan listening.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:intro-dylan4"></span>
<img src="images/intro/dylan4.png" alt="Confounding with order." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 1.4: Confounding with order.<!--</p>-->
<!--</div>--></span>
</p>
<p>Here‚Äôs our second experimental design, a <strong>between-participants</strong> design.
We get some individuals to listen to Dylan or not ‚Äì say we have our parents listen to Dylan and our friends do the experiment in silence ‚Äì and then measure their writing during the assigned listening (or non-listening) period. Now we have a different issue. The participants in the control and experimental (Dylan) are different from one another. Even though we intervened in the world, we might as well be in the situation of Figure <a href="1-experiments.html#fig:intro-dylan2">1.2</a>, in the sense that we have differences between our participants that could make a causal difference (e.g., their age). Again, it doesn‚Äôt seem like everything got held constant.</p>
<p>Although structurally these two situations are quite different, they have a single theoretical solution: <strong>randomization</strong>. Randomization ‚Äì whether of the order of an intervention in a sequence or the assignment of participants to conditions ‚Äì is the key intervention that is guaranteed to break causal dependencies. In the case of the order confound, random assignment of condition order guarantees that our estimate of Dylan on writing skill is <strong>unbiased</strong><label for="tufte-sn-8" class="margin-toggle sidenote-number">8</label><input type="checkbox" id="tufte-sn-8" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">8</span> Unbiased with respect to order, of course! It could be biased by many other things.</span> on average.<label for="tufte-sn-9" class="margin-toggle sidenote-number">9</label><input type="checkbox" id="tufte-sn-9" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">9</span> It could be biased in any one situation. Because order has two known states, we can also <strong>balance</strong> order rather than randomizing it. We‚Äôll talk in some depth in Chapter <a href="9-design.html#design">9</a> about how to deal with these sorts of nuisance procedural confounds and when randomization vs.¬†counterbalancing strategies are most appropriate.</span> In the case of the participant confound, <strong>random assignment</strong> of participants to conditions is the key step that breaks the connection between the myriad confounds related to individual participants‚Äô identities and the manipulation of interest.</p>
<p>Random assignment is so central to the recipe of experimental psychology that sometimes we don‚Äôt even think about it as an ingredient! But without it, the cake simply doesn‚Äôt rise. One classic example of a violation of random assignment is the sequential recruitment of participants to conditions (e.g., first testing the non-Dylan group and then the Dylan group). This kind of practice can seem unproblematic to a research assistant carrying out a study (we‚Äôve done it!). But it confounds time with group participation and can lead to all sorts of unforeseen issues in causal inference. Imagine that Dylan wins the Nobel prize somewhere during the recruitment process. This event obviously could change the nature of your manipulation. But so could infinite other events ‚Äì from a global pandemic all the way to the normal course of the seasons changing. Random assignment is the only way to avoid these confounds.</p>
<p>Random assignment is great, but there is a caveat. Random assignment cannot rescue your <em>individual</em> sample from biases. For example, in your randomly-assigned conditions, maybe a few more Dylan condition participants happened to be run after his Nobel than before. Instead, random assignment guarantees that <em>on average</em> your estimate of the causal effect will be unbiased ‚Äì even if you get lucky or unlucky on this individual instance of your experiment.<label for="tufte-sn-10" class="margin-toggle sidenote-number">10</label><input type="checkbox" id="tufte-sn-10" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">10</span> We‚Äôll return to the topic of whether you should test for or adjust for ‚Äúunhappy randomization‚Äù in Chapter <a href="7-models.html#models">7</a>. A brief answer here, to pique your curiosity: no.</span> That‚Äôs pretty good, and maybe it‚Äôs the best you can hope for!</p>
</div>
<div id="who-are-we-measuring" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Who are we measuring?</h2>
<p>In our running example of the Dylan hypothesis, we have been quite vague about the scope and limitations of generalization. Is this a causal estimate that should apply to all people? Of US college students? Of Boomers that grew up listening to Dylan? Estimating an effect via a <strong>sample</strong> ‚Äì the set of people who participate in the experiment ‚Äì presupposes a <strong>population</strong> that the sample is drawn from. Psychology researchers often give little thought to this issue, but generalizability across populations is a key challenge for psychology.</p>
<!-- A critical part of theorizing is saying when your theory *doesn't* apply, and as a group, psychologists tend to be terrible at this [@yarkoni2020]. Maybe Shepard is one of our few exceptions in that he very explicitly claimed a completely universal scope: his law is intended to apply to all people and all stimuli! -->
<p>To point out the ubiquity of population generalizability issues, <span class="citation">Henrich et al. (<a href="#ref-henrich2010" role="doc-biblioref">2010</a>)</span> coined the acronym <strong>WEIRD</strong>. This catchy name describes the oddness of making generalizations about all of humanity from experiments on a sample that is quite unusual because it is Western, Educated, Industrialized, Rich, and Democratic. Henrich and colleagues argue that seemingly ‚Äúfundamental‚Äù psychological like visual perception, spatial cognition, and social reasoning all differ pervasively across populations ‚Äì hence, <em>any</em> generalization from an effect estimated with a WEIRD subpopulation is unwarranted.<label for="tufte-sn-11" class="margin-toggle sidenote-number">11</label><input type="checkbox" id="tufte-sn-11" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">11</span> We‚Äôll discuss the problem of sample generalizability ‚Äì and some ways to reason about the WEIRD problem ‚Äì in more depth in Chapter <a href="10-sampling.html#sampling">10</a>.</span></p>
<p>A second generalizability challenge is that experimental manipulations are typically realized across specific stimuli. In our Dylan example, this would be using ‚ÄúLike a Rolling Stone‚Äù as our only experimental stimulus but then making the claim that our effect generalizes to all Dylan ‚Äì or even all lyrically-dense music! Yet we too rarely reason about how general our hypothesis across the broader space of stimuli. As we‚Äôll see in Chapters <a href="7-models.html#models">7</a> and <a href="9-design.html#design">9</a>, this issue has consequences for our statistical analysis as well as for our experimental designs <span class="citation">(<a href="#ref-yarkoni2020" role="doc-biblioref">Yarkoni, 2020</a>)</span>.</p>
<p>Questions of generalizability are pervasive, but the first step is simply to acknowledge and reason about them. One mechanism for enforcing thinking about theory scope is the idea that all papers should have a section reporting their limitations. We tend to think this is a good idea. <span class="citation">Simons et al. (<a href="#ref-simons2017" role="doc-biblioref">2017</a>)</span> calls this a Constraints on Generality statement.<label for="tufte-sn-12" class="margin-toggle sidenote-number">12</label><input type="checkbox" id="tufte-sn-12" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">12</span> In an early working draft, the COG statement was called a Statement of Limits on Generality (SLOG), which we found more memorable.</span> They recommend explicit qualification of generalizability across 1) experimental participants, 2) experimental stimuli, 3) procedures, and 4) historical or temporal features.</p>
<div class="ethics-box">
<p>üåø Ethics box: Consequences of poor generalizability</p>
<p>Generalizability questions can seem abstract, but it can have substantial practical consequences when experimental research is applied. To take an example from medical research, it has been common practice for decades to study drug effects in mouse models. What is less well-appreciated is that the typical mouse model has been the <em>male</em> mouse only, with a six-fold difference in use of male relative to female mice. This seemingly practical decision may have widespread consequences <span class="citation">(<a href="#ref-shansky2021" role="doc-biblioref">Shansky &amp; Murphy, 2021</a>)</span>. Women experience a greater proportion of adverse consequences from pharmaceutical treatments than men, likely because research on development and evaluation of such treatments is conducted on male animals.</p>
<p>When it comes to research with humans, the analogy is clear. We hope that our findings are implemented in practice through the creation of psychologically-based policies, treatments, or educational interventions. But if effects are measured using a population that mismatches the population to which the interventions are applied, then we should expect lowered efficacy and perhaps even unanticipated negative effects. To combat this issue, researchers have an ethical obligation to give careful consideration to the limitations of their samples and to the scope of applicability of their claims.</p>
</div>
</div>
<div id="experiments-chapter-summary" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> Experiments: Chapter summary</h2>
<p>In this chapter, we defined an experiment as a combination of a manipulation and a measure. This combination, along with randomization, licenses strong causal inferences about the structure of the world. Experiments are a terrible way to learn about the world. They are costly and time-consuming, and sometimes they fail in uninterpretable ways. Yet they are our best method for estimating causal effects; many other techniques exist but they rely on more complex statistical methods that require certain conditions to be feasible.<label for="tufte-sn-13" class="margin-toggle sidenote-number">13</label><input type="checkbox" id="tufte-sn-13" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">13</span> For example, the presence of an external ‚Äúinstrument‚Äù ‚Äì like the weather or the introduction of internet services or changes in state-level policies ‚Äì that is causally related to your cause of interest but unconnected to your outcome.</span></p>
<p>Sometimes we just need to know about the existence or magnitude effects for some applied goal, say deciding whether implementing a particular curriculum will lead to changes in mathematics test scores. In this case, a randomized trial of the intervention is all that we need, and the causal effect is the end-point of the study. But often in psychology we want to do more. The ‚Äúextra‚Äù thing we would like to do is to build theories. We‚Äôll turn to this topic in Chapter <a href="2-theories.html#theories">2</a>.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-1" class="exercise"><strong>Exercise 1.1  </strong></span>Design an experiment for testing the Dylan Hypothesis. (A) What is the condition structure, what is the measure? How do you control for nuisance variables? (B) What are the best arguments against your proposed experimental design? What would an advocate of the Dylan Hypothesis say if it failed to yield a non-zero effect?</p>
</div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-boring1954" class="csl-entry">
Boring, E. G. (1954). The nature and history of experimental control. <em>The American Journal of Psychology</em>, <em>67</em>(4), 573‚Äì589. <a href="http://www.jstor.org/stable/1418483">http://www.jstor.org/stable/1418483</a>
</div>
<div id="ref-cunningham2021" class="csl-entry">
Cunningham, S. (2021). <em>Causal inference</em>. Yale University Press.
</div>
<div id="ref-henrich2010" class="csl-entry">
Henrich, J., Heine, S. J., &amp; Norenzayan, A. (2010). The weirdest people in the world? <em>Behavioral and Brain Sciences</em>, <em>33</em>(2-3), 61‚Äì83.
</div>
<div id="ref-lewis1973" class="csl-entry">
Lewis, D. (1973). <em>Counterfactuals</em>. John Wiley &amp; Sons.
</div>
<div id="ref-mill1869" class="csl-entry">
Mill, J. S. (1869). <em>A system of logic, ratiocinative and inductive: Being a connected view of the princilples of evidence and the methods of scientific investigation</em>. Harper; brothers.
</div>
<div id="ref-pearl1998" class="csl-entry">
Pearl, J. (1998). Graphical models for probabilistic and causal reasoning. <em>Quantified Representation of Uncertainty and Imprecision</em>, 367‚Äì389.
</div>
<div id="ref-shansky2021" class="csl-entry">
Shansky, R. M., &amp; Murphy, A. Z. (2021). Considering sex as a biological variable will require a global shift in science culture. <em>Nature Neuroscience</em>, <em>24</em>(4), 457‚Äì464.
</div>
<div id="ref-simons2017" class="csl-entry">
Simons, D. J., Shoda, Y., &amp; Lindsay, D. S. (2017). Constraints on generality (COG): A proposed addition to all empirical papers. <em>Perspectives on Psychological Science</em>, <em>12</em>(6), 1123‚Äì1128.
</div>
<div id="ref-winston1996" class="csl-entry">
Winston, A. S., &amp; Blais, D. J. (1996). What counts as an experiment?: A transdisciplinary analysis of textbooks, 1930-1970. <em>The American Journal of Psychology</em>, <em>109</em>(4), 599‚Äì616. <a href="http://www.jstor.org/stable/1423397">http://www.jstor.org/stable/1423397</a>
</div>
<div id="ref-yarkoni2020" class="csl-entry">
Yarkoni, T. (2020). The generalizability crisis. <em>Behav. Brain Sci.</em>, 1‚Äì37.
</div>
</div>
<p style="text-align: center;">
<a href="index.html"><button class="btn btn-default">Previous</button></a>
<a href="2-theories.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<link href="www/global.css" rel="stylesheet">
<script src="www/global.js"></script>


</body>
</html>
