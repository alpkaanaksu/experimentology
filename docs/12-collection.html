<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 12 Data collection | Experimentology" />
<meta property="og:type" content="book" />




<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 12 Data collection | Experimentology">

<title>Chapter 12 Data collection | Experimentology</title>

<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />




<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-experiments.html#experiments"><span class="toc-section-number">1</span> Experiments</a></li>
<li><a href="2-theories.html#theories"><span class="toc-section-number">2</span> Theories</a></li>
<li><a href="3-replication.html#replication"><span class="toc-section-number">3</span> Replication and reproducibility</a></li>
<li><a href="4-ethics.html#ethics"><span class="toc-section-number">4</span> Ethics</a></li>
<li class="part"><span><b>II Statistics</b></span></li>
<li><a href="5-estimation.html#estimation"><span class="toc-section-number">5</span> Estimation</a></li>
<li><a href="6-inference.html#inference"><span class="toc-section-number">6</span> Inference</a></li>
<li><a href="7-models.html#models"><span class="toc-section-number">7</span> Models</a></li>
<li class="part"><span><b>III Design and Planning</b></span></li>
<li><a href="8-measurement.html#measurement"><span class="toc-section-number">8</span> Measurement</a></li>
<li><a href="9-design.html#design"><span class="toc-section-number">9</span> Design of experiments</a></li>
<li><a href="10-sampling.html#sampling"><span class="toc-section-number">10</span> Sampling</a></li>
<li class="part"><span><b>IV Execution</b></span></li>
<li><a href="11-prereg.html#prereg"><span class="toc-section-number">11</span> Preregistration</a></li>
<li><a href="12-collection.html#collection"><span class="toc-section-number">12</span> Data collection</a></li>
<li><a href="13-management.html#management"><span class="toc-section-number">13</span> Project management</a></li>
<li class="part"><span><b>V Analysis and Reporting</b></span></li>
<li><a href="14-viz.html#viz"><span class="toc-section-number">14</span> Visualization</a></li>
<li><a href="15-writing.html#writing"><span class="toc-section-number">15</span> Writing</a></li>
<li><a href="16-meta.html#meta"><span class="toc-section-number">16</span> Meta-analysis</a></li>
<li><a href="17-conclusions.html#conclusions"><span class="toc-section-number">17</span> Conclusions</a></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li><a href="A-git.html#git"><span class="toc-section-number">A</span> GitHub Tutorial</a></li>
<li><a href="B-rmarkdown.html#rmarkdown"><span class="toc-section-number">B</span> R Markdown Tutorial</a></li>
<li><a href="C-tidyverse.html#tidyverse"><span class="toc-section-number">C</span> Tidyverse Tutorial</a></li>
<li><a href="D-ggplot.html#ggplot"><span class="toc-section-number">D</span> ggplot Tutorial</a></li>
<li><a href="E-instructors.html#instructors"><span class="toc-section-number">E</span> Instructor‚Äôs guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="collection" class="section level1" number="12">
<h1><span class="header-section-number">Chapter 12</span> Data collection</h1>
<div class="learning-goals">
<p>üçé Learning goals:</p>
<ul>
<li>Outline the key features of informed consent and participant debriefing</li>
<li>Identify the additional protections necessary for working with vulnerable populations</li>
<li>Review best practices for online and in person data collection</li>
<li>Implement data integrity checks, manipulation checks, and pilot testing</li>
</ul>
</div>
<p>You have selected your measure and manipulation and planned your sample. Your preregistration is set. Now it‚Äôs time to think about the nuts and bolts of collecting data. While the details of data collection may vary from context to context and sample to sample, this chapter will highlight some general best practices for the data collection process. We organize these practices around two perspectives: the participant and the researcher.</p>
<p>The first section takes the perspective of a participant. We begin by reviewing the importance of informed consent. Consent is a key part of running experiments that respect the autonomy of their participants. When we neglect the impact of our research on the populations we study, we not only violate regulations governing research, but we also establish a pattern of imbalance and distrust that hinders our efforts. In the second section, we begin to shift perspectives, discussing the choice of online vs.¬†in-person data collection and how to optimize the experimental experience for participants in both in-person and online experiments. We then end by taking the experimenter‚Äôs perspective more fully, asking how we can collect high quality data. We review some best practices in pilot testing, discussing how to structure pilots to get maximal information from participants. We end by reviewing best practices regarding the inclusion of manipulation and attention checks.</p>
<div class="case-study">
<p>üî¨ Case study: The rise of online data collection</p>
<p>Since the rise of experimental psychology laboratories in university settings during the period after World War 2 <span class="citation">(<a href="#ref-benjamin2000" role="doc-biblioref">Benjamin, 2000</a>)</span>, experiments have typically been conducted by recruiting participants from what has been referred to as the ‚Äúsubject pool.‚Äù This term denotes a group of people who can be recruited for experiments, typically students from introductory psychology courses <span class="citation">(<a href="#ref-sieber1989" role="doc-biblioref">Sieber &amp; Saks, 1989</a>)</span> recruited via the requirement that students complete a certain quantity of experiments as part of their course work.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> The ready availability of this convenience population led inevitably to the massive over-representation of US undergraduates in published psychology research, in turn leading to persistent critiques of this practice as undermining the generalizability psychological research <span class="citation">(<a href="#ref-henrich2010" role="doc-biblioref">Henrich et al., 2010</a>; <a href="#ref-sears1986" role="doc-biblioref">Sears, 1986</a>)</span>.</p>
<p>Yet in the period 2005‚Äì2015, there has been a revolution in data collection from convenience populations. Instead of focusing on university undergraduates, increasingly, researchers use convenience samples of online workers recruited from crowdsourcing sites like Amazon Mechanical Turk (AMT) and Prolific Academic. Crowdsourcing services were originally designed to distribute micro-payments to workers for business purposes like retyping receipts, but they have become marketplaces to connect researchers with research participants who are willing to complete surveys and experimental tasks for small payments <span class="citation">(<a href="#ref-litman2017" role="doc-biblioref">Litman et al., 2017</a>)</span>. As of 2015, more than a third of studies in top social and personality psychology journals were conducted on crowdsourcing platforms (another third were still conducted with college undergraduates) and this proportion is likely continuing to grow <span class="citation">(<a href="#ref-anderson2019" role="doc-biblioref">Anderson et al., 2019</a>)</span>.</p>
<p>Initially, many researchers worried that crowdsourced data from online convenience samples would lead to a decrease in data quality. Yet in study after study, data quality was comparable to in-lab convenience samples <span class="citation">(<a href="#ref-buhrmester2016" role="doc-biblioref">Buhrmester et al., 2016</a>; <a href="#ref-mason2012" role="doc-biblioref">Mason &amp; Suri, 2012</a>)</span>. In one particularly compelling demonstration, a set of browser-based experiments were used to replicate a group of classic phenomena in cognitive psychology, with compelling successes on every experiment except those requiring sub-50 millisecond stimulus presentation <span class="citation">(<a href="#ref-crump2013" role="doc-biblioref">Crump et al., 2013</a>)</span>. Further, as we discuss below, researchers have learned to pay greater attention to how to ensure that online participants understand and comply with the instructions in complex experimental tasks.</p>
<p>Since these initial successes, however, attention has moved away from the validity of online experiments to the ethical challenges of engaging with crowdworkers. In 2020, nearly 130,000 people completed MTurk studies <span class="citation">(<a href="#ref-moss2020" role="doc-biblioref">Moss et al., 2020</a>)</span>. Of those, 70% identified as White, 56% identified as women, and 48% had an annual household income below $50,000. A sampling of crowd work determined that the average wage earned was just $2.00 per hour, and less than 5% of workers were paid at least the federal minimum wage <span class="citation">(<a href="#ref-hara2018" role="doc-biblioref">Hara et al., 2018</a>)</span>. Further, many experimenters routinely withheld payment from workers based on their performance in experiments. These practices clearly violate ethical guidelines for research with human participants, but are often overlooked by institutional review boards because participants are offered as a ‚Äúservice‚Äù rather than being paid directly, or because the platforms were unfamiliar to ethics reviewers.</p>
<p>With greater attention to the conditions of workers <span class="citation">(e.g., <a href="#ref-salehi2015" role="doc-biblioref">Salehi et al., 2015</a>)</span>, best practices for online researcher have progressed considerably. As we describe below, working with online populations requires attention to both standard ethical issues of consent and compensation, as well as new issues around the ‚Äúuser experience‚Äù of participating in research. The availability of online convenience samples can be transformative for the pace of research ‚Äì running large studies in a single day rather than many months. But such populations are vulnerable in different ways than university convenience samples, and we must take care to ensure that research online is conducted ethically.</p>
</div>
<div id="informed-consent-and-debriefing" class="section level2" number="12.1">
<h2><span class="header-section-number">12.1</span> Informed consent and debriefing</h2>
<p>As we discussed in Chapter <a href="4-ethics.html#ethics">4</a>, experimenters must respect the autonomy of their participants. Respect for agency means that participants must be informed about the risks and benefits of participation before they agree to participate. It also means that researchers must discuss and contextualize the study afterwards through debriefing. We discuss each of these processes in turn, ending with some guidance on the special protections that are required to protect the autonomy of vulnerable populations.</p>
<div id="getting-consent" class="section level3" number="12.1.1">
<h3><span class="header-section-number">12.1.1</span> Getting consent</h3>
<p>Before we run any experiment, participants must give consent. In the US regulatory framework, there are clear guidelines about what the process of giving consent looks like. They typically center around the <strong>consent form</strong>: a document that lays out the risks and benefits of the study and asks for participants‚Äô signature as a mark of their understanding and consent to participate. Ultimately, your drafted consent form will need to be reviewed by your IRB office, and they can advise you on whether you have adequately described all important information for participants to review.</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:collection-consent-requirements">Table 12.1: </span>US Office of Human Research Protections requirements for a consent form ( edited for length).</span><!--</caption>--></p>
<table>


<thead>
<tr>
<th style="text-align:right;">
</th>
<th style="text-align:left;">
Requirements
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
A statement that the study involves research
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
An explanation of the purposes of the research
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
The expected duration of the subject‚Äôs participation
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
A description of the procedures to be followed
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
Identification of any procedures which are experimental
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
A description of any reasonably foreseeable risks or discomforts to the subject
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:left;">
A description of any benefits to the subject or to others which may reasonably be expected from the research
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:left;">
A disclosure of appropriate alternative procedures or courses of treatment, if any, that might be advantageous to the subject
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:left;">
A statement describing the extent, if any, to which confidentiality of records identifying the subject will be maintained
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:left;">
For research involving more than minimal risk, an explanation as to whether any compensation or medical treatments are available if injury occurs
</td>
</tr>
<tr>
<td style="text-align:right;">
11
</td>
<td style="text-align:left;">
An explanation of whom to contact for answers to pertinent questions about the research and research subjects‚Äô rights
</td>
</tr>
<tr>
<td style="text-align:right;">
12
</td>
<td style="text-align:left;">
A statement that participation is voluntary, refusal to participate will involve no penalty, and that subject may discontinue participation at any time without penalty
</td>
</tr>
</tbody>
</table>
<embed src="images/collection/annotated_consent.pdf" width="\linewidth"  type="application/pdf" />
<p>Consent forms have very specific elements that are required, including explanations of the research and its procedures, description of risks and benefits, and explanation that participation is voluntary. Table <a href="12-collection.html#tab:collection-consent-requirements">12.1</a> gives the full list of consent form requirements given by the US Office for Human Research Protections, and Figure <a href="12-collection.html#fig:collection-annotated-consent">12.1</a> shows how these individual requirements are reflected in a real consent form from some of our research.</p>
<div class="figure"><span style="display:block;" id="fig:collection-annotated-consent"></span>
<p class="caption marginnote shownote">
Figure 12.1: Consent form annotated with how specific text fulfills the requirements in Table 12.1. Categories 5, 8, and 10 were not required for this minimal risk psychology experiment.
</p>
<img src="images/collection/annotated_consent.png" alt="Consent form annotated with how specific text fulfills the requirements in Table 12.1. Categories 5, 8, and 10 were not required for this minimal risk psychology experiment." width="\linewidth"  />
</div>
<p>Given how much must be stated in a consent form, some experimenters worry about demand characteristics (discussed in Chapter <a href="9-design.html#design">9</a>). In some cases, understanding the precise goals of a study may change the result of the study. But the goal of a consent form is typically not to explain the motivation of the research design as much as the topic of the study and the procedures that a participant will undergo. When providing consent information, researchers should focus on what someone might think or feel as a result of participating in the study. Are there any physical or emotional risks associated? What should someone know about the study that may give them pause about agreeing to participate in the first place? Our advice is to center the <em>participant</em> in the consent process rather than the research question. Information about research goals can be provided during debriefing.</p>
</div>
<div id="prerequisites-of-consent" class="section level3" number="12.1.2">
<h3><span class="header-section-number">12.1.2</span> Prerequisites of consent</h3>
<p>In order to give consent, participants must have the cognitive capacity to make decisions (competence), understand what they are being asked to do (comprehension), and know that they have the right to withdraw consent at any time (voluntariness) <span class="citation">(<a href="#ref-kadam2017" role="doc-biblioref">Kadam, 2017</a>)</span>.</p>
<p>Establishing competence is the first prerequisite of informed consent. Typically we assume competence for adult volunteers in our experiments, but in the case that we are working with children or other vulnerable populations (see below), we may need to consider whether they are legally competent to provide consent. Participants who cannot consent on their own should still be informed about participation in your experiment. If possible, obtain <strong>assent</strong>, or agreement to participate, when a person has no legal ability to consent, and respect their decision if they choose not to assent ‚Äì even if you previously obtained consent.</p>
<p>The second prerequisite is comprehension. It is a good practice to review consent forms verbally with participants, especially if the study is involved and takes place in person. The consent form itself must be readable for a broad audience, meaning care should be taken to use accessible language and clear formatting. Consider giving participants a copy of the consent form in advance so they can read at their own pace, think of any outstanding questions they might have, and decide how to proceed without any chance of coercion <span class="citation">(<a href="#ref-young1990" role="doc-biblioref">Young et al., 1990</a>)</span>.</p>
<p>Finally, participants must understand that their involvement is voluntary, meaning that they are under no obligation to be involved in a study: signing a consent form does not waive their right to withdraw at any time. Experimenters should not only state that participation is voluntary, they should also pay attention to other features of the study environment that might lead to <strong>structural coercion</strong> <span class="citation">(<a href="#ref-fisher2013" role="doc-biblioref">Fisher, 2013</a>)</span>. For example, high levels of compensation can make it difficult for lower-income participants to withdraw from research. Similarly, factors like race, gender, and social class can lead participants to feel discomfort around discontinuing a study. It is incumbent on experimenters to provide a comfortable study environment and to avoid such coercive factors wherever possible.</p>
</div>
<div id="debriefing-participants" class="section level3" number="12.1.3">
<h3><span class="header-section-number">12.1.3</span> Debriefing participants</h3>
<p>Once a study is completed, researchers should always debrief participants. A debriefing is composed of four parts: (1) participation gratitude, (2) discussion of goals, (3) explanation of deception, and (4) questions and clarification <span class="citation">(<a href="#ref-allen2017" role="doc-biblioref">Allen, 2017</a>)</span>. Together these serve to contextualize the experience for the participant and to mitigate any potential harms from the study.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Gratitude.</strong> Thank participants for their involvement in research study! Sometimes thanks is enough (for a short experiment), but many studies also include monetary compensation or course credit. Compensation should be commensurate with the amount of time and effort required for participation. Compensation structures vary widely from place to place; typically local IRBs will have guidelines that they ask researchers to comply with.</p></li>
<li><p><strong>Discussion of goals.</strong> Researchers should briefly share the purpose of the research study with participants. Why were participants recruited for this study in the first place? What are the researchers hoping to learn by conducting this study? It is important to ensure that participants fully understand the goals of the study, so avoiding technical jargon or confusing language is critical. You might also consider sharing any preliminary findings or where to find the completed write-up at the study‚Äôs conclusion ‚Äì many engaged participants appreciate learning about research findings, even months or years after participation.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p></li>
<li><p><strong>Explanation of deception.</strong> Researchers must reveal any deception during debriefing, regardless of how minor the deception seems to the researcher. This component of the debriefing process can be thought of as ‚Äúdehoaxing‚Äù because it is meant to illuminate any aspects of the study that were previously misleading or inaccurate <span class="citation">(<a href="#ref-holmes1976" role="doc-biblioref">Holmes, 1976</a>)</span>. The goal is both to reveal the true intent of the study and to alleviate any potential anxiety associated with the deception. Experimenters should make clear both where in the study the deception occurred and why the deception was necessary for the study‚Äôs success.</p></li>
<li><p><strong>Questions and clarification.</strong> Finally, researchers should answer any questions or address any concerns raised by participants. Many researchers use this opportunity to first ask participants about their interpretation of the study, what they thought were the study goals. This practice not only illuminates aspects of the study design that may have been unclear to or hidden from participants, it also begins a discussion where both researchers and participants can communicate about this joint experience. This step is also helpful in identifying negative emotions or feelings resulting from the study <span class="citation">(<a href="#ref-allen2017" role="doc-biblioref">Allen, 2017</a>)</span>. When participants do express negative emotions, researchers are responsible for sharing resources participants can use to work though the discomfort.</p></li>
</ol>
</div>
<div id="special-considerations-for-vulnerable-populations" class="section level3" number="12.1.4">
<h3><span class="header-section-number">12.1.4</span> Special considerations for vulnerable populations</h3>
<p>Regardless of who is participating in research, investigators have an obligation to protect the rights and well-being of all participants. However, some populations are considered especially <strong>vulnerable</strong> because of their decreased agency ‚Äì either in general or in the face of potentially coercive situations. Research with these populations receives additional oversight. In this section, we will consider several vulnerable populations.</p>
<p><strong>Children.</strong> Children are some of the most commonly used vulnerable populations in research because the study of development can contribute both to children‚Äôs welfare and to our understanding of the human mind. In the US, children under the age of 18 may only participate in research with written consent from a parent or guardian. Unless they are pre-verbal, children should additionally be asked for their assent. The risks associated with a research study focusing on children also must be no greater than minimal unless participants may receive some direct benefit from participating or participating in the study may improve a disorder or condition of which the participant was formally diagnosed.</p>
<p><strong>People with disabilities.</strong> There are thousands of disabilities that affect cognition, development, motor ability, communication, and decision-making with varying degrees of interference, so it is first important to remember that considerations for this population will be just as diverse as its members. Roughly 8% of the US population is disabled, which makes it likely that, in the context of a research study, researchers may come into contact with someone who is disabled. No laws preclude people with disabilities from participating in research. However, those with cognitive disabilities who are unable to make their own decisions may only participant with written consent from a legal guardian and with their individual assent (if applicable). Those retaining full cognitive capacity but who have other disabilities that make it challenging to participate normally in the research study should receive appropriate accommodations to access the material, including the study‚Äôs risks and benefits.</p>
<p><strong>Incarcerated populations.</strong> Nearly 2.1 million people are incarcerated in the United States alone <span class="citation">(<a href="#ref-gramlich2021" role="doc-biblioref">Gramlich, 2021</a>)</span>. Due to early (and repugnant) use of prisoners as a convenience population that could not provide consent, the use of prisoners in research has been a key focus of protective efforts. The US Office for Human Research Protections (OHRP) supports their involvement under very limited circumstances ‚Äì typically when the research specifically focuses on issues relevant to incarcerated populations <span class="citation">(<a href="#ref-ohrp2003" role="doc-biblioref"><em>Prisoner Involvement in Research</em>, 2003</a>)</span>. When researchers propose to study incarcerated individuals, the IRB must reconfigure their board to include at least one active prisoner (or someone who can speak from a prisoner‚Äôs perspective) and ensure that less than half of the board has any affiliation to the prison system, public or private. Importantly, researchers must not suggest or promise that participation will have any bearing on prison sentences or parole eligibility, and compensation must be otherwise commensurate with their contribution. A question you might ask when determining whether a study involving incarcerated individuals is appropriate is, ‚ÄúWould a reasonable adult participate if they were not imprisoned?‚Äù</p>
<p><strong>Low-income populations.</strong> Low-income populations are exceptional cases because any one person can easily fall under this category in addition to another. Participants with fewer resources may be more persuaded to participate by monetary incentives, creating a potentially coercive situation. Researchers should consult with their IRB to conform to local standards for non-coercive payment.</p>
<p><strong>Crowdworkers.</strong> As discussed above, crowdsourcing services like Amazon Mechanical Turk have become increasingly prominent as a population in psychology research. IRBs do not consider crowdworkers as a specific vulnerable population, but many of the same concerns about diminished autonomy and greater need for protection can arise. Without platform or IRB standards, it is up to individual experimenters to commit to fair pay, which should likely be at a bare minimum the applicable minimum wage (e.g., the US federal minimum wage). Further, in the context of reputation management systems like those of Amazon Mechanical Turk, participants can be penalized for withdrawing from an experiment ‚Äì once they have their work ‚Äúrejected‚Äù by an experimenter, it can be harder for them to find new jobs, causing serious long-term harm to their ability to earn on the platform.</p>
</div>
</div>
<div id="designing-the-research-experience" class="section level2" number="12.2">
<h2><span class="header-section-number">12.2</span> Designing the ‚Äúresearch experience‚Äù</h2>
<p>Standard ethical frameworks govern certain aspects of human experiments, including the consent process and the approval of risks and benefits for studies. These protections were initially conceptualized for biomedical research in which participation in an experiment carries a significant time commitment as well as substantial risks. In contrast, most lab-based psychology experiments are shorter and less risky, often involving filling out questionnaires, interacting with an experimenter or peer, or giving speeded responses on a computer. And the kinds of experiments that are run in the thousands on crowdsourcing platforms have very different profiles still. They are typically short, carry small amounts of compensation, and carry very limited quantifiable risks.</p>
<p>For the majority of psychology experiments, the biggest factor that governs whether a participant has a positive or negative experience of an experiment is not its risk profile, since this is minimal.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> Instead, it is the participants‚Äô experience. Did they feel welcome? Did they understand the instructions? Did the software work as designed? Was their compensation clearly described and promptly delivered? These aspects of ‚Äúuser experience‚Äù are critical both for ensuring that participants have a good experience in the study (an ethical imperative) and for gathering good data. An experiment that leaves participants unhappy typically doesn‚Äôt satisfy either the ethical or the scientific goals of research.</p>
<div id="ensuring-good-experiences-for-in-lab-participants" class="section level3" number="12.2.1">
<h3><span class="header-section-number">12.2.1</span> Ensuring good experiences for in-lab participants</h3>
<p>A participant‚Äôs experience for an in-lab study begins with their recruitment and continues with their transit to the site of the research. It is perhaps obvious but worth stating that each of these experiences contribute to the success of your experiment. If you have participants with incorrect expectations about the topic or compensation for research, or if communication and scheduling is frustrating, then you can expect their attitude towards the study will start out negative. Similarly, if the experience of finding the study location or parking nearby is difficult, their interaction with the experiment may be affected by these challenges.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> Anything you can do to make these experiences smoother and more predicable ‚Äì prompt communication, well-tested directions, reserved parking slots, etc. ‚Äì will increase the quality of your data.</p>
<p>Once a participant enters the lab, every aspect of the interaction with the experimenter can have an effect on their measured behavior. A vast literature also suggests that various aspects of social interaction facilitate compliance ‚Äì both in an experimental interaction and beyond <span class="citation">(<a href="#ref-gass2018" role="doc-biblioref">Gass &amp; Seiter, 2018</a>)</span>! Creating an environment that facilitates compliance with the experimental procedure ‚Äì whether through the simple expedient of making eye-contact with the participant or through the broader range of persuasive strategies <span class="citation">(<a href="#ref-cialdini2004" role="doc-biblioref">Cialdini &amp; Goldstein, 2004</a>)</span> ‚Äì should lead to a better result as long as it is deployed equally across conditions.</p>
<p>Standardization is the key concern balancing a warmer and more interactive experience for in-lab experiments. As much as possible, any interaction with participants should be scripted and standardized. Doing otherwise may result in differential treatment for participants with different characteristics, which could in turn result in greater variability in outcomes or even sociodemographic biases reflected in data. Even more importantly, such interactions should be undertaken with condition masked to the experimenter. Otherwise, as we reviewed in Chapter <a href="9-design.html#design">9</a>, it is extremely easy for these interactions to result in substantial experimenter expectancy effects! Even if the experimenter must know condition assignment for a participant ‚Äì as is sometimes the case ‚Äì this information should be revealed at the last possible moment to avoid contamination of other aspects of the experimental session.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
</div>
<div id="ensuring-good-experiences-for-online-participants" class="section level3" number="12.2.2">
<h3><span class="header-section-number">12.2.2</span> Ensuring good experiences for online participants</h3>
<p>The design challenges for online experiments are very different than for in-lab experiments. Experimenter expectancy and variability are almost completely eliminated, removing concerns about bias. On the other hand, many Turkers multi-task and do tens of HITs for many hours a day. It can be much harder to induce a particular state of interest when your manipulation is one of dozens the participant has experienced that day and where your interactions with them are mediated by a browser window. When creating an online experimental experience, we consider four issues: (1) design, (2) communication, (3) payment policies, and (4) effective consent and debriefing.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p><strong>Design for online experiments</strong>. If your web experiment is unpleasant to interact with, participants will likely become confused and frustrated. They will either drop out or provide data that are lower quality. Good experiment design online is a subset of good web and interaction design more generally. A good interface should be clean and well-tested and should offer clear affordances for interaction. As a simple example, if a participant presses a key at an appropriate time, the experiment should offer a response ‚Äì otherwise the participant will likely press it again. If the participant is uncertain how many trials are left, they may be more likely to drop out of the experiment. And if they are performing a speeded paradigm, they should receive practice trials to ensure that they understand the controls of the experiment and the instructions prior to beginning the critical blocks of trials.</p>
<p><strong>Communication</strong>. Many online studies run through platforms like Mechanical Turk involve almost no contact with participants. When participants do communicate it is very important to be responsive and polite, since ‚Äì unlike the typical undergraduate participant ‚Äì the work that a crowdworker is doing for your study may be part of how they earn their livelihood. A small issue in the study for you may feel very important for them. For that reason, rapid resolution of issues with studies ‚Äì typically through appropriate compensation ‚Äì is very important. Increasingly, crowdworkers track the reputation of specific labs and experimenters <span class="citation">(<a href="#ref-irani2013" role="doc-biblioref">Irani &amp; Silberman, 2013</a>)</span>. A quick and generous response to an issue will ensure that future crowdworkers do not avoid your studies.</p>
<p><strong>Payment policies</strong>. Many crowdworkers doing online experiments use their payments as part of their primary income stream. As a result, unclear or punitive payment policies can have a major impact. We strongly recommend always paying workers if they complete your experiment, regardless of result. This policy is comparable to standard payment policies for in-lab work. We assume good faith in our participants: if someone comes to the lab, they are paid for the experiment, even if it turns out that they did not perform correctly. The major counterargument to this policy is that some online marketplaces have a population of workers who are looking to cheat by being non-compliant with the experiment (e.g., entering gibberish or even using scripts to progress quickly through studies). Our recommendation is to address this issue through instruction, attention, and manipulation checks (as discussed below) such that the easiest way for a participant to complete your experiment is to comply with your instructions.</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:collection-online-consent">Table 12.2: </span>Sample online consent statement from our course.</span><!--</caption>--></p>
<table>


<thead>
<tr>
<th style="text-align:left;">
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
By answering the following questions, you are participating in a study being performed by cognitive scientists in the Stanford Department of Psychology. If you have questions about this research, please contact us at <a href="mailto:stanfordpsych251@gmail.com" class="email">stanfordpsych251@gmail.com</a>. You must be at least 18 years old to participate. Your participation in this research is voluntary. You may decline to answer any or all of the following questions. You may decline further participation, at any time, without adverse consequences. Your anonymity is assured; the researchers who have requested your participation will not receive any personal information about you.
</td>
</tr>
</tbody>
</table>
<p><strong>Consent and debriefing</strong>. Finally, because online studies are typically fully automated, participants do not have a chance to interact with researchers around consent and debriefing. Further, engagement with long consent forms may be minimal. In our work we have typically relied on short consent statements like the one from our class, shown in Table <a href="12-collection.html#tab:collection-online-consent">12.2</a>. Similarly, debriefing often occurs through a set of pages that summarize all four components of the debriefing process (participation gratitude, discussion of goals, explanation of deception, and questions and clarification).</p>
</div>
<div id="when-to-collect-data-online" class="section level3" number="12.2.3">
<h3><span class="header-section-number">12.2.3</span> When to collect data online?</h3>
<p>Online data collection is increasingly ubiquitous in the behavioral sciences. Further, the web browser ‚Äì- alongside survey software like Qualtrics or packages like jsPsych <span class="citation">(<a href="#ref-de-leeuw2015" role="doc-biblioref">De Leeuw, 2015</a>)</span> ‚Äì- can be a major aid to transparency in sharing experimental materials. Replication and reuse of experimental materials is vastly simpler if readers and reviewers can click on a link and share the same experience as a participant in your experiment. Furthermore, as we discussed in the Case Study above, a whole host of studies support the validity of data from well-designed online studies <span class="citation">(<a href="#ref-buhrmester2016" role="doc-biblioref">Buhrmester et al., 2016</a>; <a href="#ref-crump2013" role="doc-biblioref">Crump et al., 2013</a>; <a href="#ref-mason2012" role="doc-biblioref">Mason &amp; Suri, 2012</a>)</span>.</p>
<p>Still, online data collection is not right for every experiment. Some studies may not be ethically appropriate for being run online: Studies that have substantial deception or that induce negative emotions may require an experimenter present to alleviate concerns and address points of deception rather than relying on a written statement. Beyond this sort of ethical issues, we discuss four broader concerns for online data collection: (1) population availability, (2) the availability of particular measures, (3) the feasibility of particular manipulations, and (4) the length of experiments.</p>
<p><strong>Population</strong>. The first key question to answer is whether the population of interest for an experiment is even available online. Initially, convenience samples from Amazon Mechanical Turk were the only group that was easily available. The demographics of this group are more diverse than some might expect <span class="citation">(<a href="#ref-moss2020" role="doc-biblioref">Moss et al., 2020</a>)</span>. More recently, new tools have emerged to allow demographic pre-screening of crowd participants, including using sites like Cloud Research and Prolific, which offer more granular participant screening mechanisms as well as greater participant reputation controls <span class="citation">(<a href="#ref-eyal2021" role="doc-biblioref">Eyal et al., 2021</a>; <a href="#ref-peer2021" role="doc-biblioref">Peer et al., 2021</a>)</span>.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p>Increasingly, specialty recruitment platforms are growing to meet this challenge. Specialty survey research companies, such as Qualtrics Panels, can be used to recruit samples with specific demographic profiles for simple experiments. And it may initially have seemed implausible that children could be recruited online, but during the COVID-19 pandemic a substantial amount of developmental data collection moved online, with many studies yielding comparable results <span class="citation">(e.g., <a href="#ref-chuey2021" role="doc-biblioref">Chuey et al., 2021</a>)</span>. Sites like <a href="http://lookit.mit.edu">LookIt</a> now offer sophisticated platforms for hosting studies for children and families <span class="citation">(<a href="#ref-scott2017" role="doc-biblioref">Scott &amp; Schulz, 2017</a>)</span>. Finally, new, non-US crowdsourcing platforms continue to grow in popularity, leading to greater global diversity in the available online populations.</p>
<p>Online participant recruitment tools continue to grow and change, and it is likely that in the near future, many populations will be accessible online. However, researchers recruiting participants online should keep in mind the fundamental fact that the specific characteristics that lead a particular person to work part- or full-time on a crowdsourcing platform likely mean that they are not representative of the broader population. Unfortunately, similar caveats hold true for in-person convenience samples (see Chapter <a href="10-sampling.html#sampling">10</a>). Ultimately, researchers must reason about what their generalization goal is and whether that is consistent with the samples they can access online.</p>
<p><strong>Online measures</strong>. Although online data collection was initially restricted to the use of survey measures ‚Äì including ratings and text responses ‚Äì measurement options have rapidly expanded. The widespread use of libraries like jsPsych <span class="citation">(<a href="#ref-de-leeuw2015" role="doc-biblioref">De Leeuw, 2015</a>)</span> has meant that millisecond accuracy in capturing response times is now possible within web-browsers; thus, most reaction time tasks are quite feasible <span class="citation">(<a href="#ref-crump2013" role="doc-biblioref">Crump et al., 2013</a>)</span>. The capture of sound and video is possible with modern browser frameworks <span class="citation">(<a href="#ref-scott2017" role="doc-biblioref">Scott &amp; Schulz, 2017</a>)</span>. Further, even measures like mouse- and eye-tracking are beginning to become available <span class="citation">(<a href="#ref-maldonado2019" role="doc-biblioref">Maldonado et al., 2019</a>; <a href="#ref-slim2021" role="doc-biblioref">Slim &amp; Hartsuiker, 2021</a>)</span>. In general, almost any measure that can be measured in the lab without specialized apparatus can also be collected online. On the other hand, studies that measure a broader range of physiological variables (e.g., heart rate or skin conductance) or a larger range of physical behaviors (e.g., walking speed or pose) are still likely difficult to implement online.</p>
<p><strong>Online manipulations</strong>. A third question is whether particular manipulations can be deployed online. Here, greater caution is called for. Manipulations available online are any interaction that can be created within a browser window ‚Äì but this restriction excludes many different manipulations that involve real-time social interactions with a human being.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> Synchronous chat sessions can be a useful substitute <span class="citation">(<a href="#ref-hawkins2020" role="doc-biblioref">Hawkins et al., 2020</a>)</span>, but these focus the experiment on the content of what is said and exclude the broader set of non-verbal cues available to participants in a live interaction (e.g., gaze, race, appearance, accent, etc.). Creative experimenters can circumvent these limitations by using pictures, videos, and other methods. But more broadly, an experimenter interested in implementing a particular manipulation online should ask how compelling the online implementation is compared with an in-lab implementation. If the intention is to induce some psychological state ‚Äì say stress, fear, or disgust ‚Äì experimenters must trade off the greater ease of recruitment and larger scale of online studies with the far more compelling experience they can offer in a controlled lab context.</p>
<p><strong>The length of online studies</strong>. One last concern is about attention and focus in online studies. Early guidance around online studies tended to focus on making studies short and easy, with the rationale that crowdsourcing workers were used to short jobs. Our sense is that this guidance no longer holds. Increasingly, researchers are deploying long and complex batteries of tasks to relatively good effect <span class="citation">(e.g., <a href="#ref-enkavi2019" role="doc-biblioref">Enkavi et al., 2019</a>)</span> and conducting repeated longitudinal sampling protocols <span class="citation">(discussed in depth on <a href="#ref-litman2020" role="doc-biblioref">Litman &amp; Robinson, 2020</a>)</span>. Rather than relying on hard and fast rules about study length, a better approach for online testing is to ensure that participants‚Äô experience is as smooth and compelling as possible. Under these conditions, if an experiment is viable in the lab, it is likely viable online.</p>
</div>
</div>
<div id="ensuring-high-quality-data" class="section level2" number="12.3">
<h2><span class="header-section-number">12.3</span> Ensuring high quality data</h2>
<p>In the final section of this chapter, we review some key practices surrounding data collection that can help to collect high quality datasets while respecting our ethical obligations to participants. By ‚Äúhigh quality,‚Äù here we especially mean datasets that are uncontaminated by a variety of experimental artifacts due to factors like misunderstanding of instructions, fatigue, incomprehension, or intentional neglect of the experimental task.</p>
<p>We‚Äôll begin by discussing the issue of <strong>pilot testing</strong>; we recommend a systematic procedure for piloting that can maximize the chance of collecting high quality data. next, we‚Äôll discuss the practice of checking participants‚Äô comprehension and attention and what such checks should and shouldn‚Äôt be used for.</p>
<div id="keep-consistent-data-collection-records" class="section level3" number="12.3.1">
<h3><span class="header-section-number">12.3.1</span> Keep consistent data collection records</h3>
<p>As an experimentalist, one of the worst feelings is to come back to your data directory and see a group of data files, <code>run1.csv</code>, <code>run2.csv</code>, <code>run3.csv</code> and not to know for each what experimental protocol was run for each. Was <code>run1</code> the pilot? Maybe a little bit of personal archaeology with timestamps and version history can tell you (more on this in Chapter <a href="13-management.html#management">13</a>). But there is no guarantee.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span style="display:block;" id="fig:collection-runsheet"></span>
<img src="images/collection/runsheet.png" alt="Part of a run sheet for a developmental study." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 12.2: Part of a run sheet for a developmental study.<!--</p>-->
<!--</div>--></span>
</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span style="display:block;" id="fig:collection-log"></span>
<img src="images/collection/log.png" alt="Excerpt of a log for an iterative run of online experiments." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 12.3: Excerpt of a log for an iterative run of online experiments.<!--</p>-->
<!--</div>--></span>
</p>
<p>When you collect data, of course you have to collect the actual data ‚Äì in whatever form they take, whether it is paper surveys, videos, or files on a computer. But good data collection also requires records to be kept of the fact of the data being collected. <strong>Metadata</strong> ‚Äì data about your data ‚Äì should include relevant features like the date of data collection, the sample that was collected, the experiment version, the research assistants who were present, etc. The details of what features precisely are relevant will vary substantially from study to study ‚Äì the important part is that you keep Figures <a href="12-collection.html#fig:collection-runsheet">12.2</a> and <a href="12-collection.html#fig:collection-log">12.3</a> give two examples from our own research. The key feature is that they provide some persistent metadata about how the experiments were conducted.</p>
<div class="accident-report">
<p>‚ö†Ô∏è Accident report: Does data quality vary throughout the semester?</p>
<p>Every lab that collects empirical data repeatedly using the same population builds up lore about how that population varies. Many researchers who conducted experiments with college undergraduates were taught never to run their studies at the end of the semester. Exhausted and stressed students would likely yield low-quality data, or so the argument went. Until the rise of multi-lab collaborative projects like ManyLabs (see Chapter <a href="3-replication.html#replication">3</a>), such beliefs were almost impossible to test.</p>
<p>ManyLabs 3 aimed specifically to evaluate data quality variation across the academic calendar <span class="citation">(<a href="#ref-ebersole2016" role="doc-biblioref">Ebersole et al., 2016</a>)</span>. With 2,696 participants at 20 sites, the study conducted replications of 13 findings. Although only six of these showed strong evidence of replicating across sites, none of the six effects was substantially moderated by being collected later in the semester. The biggest effect they observed was a change in the Stroop effect from <span class="math inline">\(d=.89\)</span> during the beginning and middle of the semester to <span class="math inline">\(d=.92\)</span> at the end. There was some evidence that participants <em>reported</em> being less attentive at the end of the semester, but this trend wasn‚Äôt accompanied by a moderation of experimental effects.</p>
<p>Researchers are subject to the same cognitive illusions and biases as any human. One of these biases is the search to find meaning in the random fluctuations they sometimes observe in their experiments. The intuitions formed through this process can be helpful prompts for generating hypotheses ‚Äì but beware of adopting them into your ‚Äústandard operating procedures‚Äù without further examination. Labs that avoided data collection during the end of the semester might have sacrificed 10‚Äì20% of their data collection capacity for no reason!</p>
</div>
</div>
<div id="best-practices-for-pilot-studies" class="section level3" number="12.3.2">
<h3><span class="header-section-number">12.3.2</span> Best practices for pilot studies</h3>
<p>A <strong>pilot study</strong> is a small study conducted before you collect your main sample. Smooth and successful data collection is typically difficult without piloting, at least the first time you do an experiment of a given type. Fundamentally, experiments induce a particular experience in their participants, and careful attention to the nature of that experience<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> requires iterative development.</p>
<p>Pilot studies cannot tell you about expected effect size (as we discussed in Chapter <a href="10-sampling.html#sampling">10</a>). They also cannot tell you about the significance of your main result. What they <em>can</em> do is tell you about whether your paradigm works. They can reveal:</p>
<ul>
<li>if your code crashes under certain circumstances</li>
<li>if your instructions confuse a substantial portion of your participants</li>
<li>if you have a very high dropout rate</li>
<li>if your data collection procedure fails to log variables of interest</li>
<li>if participants are disgruntled by the end of the experiment</li>
</ul>
<p>We recommend that all experimenters perform ‚Äì at the very minimum ‚Äì two pilot studies before they launch their experiment.</p>
<p>The first pilot, your <strong>non-naive pilot</strong>, should be a test to ensure that your experiment is comprehensible, that participants can complete it, and that the data are logged appropriately. This last goal means that you must <em>analyze</em> the data from the non-naive pilot, at least to the point of checking that the relevant data about each trial is logged.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> Your friends or parents can do this experiment, or in a pinch you can run yourself a bunch of times (though this isn‚Äôt preferable because you‚Äôre likely to miss a lot of aspects of the experience that you are habituated to, especially if you‚Äôve been debugging the software).</p>
<p>The second pilot, your <strong>naive pilot</strong>, should consists of a test of a small set of participants recruited via the channel you plan to use for your main study. Pilot size will depend on the costliness of running the experiment (in time, money, and opportunity cost) as well as your worries about the paradigm. If we‚Äôre talking about a short online survey experiment, then running a pilot of 10‚Äì20 people is reasonable. A more extensive laboratory study might be better served by piloting just two or three people. The goal of the naive pilot study is to understand properties of the participant experience: for example, were they confused? Did they withdraw before the study finished? You won‚Äôt have the numbers to make robust statistical inferences about these questions, but even a small number of pilots can tell you that your dropout rate is likely too high: if 5 of 10 pilot participants withdraw you may need to reconsider aspects of your design. It‚Äôs critical for this pilot that you debrief more extensively with your participants. This debriefing often takes the form of an interview questionnaire after the study is over (‚Äúwhat did you think the study was about?‚Äù and ‚Äúis there any way we could improve the experience of being in the study?‚Äù can be helpful questions).</p>
<p>Piloting ‚Äì especially piloting with naive participants to optimize the participant experience ‚Äì is typically an iterative process. We frequently launch an experiment for a naive pilot, then recognize from the data or from qualitative feedback that the experience can be improved. We make tweaks and pilot again. Be careful not to over-fit to small differences in pilot data, however ‚Äì the samples are small and so inferences will not be robust. The process of piloting should be more like workshopping a manuscript to remove typos and make it read better. If someone has trouble understanding a particular sentence ‚Äì whether in your manuscript or in your experiment instructions ‚Äì you should edit it to make it clearer!</p>
<p>In the case of especially expensive experiments, it can be a dilemma whether to run a larger pilot to identify difficulties since such a pilot will be costly. In these cases, one possibility can be to preregister a contingent testing strategy. For example, in a planned sample of 100 participants, you could preregister running 20 as a pilot sample with the stipulation that you will look only at their dropout rate ‚Äì and not at any condition differences in the target measure. Then the registration can state that, if the dropout rate is lower than 25%, you will collect the next 80 participants and analyze the whole dataset including the initial pilot. This sort of registration can help you split the difference between cautious piloting and conservation of rare or costly data.</p>
</div>
<div id="measure-participant-compliance" class="section level3" number="12.3.3">
<h3><span class="header-section-number">12.3.3</span> Measure participant compliance</h3>
<p>You‚Äôve constructed your experiment and piloted it. Your registration is just about ready to go and you have ethics approval and a good consent process. You are almost ready to go ‚Äì but there is one more family of tricks for helping to achieve high quality data: integrating measures of participant compliance into your paradigm. Collecting data on participant compliance can help you quantify whether participants understand your task, engage with your manipulation, and pay attention to the full experimental experience. These measures in turn can be used both to modify your experimental paradigm and to exclude specific participants that were especially non-compliance <span class="citation">(<a href="#ref-ejelov2020" role="doc-biblioref">Ejel√∂v &amp; Luke, 2020</a>; <a href="#ref-hauser2018" role="doc-biblioref">Hauser et al., 2018</a>)</span>.</p>
<p>Below we make recommendations about four types of compliance checks: (1) passive measures, (2) comprehension checks, (3) manipulation checks, and (4) attention checks. We then discuss their use (and misuse) in data analysis. In summary, we believe that passive measures, comprehension checks, and ‚Äì in some cases ‚Äì manipulation checks all can be quite useful. In contrast, we recommend against the use of attention checks.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Passive measures of compliance</strong>. Even if you do not ask participants anything extra in an experiment, it is often possible to tell if they have engaged with the experimental procedure simply by how long it takes them to complete the experiment. If you see participants with completion times substantially above or below the median, there is a good chance that they are either multi-tasking or rushing through the experiment without engaging.<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> These measures cost little to implement and should be inserted whenever possible in experiments.^[One variation that we endorse in certain cases is to force participants to engage with particular pages for a certain amount of time through the use of timers. Though, beware, this kind of feature can lead to an adversarial relationship with participants ‚Äì in the face of this kind of coercion, many will be apt to pull out their phone and multi-task until the timer runs down.</p></li>
<li><p><strong>Comprehension checks</strong>. For tasks with complex instructions or experimental materials (say a passage that must be understood for a judgment to be made about it), it can be very helpful to get a signal that participants have understood. Comprehension checks, which ask about the content of the experimental instructions or materials, are often included for this purpose. For the comprehension of instructions, the best kinds of questions simply query the knowledge necessary to succeed in the experiment, for example, ‚Äúwhat key do you press if you want to respond that a word is made up?‚Äù In many platforms, it is possible to make participants reread the instructions again until they can answer these correctly. This kind of repetition is nice because it corrects participants‚Äô misconceptions rather than allowing them to continue the experiment. If you are querying comprehension of experimental materials, however, you may not want to re-expose participants to the same passage again in order to avoid confounding between comprehension and the amount of exposure that some participants receive.</p></li>
<li><p><strong>Manipulation checks</strong>. If your experiment involves more than a very transient manipulation ‚Äì for example, if you plan to induce some state in participants or have them learn some content ‚Äì then it is very nice to include a measure in your experiment that confirms that your manipulation succeeded <span class="citation">(<a href="#ref-ejelov2020" role="doc-biblioref">Ejel√∂v &amp; Luke, 2020</a>)</span>. This measure is known as a manipulation check because it measures some prerequisite difference between conditions that is not the key causal effect of interest but is causally prerequisite. For example, if you want to see if anger affects moral judgment, then it makes sense to measure whether participants in your anger induction condition rate themselves as angrier than participants in your control condition. Manipulation checks are very useful in the interpretation of experimental findings because they can decouple the failure of a manipulation from the failure of a manipulation to affect your specific measure of interest.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p></li>
</ol>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span style="display:block;" id="fig:collection-attention-check"></span>
<img src="images/collection/instructional-manip.jpg" alt="An attention check trial from Oppenheimer, Mervis, and Davidenko (2009). These trials can catch inattentive participants, but they also may promote an adversarial relationship between particiapant and experimenter." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 12.4: An attention check trial from Oppenheimer, Mervis, and Davidenko (2009). These trials can catch inattentive participants, but they also may promote an adversarial relationship between particiapant and experimenter.<!--</p>-->
<!--</div>--></span>
</p>
<ol start="4" style="list-style-type: decimal">
<li><strong>Attention checks</strong>. A final type of compliance check is a check that participants are paying attention to the experiment at all. One simple technique is to add questions that have a known and fairly obvious right answer (e.g., ‚Äúwhat‚Äôs the capital of the United States.‚Äù). These trials can catch participants that are simply ignoring all text and ‚Äúmashing buttons‚Äù but will not find participants who are mildly inattentive. A more extreme example comes from <span class="citation">Oppenheimer et al. (<a href="#ref-oppenheimer2009" role="doc-biblioref">2009</a>)</span>, who embedded tricky trials in their experiments like the one shown in Figure <a href="12-collection.html#fig:collection-attention-check">12.4</a>.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a> Such compliance checks certainly decrease satisficing behavior, in which participants read as quickly as they can get away with. On the other hand, participants may see such trials as indications that the experimenter is trying to trick them, and adopt a more adversarial stance towards the experiment, which may result in less compliance with other aspects of the design <span class="citation">(<a href="#ref-hauser2018" role="doc-biblioref">Hauser et al., 2018</a>)</span>. Further, our experience is that tricky attention checks are often failed by many participants, even those who are otherwise engaging with the experimental materials. An alternative approach is to design you experiment to be as clear and engaging as possible, assuming that satisficing is the default way that participants will interact.</li>
</ol>
<p>Data from all of these types of checks are used in many different ‚Äì often inconsistent ‚Äì ways in the literature. We recommend that you use passive measures and comprehension checks as pre-registered exclusion criteria to eliminate a (hopefully small) group of participants who might be non-compliant with your experiment. If exclusion rates are high, this likely will indicate deeper issues with your design. For manipulation checks, we recommend that you analyze the manipulation check separately from the dependent variable to test whether the manipulation was causally effective <span class="citation">(<a href="#ref-ejelov2020" role="doc-biblioref">Ejel√∂v &amp; Luke, 2020</a>)</span>. We <em>do not</em> recommend that you include any of these checks in your analytic models as a covariate, as including this information in your analysis breaks the causal inference from randomization and introduces bias in your analysis.<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a> Used appropriately, compliance checks can provide both a useful set of exclusion criteria and a powerful tool for diagnosing potential issues with your experiment during data analysis and correcting them down the road.</p>
</div>
</div>
<div id="chapter-summary-data-collection" class="section level2" number="12.4">
<h2><span class="header-section-number">12.4</span> Chapter summary: Data collection</h2>
<p>In this chapter, we took the perspective of both the participant and the researcher. Our goal was to discuss how to achieve a good research outcome for both. On the side of the participant, we highlighted the responsibility of the experimenter to ensure a robust consent and debriefing process. We also discussed the importance of a good experimental experience in the lab and online ‚Äì ensuring that the experiment is not only conducted ethically but is also pleasant to participate in. Finally, we discussed how to address some concerns about data quality from the researcher perspective, recommending both the extensive use of non-naive and naive pilot participants and the use of comprehension and manipulation checks.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-allen2017" class="csl-entry">
Allen, M. (2017). Debriefing of participants. In <em>The sage encyclopedia of communication research methods</em> (Vols. 1‚Äì4). Sage Publications.
</div>
<div id="ref-anderson2019" class="csl-entry">
Anderson, C. A., Allen, J. J., Plante, C., Quigley-McBride, A., Lovett, A., &amp; Rokkum, J. N. (2019). The MTurkification of social and personality psychology. <em>Personality and Social Psychology Bulletin</em>, <em>45</em>(6), 842‚Äì850.
</div>
<div id="ref-benjamin2000" class="csl-entry">
Benjamin, L. T. (2000). The psychology laboratory at the turn of the 20th century. <em>American Psychologist</em>, <em>55</em>(3), 318.
</div>
<div id="ref-buhrmester2016" class="csl-entry">
Buhrmester, M., Kwang, T., &amp; Gosling, S. D. (2016). <em>Amazon‚Äôs mechanical turk: A new source of inexpensive, yet high-quality data?</em>
</div>
<div id="ref-chuey2021" class="csl-entry">
Chuey, A., Asaba, M., Bridgers, S., Carrillo, B., Dietz, G., Garcia, T., Leonard, J. A., Liu, S., Merrick, M., Radwan, S.others. (2021). Moderated online data-collection for developmental research: Methods and replications. <em>Frontiers in Psychology</em>, 4968.
</div>
<div id="ref-cialdini2004" class="csl-entry">
Cialdini, R. B., &amp; Goldstein, N. J. (2004). Social influence: Compliance and conformity. <em>Annual Review of Psychology</em>, <em>55</em>(1), 591‚Äì621.
</div>
<div id="ref-crump2013" class="csl-entry">
Crump, M. J. C., McDonnell, J. V., &amp; Gureckis, T. M. (2013). Evaluating amazon‚Äôs mechanical turk as a tool for experimental behavioral research. <em>PLoS One</em>, <em>8</em>(3), e57410.
</div>
<div id="ref-de-leeuw2015" class="csl-entry">
De Leeuw, J. R. (2015). jsPsych: A JavaScript library for creating behavioral experiments in a web browser. <em>Behavior Research Methods</em>, <em>47</em>(1), 1‚Äì12.
</div>
<div id="ref-demayo2021" class="csl-entry">
DeMayo, B., Kellier, D., Braginsky, M., Bergmann, C., Hendriks, C., Rowland, C. F., Frank, M., &amp; Marchman, V. (2021). Web-CDI: A system for online administration of the MacArthur-bates communicative development inventories. <em>Language Development Research</em>.
</div>
<div id="ref-ebersole2016" class="csl-entry">
Ebersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H. M., Allen, J. M., Banks, J. B., Baranski, E., Bernstein, M. J., Bonfiglio, D. B. V., Boucher, L., Brown, E. R., Budiman, N. I., Cairo, A. H., Capaldi, C. A., Chartier, C. R., Chung, J. M., Cicero, D. C., Coleman, J. A., Conway, J. G., ‚Ä¶ Nosek, B. A. (2016). Many labs 3: Evaluating participant pool quality across the academic semester via replication. <em>J. Exp. Soc. Psychol.</em>, <em>67</em>, 68‚Äì82.
</div>
<div id="ref-ejelov2020" class="csl-entry">
Ejel√∂v, E., &amp; Luke, T. J. (2020). <span>‚ÄúRarely safe to assume‚Äù</span>: Evaluating the use and interpretation of manipulation checks in experimental social psychology. <em>Journal of Experimental Social Psychology</em>, <em>87</em>, 103937.
</div>
<div id="ref-enkavi2019" class="csl-entry">
Enkavi, A. Z., Eisenberg, I. W., Bissett, P. G., Mazza, G. L., MacKinnon, D. P., Marsch, L. A., &amp; Poldrack, R. A. (2019). Large-scale analysis of test‚Äìretest reliabilities of self-regulation measures. <em>Proceedings of the National Academy of Sciences</em>, <em>116</em>(12), 5472‚Äì5477.
</div>
<div id="ref-eyal2021" class="csl-entry">
Eyal, P., David, R., Andrew, G., Zak, E., &amp; Ekaterina, D. (2021). Data quality of platforms and panels for online behavioral research. <em>Behavior Research Methods</em>, 1‚Äì20.
</div>
<div id="ref-fisher2013" class="csl-entry">
Fisher, J. A. (2013). Expanding the frame of" voluntariness" in informed consent: Structural coercion and the power of social and economic context. <em>Kennedy Institute of Ethics Journal</em>, <em>23</em>(4), 355‚Äì379.
</div>
<div id="ref-gass2018" class="csl-entry">
Gass, R. H., &amp; Seiter, J. S. (2018). <em>Persuasion: Social influence and compliance gaining</em>. Routledge.
</div>
<div id="ref-gramlich2021" class="csl-entry">
Gramlich, J. (2021). <em>America‚Äôs incarceration rate falls to lowest level since 1995</em>. <a href="https://www.pewresearch.org/fact-tank/2021/08/16/americas-incarceration-rate-lowest-since-1995/">https://www.pewresearch.org/fact-tank/2021/08/16/americas-incarceration-rate-lowest-since-1995/</a>
</div>
<div id="ref-hara2018" class="csl-entry">
Hara, K., Adams, A., Milland, K., Savage, S., Callison-Burch, C., &amp; Bigham, J. P. (2018). A data-driven analysis of workers‚Äô earnings on amazon mechanical turk. <em>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em>, 1‚Äì14.
</div>
<div id="ref-hauser2018" class="csl-entry">
Hauser, D. J., Ellsworth, P. C., &amp; Gonzalez, R. (2018). Are manipulation checks necessary? <em>Frontiers in Psychology</em>, <em>9</em>, 998.
</div>
<div id="ref-hawkins2020" class="csl-entry">
Hawkins, R. D., Frank, M. C., &amp; Goodman, N. D. (2020). Characterizing the dynamics of learning in repeated reference games. <em>Cognitive Science</em>, <em>44</em>(6), e12845.
</div>
<div id="ref-henrich2010" class="csl-entry">
Henrich, J., Heine, S. J., &amp; Norenzayan, A. (2010). The weirdest people in the world? <em>Behavioral and Brain Sciences</em>, <em>33</em>(2-3), 61‚Äì83.
</div>
<div id="ref-holmes1976" class="csl-entry">
Holmes, D. S. (1976). Debriefing after psychological experiments: I. Effectiveness of postdeception dehoaxing. <em>American Psychologist</em>, <em>31</em>(12), 858.
</div>
<div id="ref-irani2013" class="csl-entry">
Irani, L. C., &amp; Silberman, M. S. (2013). Turkopticon: Interrupting worker invisibility in amazon mechanical turk. <em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em>, 611‚Äì620.
</div>
<div id="ref-kadam2017" class="csl-entry">
Kadam, R. A. (2017). Informed consent process: A step further towards making it meaningful! <em>Perspectives in Clinical Research</em>, <em>8</em>(3), 107.
</div>
<div id="ref-litman2020" class="csl-entry">
Litman, L., &amp; Robinson, J. (2020). <em>Conducting online research on amazon mechanical turk and beyond</em>. Sage Publications.
</div>
<div id="ref-litman2017" class="csl-entry">
Litman, L., Robinson, J., &amp; Abberbock, T. (2017). TurkPrime. Com: A versatile crowdsourcing data acquisition platform for the behavioral sciences. <em>Behavior Research Methods</em>, <em>49</em>(2), 433‚Äì442.
</div>
<div id="ref-maldonado2019" class="csl-entry">
Maldonado, M., Dunbar, E., &amp; Chemla, E. (2019). Mouse tracking as a window into decision making. <em>Behavior Research Methods</em>, <em>51</em>(3), 1085‚Äì1101.
</div>
<div id="ref-mason2012" class="csl-entry">
Mason, W., &amp; Suri, S. (2012). Conducting behavioral research on amazon‚Äôs mechanical turk. <em>Behavior Research Methods</em>, <em>44</em>(1), 1‚Äì23.
</div>
<div id="ref-moss2020" class="csl-entry">
Moss, A. J., Rosenzweig, C., Robinson, J., &amp; Litman, L. (2020). Demographic stability on mechanical turk despite COVID-19. <em>Trends in Cognitive Sciences</em>, <em>24</em>(9), 678‚Äì680.
</div>
<div id="ref-oppenheimer2009" class="csl-entry">
Oppenheimer, D. M., Meyvis, T., &amp; Davidenko, N. (2009). Instructional manipulation checks: Detecting satisficing to increase statistical power. <em>Journal of Experimental Social Psychology</em>, <em>45</em>(4), 867‚Äì872.
</div>
<div id="ref-peer2021" class="csl-entry">
Peer, E., Rothschild, D. M., Evernden, Z., Gordon, A., &amp; Damer, E. (2021). MTurk, prolific or panels? Choosing the right audience for online research. <em>Choosing the Right Audience for Online Research (January 10, 2021)</em>.
</div>
<div id="ref-ohrp2003" class="csl-entry">
<em>Prisoner involvement in research</em>. (2003). <a href="https://www.hhs.gov/ohrp/regulations-and-policy/guidance/prisoner-research-ohrp-guidance-2003/index.html">https://www.hhs.gov/ohrp/regulations-and-policy/guidance/prisoner-research-ohrp-guidance-2003/index.html</a>
</div>
<div id="ref-salehi2015" class="csl-entry">
Salehi, N., Irani, L. C., Bernstein, M. S., Alkhatib, A., Ogbe, E., &amp; Milland, K. (2015). We are dynamo: Overcoming stalling and friction in collective action for crowd workers. <em>Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</em>, 1621‚Äì1630.
</div>
<div id="ref-scott2017" class="csl-entry">
Scott, K., &amp; Schulz, L. (2017). Lookit (part 1): A new online platform for developmental research. <em>Open Mind</em>, <em>1</em>(1), 4‚Äì14.
</div>
<div id="ref-sears1986" class="csl-entry">
Sears, D. O. (1986). College sophomores in the laboratory: Influences of a narrow data base on social psychology‚Äôs view of human nature. <em>Journal of Personality and Social Psychology</em>, <em>51</em>(3), 515.
</div>
<div id="ref-sieber1989" class="csl-entry">
Sieber, J. E., &amp; Saks, M. J. (1989). A census of subject pool characteristics and policies. <em>American Psychologist</em>, <em>44</em>(7), 1053.
</div>
<div id="ref-slim2021" class="csl-entry">
Slim, M. S., &amp; Hartsuiker, R. (2021). <em>Visual world eyetracking using WebGazer. js</em>.
</div>
<div id="ref-young1990" class="csl-entry">
Young, D. R., Hooker, D. T., &amp; Freeberg, F. E. (1990). Informed consent documents: Increasing comprehension by reducing reading level. <em>IRB: Ethics &amp; Human Research</em>, <em>12</em>(3), 1‚Äì5.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>At various times, students have raised ethical concerns about these requirements as being coercive of participation in precisely the way that should be off limits for psychology experiments (see Chapter <a href="4-ethics.html#ethics">4</a>). As a result, most programs now provide some more or less onerous alternative to participation.<a href="12-collection.html#fnref1" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn2"><p>Sharing goals is especially important when some aspect of the study appears evaluative ‚Äì participants will often be interested in knowing how well they preformed against their peers. For example, a parent whose child completed a word-recognition task may request information about their child‚Äôs performance. It is often important to highlight that the goals of the study are not about individual evaluation and ranking.<a href="12-collection.html#fnref2" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn3"><p>There are of course exceptions, including research with more sensitive content. Even in these cases, however, attention to the participant‚Äôs experience can be important for ensuring good scientific outcomes.<a href="12-collection.html#fnref3" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn4"><p>For some reason, the Stanford psychology is notoriously difficult to navigate. This seemingly minor issue has resulted in a substantial number of late, frustrated, and flustered participants over the years.<a href="12-collection.html#fnref4" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn5"><p>In some experiments, an experimenter delivers a manipulation and hence it cannot be masked from them. In such cases, it‚Äôs common to have two experimenters such that one delivers the manipulation and another (masked to condition) collect the measurement. This situation often comes up with studies of infancy, since stimuli are often delivered via an in-person puppet show; at a minimum, behavior should be coded by someone other than the puppeteer.<a href="12-collection.html#fnref5" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn6"><p>For extensive further guidance on this topic, see <span class="citation">Litman &amp; Robinson (<a href="#ref-litman2020" role="doc-biblioref">2020</a>)</span>.<a href="12-collection.html#fnref6" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn7"><p>These tools still have significant weaknesses for accessing socio-demographically diverse populations within and outside the US, however ‚Äì screening tools can remove participants, but if the underlying population does not contain many participants from a particular demographic, it can be hard to gather large enough samples. For an example of using crowdsourcing and social media sites to gather diverse participants, see <span class="citation">DeMayo et al. (<a href="#ref-demayo2021" role="doc-biblioref">2021</a>)</span>.<a href="12-collection.html#fnref7" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn8"><p>So called ‚Äúmoderated‚Äù experiments ‚Äì in which the experimental session is administered through a synchronous video chat have been used widely in online experiments for children but these designs are less common in experiments with adults because they are expensive and time-consuming to administer <span class="citation">(<a href="#ref-chuey2021" role="doc-biblioref">Chuey et al., 2021</a>)</span>.<a href="12-collection.html#fnref8" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn9"><p>Even if the experience is somewhat tedious, like searching for a T among Ls for hundreds of trials!<a href="12-collection.html#fnref9" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn10"><p>At a minimum, for each trial you need to know a subject ID, a trial ID, the state of any manipulation (condition, trial type, etc.), and the value for the measure.<a href="12-collection.html#fnref10" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn11"><p>Measurements of per-page or per-element completion times can be even more specific since they can, for example, identify participants that simply did not read an assigned passage.<a href="12-collection.html#fnref11" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn12"><p><span class="citation">Hauser et al. (<a href="#ref-hauser2018" role="doc-biblioref">2018</a>)</span> worry that manipulation checks can themselves change the effect of a manipulation ‚Äì this worry strikes us as sensible, especially for some types of manipulations like emotion inductions. Their recommendation is to test the efficacy of the manipulation in a separate study, rather than trying to nest the manipulation check within the main study.<a href="12-collection.html#fnref12" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn13"><p>The original authors called these ‚Äúinstructional manipulation checks‚Äù but this label is a bit confusing because they don‚Äôt check the manipulation, they are more like a check of careful attention to the instructions.<a href="12-collection.html#fnref13" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn14"><p>Including this information means you are ‚Äúconditioning on a post-stratification variable,‚Äù as we described in Chapter <a href="7-models.html#models">7</a>. In medicine, analysts distinguish ‚Äúintent-to-treat‚Äù analysis, where you analyze data from everyone you gave a drug, and ‚Äúas treated‚Äù analysis, where you analyze data depending on how much of the drug people actually took. In general, intent-to-treat gives you the generalizable causal estimate. In our current situation, if you include compliance as a covariate, you are essential doing an ‚Äúas treated‚Äù analysis and your estimate can be biased as a result.<a href="12-collection.html#fnref14" class="footnote-back">‚Ü©Ô∏é</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="11-prereg.html"><button class="btn btn-default">Previous</button></a>
<a href="13-management.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<link href="www/global.css" rel="stylesheet">
<script src="www/global.js"></script>


</body>
</html>
