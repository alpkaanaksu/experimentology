<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 17 Meta-analysis | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 17 Meta-analysis | Experimentology">

<title>Chapter 17 Meta-analysis | Experimentology</title>

<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />




<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="toc/toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-intro.html#intro"><span class="toc-section-number">1</span> Experiments and theories</a></li>
<li><a href="2-replication.html#replication"><span class="toc-section-number">2</span> Replication and reproducibility</a></li>
<li><a href="3-ethics.html#ethics"><span class="toc-section-number">3</span> Ethics</a></li>
<li class="part"><span><b>II Statistics</b></span></li>
<li><a href="4-estimation.html#estimation"><span class="toc-section-number">4</span> Estimation</a></li>
<li><a href="5-inference.html#inference"><span class="toc-section-number">5</span> Inference</a></li>
<li><a href="6-models.html#models"><span class="toc-section-number">6</span> Models</a></li>
<li class="part"><span><b>III Design and Planning</b></span></li>
<li><a href="7-measurement.html#measurement"><span class="toc-section-number">7</span> Measurement</a></li>
<li><a href="8-design.html#design"><span class="toc-section-number">8</span> Design of experiments</a></li>
<li><a href="9-sampling.html#sampling"><span class="toc-section-number">9</span> Sampling</a></li>
<li><a href="10-prereg.html#prereg"><span class="toc-section-number">10</span> Preregistration</a></li>
<li class="part"><span><b>IV Execution</b></span></li>
<li><a href="11-selection.html#selection"><span class="toc-section-number">11</span> Experimental strategy</a></li>
<li><a href="12-collection.html#collection"><span class="toc-section-number">12</span> Data collection</a></li>
<li><a href="13-management.html#management"><span class="toc-section-number">13</span> Project management</a></li>
<li class="part"><span><b>V Analysis and Reporting</b></span></li>
<li><a href="14-viz.html#viz"><span class="toc-section-number">14</span> Visualization</a></li>
<li><a href="15-eda.html#eda"><span class="toc-section-number">15</span> Exploratory data analysis</a></li>
<li><a href="16-writing.html#writing"><span class="toc-section-number">16</span> Reproducible writing</a></li>
<li><a href="17-meta.html#meta"><span class="toc-section-number">17</span> Meta-analysis</a></li>
<li><a href="18-conclusions.html#conclusions"><span class="toc-section-number">18</span> Conclusions</a></li>
<li class="part"><span><b>VI Appendices</b></span></li>
<li><a href="19-git.html#git"><span class="toc-section-number">19</span> GitHub Tutorial</a></li>
<li><a href="20-rmarkdown.html#rmarkdown"><span class="toc-section-number">20</span> R Markdown Tutorial</a></li>
<li><a href="21-tidyverse.html#tidyverse"><span class="toc-section-number">21</span> Tidyverse Tutorial</a></li>
<li><a href="22-ggplot.html#ggplot"><span class="toc-section-number">22</span> ggplot Tutorial</a></li>
<li><a href="23-instructors.html#instructors"><span class="toc-section-number">23</span> Instructor‚Äôs guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="meta" class="section level1" number="17">
<h1><span class="header-section-number">Chapter 17</span> Meta-analysis</h1>
<!-- NC: May need to remind the reader that we are using SMD as the effect size -->
<div class="learning-goals">
<p>üçé Learning goals:</p>
<ul>
<li>Understand the benefits of synthesizing evidence across studies.</li>
<li>Understand the pitfalls of intuitive approaches to evidence synthesis, with a focus on how many of these issues are addressed via meta-analysis.</li>
<li>Conduct a simple fixed- or random-effects meta analysis.</li>
<li>Understand that within- and across-study biases affect not only individual studies, but also meta-analysis.</li>
</ul>
</div>
<div class="case-study">
<p>üî¨ Case study: Hotel towel reuse (Scheibehenne, Jamil, and Wagenmakers 2016). A simple example of aggregating noisy evidence from replications.</p>
<p>In a widely-cited study on the power of describing social norms, <span class="citation"><a href="#ref-goldstein2008room" role="doc-biblioref">Goldstein et al.</a> (<a href="#ref-goldstein2008room" role="doc-biblioref">2008</a>)</span> examined the effect of social norm messaging on hotel towel reuse. Across two studies, they found that guests were significantly more likely to reuse their towels after receiving a social norm message that stated most other people reuse their towels (Study 1 <span class="math inline">\(p = .05\)</span>, Study 2 <span class="math inline">\(p = .03\)</span>). However, five subsequent studies consistently failed to indicate that social norm messages increased hotel towel reuse (all <span class="math inline">\(p &gt; .05\)</span>).</p>
<p>At first glance, you might conclude that these studies indicate that social norm messages do not reliably impact towel reuse. You might even go a step further and try to think of explanations for why one team found the effect, but others did not. (Maybe the messages change behavior when guest receive nice fluffy towels, but not when they receive cheap, low-quality towels.) However, this is a case where binary thinking about <span class="math inline">\(p\)</span>-values can mislead us. When <span class="citation"><a href="#ref-scheibehenne2016" role="doc-biblioref">Scheibehenne et al.</a> (<a href="#ref-scheibehenne2016" role="doc-biblioref">2016</a>)</span> combined the evidence via a <em>meta-analysis</em>, they found that the results across these studies were, for the most part, quite consistent. Participants who received a social norm message were slightly more likely to reuse their towels in four out of five of the studies‚Äìbut most of the studies simply did not have large enough samples for this difference to be statistically significant. When combined, however, meta-analysis indicated that social norm messages did have a significant overall effect on hotel towel reuse.</p>
</div>
<p>When students hear the term ‚Äúreview,‚Äù they often have flashbacks to times where they threw some search term into Google Scholar, downloaded a bunch of articles that looked interesting, spent a few days reading those articles, and then wrote a summary of what they learned. This is one of many ways of performing a review (see <span class="citation"><a href="#ref-grant2009typology" role="doc-biblioref">Grant &amp; Booth</a> (<a href="#ref-grant2009typology" role="doc-biblioref">2009</a>)</span>), but this chapter focuses on a specific type of quantitative review called <em>meta-analysis</em>.</p>
<p>One of the central parts of the meta-analysis is extracting <em>effect sizes</em> from individual studies and then combining these effect sizes. (Feel free to check back to Chapter <a href="4-estimation.html#estimation">4</a> if you need a refresher on effect sizes. We‚Äôll wait here for you, we promise.) By combining information from multiple studies, meta-analysis often provides more precise estimates of an effect size than any single study. In addition, meta-analysis also allows the researcher to look at the extent to which an effect varies across studies. If an effect does vary across studies, meta-analysis can be used to test whether certain study characteristics systematically produce different results (e.g., whether an effect is larger in certain world regions).</p>
<p>Meta-analysis often teaches us something about a body of evidence that we do not intuitively grasp when we casually read through a bunch of articles. Indeed, in the above case study, merely reading all the studies that tested the effect of social norm messaging on hotel towel re-use would give the impression that the effect is (at best) unreliable. However, meta-analysis indicated that the results across studies are not as inconsistent as they seem‚Äìand that social norm messaging did decrease towel re-use overall. Given the number of hotel bookings worldwide‚Äì1.7 billion in the European Union alone in 2013 <span class="citation"><a href="#ref-kotzeva2015eurostat" role="doc-biblioref">Kotzeva et al.</a> (<a href="#ref-kotzeva2015eurostat" role="doc-biblioref">2015</a>)</span>‚Äìthe unique insight provided by the meta-analysis is not at all trivial!</p>
<p>The hotel towel example is a good starting point but there is much more we can do with meta-analysis. To illustrate, we‚Äôll turn to another example: a meta-analysis of an idea called the <em>contact hypothesis</em>. According to contact hypothesis, prejudice can be reduced when members of majority and minority groups come together in the pursuit of a common goal <span class="citation"><a href="#ref-allport1954nature" role="doc-biblioref">Allport</a> (<a href="#ref-allport1954nature" role="doc-biblioref">1954</a>)</span>. To examine whether this type of intergroup contact can mitigate prejudice in the real world, <span class="citation"><a href="#ref-paluck2019contact" role="doc-biblioref">Paluck et al.</a> (<a href="#ref-paluck2019contact" role="doc-biblioref">2019</a>)</span> performed a meta-analysis of studies that tested the effects of randomly-assigned intergroup contact interventions on prejudice-related outcomes. Cohen‚Äôs <span class="math inline">\(d\)</span>‚Äìwhich, if you recall from Chapter <a href="4-estimation.html#estimation">4</a>, represents the standardized mean difference‚Äìwas used as the effect size index. As we show in the remainder of the chapter, the meta-analytic tools <span class="citation"><a href="#ref-paluck2019contact" role="doc-biblioref">Paluck et al.</a> (<a href="#ref-paluck2019contact" role="doc-biblioref">2019</a>)</span> used provide several useful insights about this proposed prejudice-reduction intervention.</p>
<div id="the-basics-of-evidence-synthesis" class="section level2" number="17.1">
<h2><span class="header-section-number">17.1</span> The basics of evidence synthesis</h2>
<div id="how-not-to-synthesize-evidence" class="section level3" number="17.1.1">
<h3><span class="header-section-number">17.1.1</span> How <em>not</em> to synthesize evidence</h3>
<p>When considering evidence across multiple studies, many people intuitively count how many studies supported versus did not support the hypothesis under investigation. This usually amounts to counting the number of studies with ‚Äúsignificant‚Äù <span class="math inline">\(p\)</span>-values, since (for better or for worse) ‚Äúsignificance‚Äù is largely what drives the take-home conclusions researchers report <span class="citation">(<a href="#ref-mcshane2017statistical" role="doc-biblioref">McShane &amp; Gal, 2017</a>; <a href="#ref-nelson1986interpretation" role="doc-biblioref">Nelson et al., 1986</a>)</span>. In meta-analysis, we call this practice of counting the number of significant <span class="math inline">\(p\)</span>-values <em>vote-counting</em> <span class="citation">(<a href="#ref-borenstein2021introduction" role="doc-biblioref">Borenstein et al., 2021</a>)</span>. For example, in the <span class="citation"><a href="#ref-paluck2019contact" role="doc-biblioref">Paluck et al.</a> (<a href="#ref-paluck2019contact" role="doc-biblioref">2019</a>)</span> meta-analysis, all studies yielded positive effect sizes, but only approximately 12 of 27 were significant. So, based on this vote-count, we would have the impression that most studies do not support the contact hypothesis.</p>
<p>Many literature reviews use this vote-counting approach‚Äìalbeit often not explicitly. Despite its intuitive appeal, though, vote-counting can be very misleading because it characterizes evidence solely in terms of dichotomized <span class="math inline">\(p\)</span>-values, while entirely ignoring effect sizes. In Chapter <a href="2-replication.html#replication">2</a>, we saw how this fetishism of statistical significance can mislead us when we consider individual studies. These problems propagate to evidence syntheses if we simply vote-count statistical significance across studies as well. For example, small studies on large effects may consistently produce non-significant effects that, when combined in a large meta-analysis, provide strong evidence of an effect. Inversely, it is also possible for a meta-analysis to not provide strong evidence of an effect despite several studies finding statistically significant effect size estimates. In these cases, vote-counting could lead us badly astray <span class="citation">(<a href="#ref-borenstein2021introduction" role="doc-biblioref">Borenstein et al., 2021</a>)</span>. To avoid these pitfalls, meta-analysis combines the effect sizes estimates from each study.</p>
</div>
<div id="combining-results-across-studies-using-fixed-effect-meta-analysis" class="section level3" number="17.1.2">
<h3><span class="header-section-number">17.1.2</span> Combining results across studies using fixed-effect meta-analysis</h3>
<p>Once you are ready to combine results across studies, you have to figure out how you will do so. A seemingly reasonable approach would be to simply average the effect size estimates from each study. For example, in Paluck et al.‚Äôs meta-analysis, the mean of the studies‚Äô effect size estimates is 0.44. This approach is a step in the right direction, but it has an important limitation. Specifically, simply averaging effect size estimates gives equal weight to each study. A small study (like <span class="citation"><a href="#ref-clunies1989changing" role="doc-biblioref">Clunies-Ross &amp; O‚Äômeara</a> (<a href="#ref-clunies1989changing" role="doc-biblioref">1989</a>)</span>, with a sample size of 1243) contributes as much to the mean effect size as a large study (like <span class="citation"><a href="#ref-boisjoly2006empathy" role="doc-biblioref">Boisjoly et al.</a> (<a href="#ref-boisjoly2006empathy" role="doc-biblioref">2006</a>)</span>, with a sample size of 30). This is problematic‚Äìas we know that larger studies typically provide more accurate estimates of effect sizes. Thus, larger studies should carry more weight in the analysis. This brings us to a common approach to principled evidence synthesis, <em>fixed-effect meta-analysis</em>.</p>
<!-- NC: I'm not really sure if this forest plot adds much to the explanation. As is, I worry that it may confuse more than it informs  -->
<p>As we‚Äôve seen throughout this book, visualizing data before and after analysis helps benchmark and sanity-check our intuitions about the formal statistical results. In a meta-analysis, a common way to do so is the <strong>forest plot</strong>, which depicts individual studies‚Äô estimates and confidence intervals. (You can ignore for now the column of percentages and the final line, ‚ÄúRE Model‚Äù; we will return to these later.) In this plot, the larger squares correspond to more precise studies; notice how much narrower their confidence intervals are than the confidence intervals of less precise studies.</p>
<div class="figure"><span style="display:block;" id="fig:meta-forest"></span>
<p class="caption marginnote shownote">
Figure 17.1: Forest plot for Paluck et al.¬†meta-analysis. Studies are ordered from smallest to largest standard error.
</p>
<img src="experimentology_files/figure-html/meta-forest-1.png" alt="Forest plot for Paluck et al. meta-analysis. Studies are ordered from smallest to largest standard error." width="\linewidth"  />
</div>
<p>Fixed-effect meta-analysis uses a <em>weighted-average</em>, wherein larger, more precise studies are given more weight in the calculation of the overall effect size. Specifically, studies are weighed by the inverse of their variance (i.e., the inverse of their squared standard error). This makes sense because larger, more precise studies have smaller variances, and thus get more weight in the analysis. The fixed-effect pooled estimate is:</p>
<p><span class="math display">\[\widehat{\mu} = \frac{ \sum_{i=1}^k w_i \widehat{\theta}_i}{\sum_{i=1}^k w_i}\]</span> where <span class="math inline">\(k\)</span> is the number of studies, <span class="math inline">\(\widehat{\theta}_i\)</span> is the point estimate of the <span class="math inline">\(i^{th}\)</span> study, and <span class="math inline">\(w_i = 1/\widehat{\sigma}^2_i\)</span> is study <span class="math inline">\(i\)</span>‚Äôs weight in the analysis (i.e., the inverse of its variance).<label for="tufte-sn-1" class="margin-toggle sidenote-number">1</label><input type="checkbox" id="tufte-sn-1" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">1</span> If you are curious, the standard error of the fixed-effect <span class="math inline">\(\widehat{\mu}\)</span> is <span class="math inline">\(\frac{1}{\sum_{i=1}^k w_i}\)</span>. This can be used to construct a confidence interval or <span class="math inline">\(p\)</span>-value.</span></p>
<p>In Paluck et al.‚Äôs meta-analysis, we would calculate the fixed-effect estimate, <span class="math inline">\(\widehat{\mu}\)</span>, as:</p>
<!-- hard-coded from DF$d[1:2] and DF$se_d[1:2] -->
<p><span class="math display">\[\widehat{\mu} = \frac{ \frac{\widehat{\theta}_{study1}}{\widehat{\sigma}^2_{study1}} + \frac{\widehat{\theta}_{study2}}{\widehat{\sigma}^2_{study2}} + \cdots}{ \frac{1}{\widehat{\sigma}^2_{study1}} + \frac{1}{\widehat{\sigma}^2_{study2}} + \cdots } =
\frac{ \frac{0.03}{0.08^2} + \frac{0.30}{0.08^2} + \cdots }{ \frac{1}{0.08^2} + \frac{1}{0.08^2} + \cdots }\]</span></p>
<p>We thus estimate that the overall effect size in these studies is a standardized mean difference of <span class="math inline">\(\widehat{\mu}\)</span> = 0.28; 95% confidence interval [0.23, 0.34]; <span class="math inline">\(p=\)</span> &lt; .001. Because cohen‚Äôs <span class="math inline">\(d\)</span> is our effect size index, this means that intergroup contact decreased prejudice by 0.28 standard deviations.</p>
</div>
<div id="limitations-of-fixed-effects-meta-analysis" class="section level3" number="17.1.3">
<h3><span class="header-section-number">17.1.3</span> Limitations of fixed-effects meta-analysis</h3>
<p>One of the limitations of fixed-effect meta-analysis is that it assumes that the true effect size is, well, <em>fixed</em> across studies. In other words, fixed-effect meta-analysis assumes that there is a single effect size that all studies are estimating. This is a bold assumption. For example, imagine that intergroup contact (a) decreases prejudice when the groups succeed at their goal, but (b) increases prejudice when the groups fail at their goal. If we combined one study that had intergroups fail at their goal with another study that had intergroups succeed at their goal, it would appear that the true effect of intergroup contact is zero. However, is it reasonable to assume that these studies are examining the same fixed effect? Is it perhaps more useful to think of the <em>multiple</em> effects of intergroup contact (e.g., one effect for intergroup success and one effect for intergroup failure?)</p>
<p>In Paluck et al.‚Äôs meta-analysis, studies differed in several ways that could lead to different true effects. For example, some studies recruited adult participants while others recruited children. If we assume that intergroup contact works different for adults vs.¬†children, then it is misleading to talk about a single intergroup contact effect. Instead, we would say that the effects of intergroup contact varies across studies: an idea called <strong>heterogeneity</strong>.</p>
<p>Does this presence of heterogeneity remind you of anything from when we analyzed repeated-measures data in Chapter <a href="6-models.html#models">6</a> on models? Recall that, with repeated-measures data, we had dealt with the possibility of heterogeneity across participants by introducing participant-level random intercepts to our regression model. Turns out that we can do a similar thing in meta-analysis!</p>
</div>
<div id="random-effects-meta-analysis" class="section level3" number="17.1.4">
<h3><span class="header-section-number">17.1.4</span> Random-effects meta-analysis</h3>
<p>Whereas fixed-effect meta-analysis essentially assumes that all studies in the meta-analysis have the same population effect size, <span class="math inline">\(\mu\)</span>, random-effects meta-analysis instead postulates that studies‚Äô population effects come from a normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\tau\)</span>.<label for="tufte-sn-2" class="margin-toggle sidenote-number">2</label><input type="checkbox" id="tufte-sn-2" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">2</span> Technically, other specifications of random-effects meta-analysis are possible. For example, robust variance estimation does not require making assumptions about the distribution of effects across studies <span class="citation">(<a href="#ref-hedges2010robust" role="doc-biblioref">Hedges et al., 2010</a>)</span>. These approaches also have other substantial advantages, like their ability to handle effects that are clustered (e.g., because some papers contribute multiple estimates; <span class="citation">(<a href="#ref-hedges2010robust" role="doc-biblioref">Hedges et al., 2010</a>)</span>; <span class="citation">(<a href="#ref-pustejovsky2021meta" role="doc-biblioref">Pustejovsky &amp; Tipton, 2021</a>)</span>) and their ability to provide better inference in meta-analyses with relatively few studies <span class="citation">(<a href="#ref-tipton2015small" role="doc-biblioref">Tipton, 2015</a>)</span>. For these reasons, we tend to use these methods by default when conducting meta-analyses.</span> The larger the standard deviation, <span class="math inline">\(\tau\)</span>, the more heterogeneous the effects are across studies. A random-effects model then estimates both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span>, for example by maximum likelihood (<span class="citation"><a href="#ref-dersimonian1986meta" role="doc-biblioref">DerSimonian &amp; Laird</a> (<a href="#ref-dersimonian1986meta" role="doc-biblioref">1986</a>)</span>; <span class="citation"><a href="#ref-brockwell2001comparison" role="doc-biblioref">Brockwell &amp; Gordon</a> (<a href="#ref-brockwell2001comparison" role="doc-biblioref">2001</a>)</span>).<label for="tufte-sn-3" class="margin-toggle sidenote-number">3</label><input type="checkbox" id="tufte-sn-3" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">3</span> A confidence interval and <span class="math inline">\(p\)</span>-value for the random-effects estimate <span class="math inline">\(\widehat{\mu}\)</span> can be obtained using standard theory for maximum likelihood estimates with an additional adjustment that helps account for uncertainty in estimating <span class="math inline">\(\tau\)</span> <span class="citation">(<a href="#ref-knapp2003improved" role="doc-biblioref">Knapp &amp; Hartung, 2003</a>)</span>.</span></p>
<p>Like fixed-effect meta-analysis, the random-effects estimate of <span class="math inline">\(\widehat{\mu}\)</span> is still a weighted average of studies‚Äô effect size estimates: <span class="math display">\[\widehat{\mu} = \frac{ \sum_{i=1}^k w_i \widehat{\theta}_i}{\sum_{i=1}^k w_i}\]</span></p>
<p>However, in random-effects meta-analysis, the inverse-variance weights now incorporate heterogeneity: <span class="math inline">\(w_i = 1/\left(\widehat{\tau}^2 + \widehat{\sigma}^2_i \right)\)</span>. These weights represent the inverse of studies‚Äô <em>marginal</em> variances, comprising not only statistical error due to their finite sample sizes (<span class="math inline">\(\widehat{\sigma}^2_i\)</span>) but also genuine effect heterogeneity (<span class="math inline">\(\widehat{\tau}^2\)</span>).<label for="tufte-sn-4" class="margin-toggle sidenote-number">4</label><input type="checkbox" id="tufte-sn-4" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">4</span> The estimate of <span class="math inline">\(\widehat{\tau}^2\)</span> is a bit more complicated, but is essentially a weighted average of studies‚Äô residuals, <span class="math inline">\(\widehat{\theta_i} - \widehat{\mu}\)</span>, while subtracting away variation due to statistical error, <span class="math inline">\(\widehat{\sigma}^2_i\)</span> <span class="citation">(<a href="#ref-brockwell2001comparison" role="doc-biblioref">Brockwell &amp; Gordon, 2001</a>; <a href="#ref-dersimonian1986meta" role="doc-biblioref">DerSimonian &amp; Laird, 1986</a>)</span>.</span></p>
</div>
<div id="reporting-on-heterogeneity" class="section level3" number="17.1.5">
<h3><span class="header-section-number">17.1.5</span> Reporting on heterogeneity</h3>
<p>Remember that in random-effects meta-analysis, the estimate <span class="math inline">\(\widehat{\mu}\)</span> represents only the <em>mean</em> population effect across studies. It tells us nothing about how much the effects <em>vary</em> across studies. Thus, we recommend always reporting the heterogeneity estimate <span class="math inline">\(\widehat{\tau}\)</span>, perhaps supplemented by other related metrics (<span class="citation"><a href="#ref-riley2011interpretation" role="doc-biblioref">Riley et al.</a> (<a href="#ref-riley2011interpretation" role="doc-biblioref">2011</a>)</span>; <span class="citation"><a href="#ref-wang2019simple" role="doc-biblioref">Wang &amp; Lee</a> (<a href="#ref-wang2019simple" role="doc-biblioref">2019</a>)</span>; <span class="citation"><a href="#ref-mathur_mam" role="doc-biblioref">Mathur &amp; VanderWeele</a> (<a href="#ref-mathur_mam" role="doc-biblioref">2019</a>)</span>; <span class="citation"><a href="#ref-npphat" role="doc-biblioref">Mathur &amp; VanderWeele</a> (<a href="#ref-npphat" role="doc-biblioref">2020a</a>)</span>). Reporting the heterogeneity indicates how consistent or inconsistent the effects are across studies, which may point to the need to investigate <em>moderators</em> of the effect (i.e., factors that can cause a shift in the size of the effect, such as the type of intergroup contact intervention).<label for="tufte-sn-5" class="margin-toggle sidenote-number">5</label><input type="checkbox" id="tufte-sn-5" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">5</span> One common approach to investigating moderators in meta-analysis is meta-regression, in which moderators (e.g., type of intergroup contact) are included as covariates in a random-effects meta-analysis model <span class="citation">(<a href="#ref-thompson2002should" role="doc-biblioref">Thompson &amp; Higgins, 2002</a>)</span>. As in standard regression, coefficients can then be estimated for each moderator, representing the mean difference in population effect between studies with versus without the moderator.</span></p>
</div>
<div id="applied-example" class="section level3" number="17.1.6">
<h3><span class="header-section-number">17.1.6</span> Applied example</h3>
<p>Conducting a random-effects meta-analysis of Paluck et al.‚Äôs dataset yields <span class="math inline">\(\widehat{\mu}\)</span> = 0.4 ; 95% confidence interval [ 0.2, 0.61 ]; <span class="math inline">\(p=\)</span> &lt; .001. That is, they estimated that, <em>on average across studies</em>, intergroup contact was associated with a decrease in prejudice of 0.4 standard deviations. However, these effects appeared to differ considerably across studies; they estimated that the standard deviation of effects across studies was <span class="math inline">\(\widehat{\tau}\)</span> = 0.44 ; 95% confidence interval [0.25, 0.57]. To conveniently visualize these results, we can plot the estimated density of the population effects, which is just a normal distribution with mean <span class="math inline">\(\widehat{\mu}\)</span> and standard deviation <span class="math inline">\(\widehat{\tau}\)</span>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span style="display:block;" id="fig:meta-densities"></span>
<img src="experimentology_files/figure-html/meta-densities-1.png" alt="Estimated distribution of population effects from random-effects meta-analysis of Paluck et. al's dataset (heavy red curve) and estimated density of studies' point estimates (thin black curve)." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 17.2: Estimated distribution of population effects from random-effects meta-analysis of Paluck et. al‚Äôs dataset (heavy red curve) and estimated density of studies‚Äô point estimates (thin black curve).<!--</p>-->
<!--</div>--></span>
</p>
</div>
</div>
<div id="bias-in-meta-analysis" class="section level2" number="17.2">
<h2><span class="header-section-number">17.2</span> Bias in meta-analysis</h2>
<p>Meta-analysis is an invaluable tool to synthesize evidence across studies. However, meta-analyses can be compromised by two categories of bias: <em>within-study biases</em> and <em>across-study biases</em>. Either type of bias can lead to meta-analysis estimates that are too large, too small, or in the wrong direction. We will now discuss examples of each type of bias. We will also discuss ways to address these biases when conducting a meta-analysis, including mitigating the biases at the outset through sound meta-analysis design and also assessing the robustness of the ultimate conclusions to possible remaining bias.</p>
<div id="within-study-biases" class="section level3" number="17.2.1">
<h3><span class="header-section-number">17.2.1</span> Within-study biases</h3>
<p>Within-study biases‚Äìsuch as demand characteristics, confounds, and order effects‚Äìnot only impact the validity of individual studies, but also any attempt to synthesize those studies. In other words: garbage in, garbage out. For example, <span class="citation"><a href="#ref-paluck2019contact" role="doc-biblioref">Paluck et al.</a> (<a href="#ref-paluck2019contact" role="doc-biblioref">2019</a>)</span> noted that early studies on intergroup contact almost exclusively used nonrandomized designs. Imagine a hypothetical study where researchers (a) are studying a completely ineffective intergroup contact intervention, and (b) non-randomly assign low-prejudice people to the intergroup contact condition and high-prejudice people to the control condition. In a scenario like this, the researcher will, of course, find that the prejudice was lower in the intergroup contact condition. However, this is not a true effect of the contact intervention, but rather a spurious effect of non-random assignment. Now imagine repeating this across many studies and then performing a meta-analysis. The meta-analyst would find impressive evidence of an intergroup contact effect, but this is simply driven by systematic non-random assignment. Once again: you put garbage in, you get garbage out.</p>
<p>To mitigate this problem, meta-analysts often exclude studies that may be affected by within-study bias. For example, <span class="citation"><a href="#ref-paluck2019contact" role="doc-biblioref">Paluck et al.</a> (<a href="#ref-paluck2019contact" role="doc-biblioref">2019</a>)</span> excluded nonrandomized studies to avoid concerns about confounding. After data have been collected, meta-analysts often also qualitatively assess studies‚Äô risks of bias using established rating tools <span class="citation">(<a href="#ref-art" role="doc-biblioref">Mathur &amp; VanderWeele, 2022</a>)</span>. Doing so allows the meta-analyst to communicate just how much within-study bias there may be. <span class="citation"><a href="#ref-paluck2019contact" role="doc-biblioref">Paluck et al.</a> (<a href="#ref-paluck2019contact" role="doc-biblioref">2019</a>)</span> did not use this tool, but they could have used it to communicate for example, the extent to which participants may have differentially dropped out of the study. Meta-analysts also often conduct sensitivity analyses that examine how much meta-analysis results change when excluding certain types of studies <span class="citation">(<a href="#ref-art" role="doc-biblioref">Mathur &amp; VanderWeele, 2022</a>)</span>. For example, if nonrandom assignment is a concern, a meta-analysts may run the analyses with and without studies using nonrandom assignment in order to determine if including these studies changes the meta-analysis results.</p>
<!-- It would be nice to show the reader an example of a risk of bias rating scale. Doing so would also help clarify what types of within-study biases meta-analysts are typically concerned about. https://drive.google.com/file/d/1Q4Fk3HCuBRwIDWTGZa5oH11OdR4Gbhdo/view-->
</div>
<div id="across-study-biases" class="section level3" number="17.2.2">
<h3><span class="header-section-number">17.2.2</span> Across-study biases</h3>
<p>Across-study biases refer to behaviors like researchers selecting reporting certain types of findings (<em>selective reporting</em>) or selectively publishing certain types of findings (<em>publication bias</em>; see below accident report). Often, these across-study biases favor statistically significant positive results, which means the meta-analysis of the available results will be inflated. For example, if researchers publish only the studies that yield statistically significant positive results and hide the studies that don‚Äôt, statistically combining the published studies via meta-analysis will obviously lead to exaggerated effect size estimates. Bias in, bias out‚Äìgarbage in, garbage out.</p>
<div class="accident-report">
<p>üî¨ Accident Report: Quantifying publication bias in the social sciences</p>
<p>In 2014, <span class="citation"><a href="#ref-franco2014" role="doc-biblioref">Franco et al.</a> (<a href="#ref-franco2014" role="doc-biblioref">2014</a>)</span> and colleagues examined the population of 221 studies conducted through a funding initiative that helps researchers run experiments on nationally-representative samples in the U.S. First, Franco and colleagues examined the records of these studies to determine whether the researchers found statistically significant results, a mixture of statistically significant and nonsignificant results, or only nonsignificant results. Then, Franco and colleagues examined the likelihood that these results were published in the scientific literature.</p>
<p>What results do you think were most likely to be published? If you guessed ‚Äúsignificant results‚Äù you have a good intuition about publication bias. Over 60% of studies with statistically significant results were published, compared to the mere 25% of studies that produced only statistically non-significant results.</p>
</div>
<p>Like within-study biases, meta-analysts often try to mitigate across-study biases by being careful about what studies make it into the meta-analysis. Meta-analysts don‚Äôt only want to capture high-profile, published studies on their effect of interest, but also studies published in low-profile journals and the so-called ‚Äúgray literature‚Äù (i.e., unpublished dissertations and theses; <span class="citation"><a href="#ref-lefebvre2019searching" role="doc-biblioref">Lefebvre et al.</a> (<a href="#ref-lefebvre2019searching" role="doc-biblioref">2019</a>)</span>].<label for="tufte-sn-6" class="margin-toggle sidenote-number">6</label><input type="checkbox" id="tufte-sn-6" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">6</span> Evidence is mixed regarding whether including gray literature actually reduces across-study biases in meta-analysis <span class="citation">(<a href="#ref-sapbe" role="doc-biblioref">Mathur &amp; VanderWeele, 2021</a>; <a href="#ref-tsuji2020addressing" role="doc-biblioref">Tsuji et al., 2020</a>)</span>, but it is still common practice to try to oinclude this literature.</span></p>
<p>There are also statistical methods to help assess how robust the results may be to across-study biases. Among the most popular tools to assess and correct for publication bias is the <strong>funnel plot</strong> <span class="citation">(<a href="#ref-duval2000trim" role="doc-biblioref">Duval &amp; Tweedie, 2000</a>; <a href="#ref-egger1997bias" role="doc-biblioref">Egger et al., 1997</a>)</span>, a graph relating the meta-analyzed studies‚Äô point estimates to some measure of their precision, such as sample size or standard error. Here is an example of a type of funnel plot <span class="citation">(<a href="#ref-sapb" role="doc-biblioref">Mathur &amp; VanderWeele, 2020b</a>)</span> for a simulated meta-analysis of 100 studies with no publication bias:</p>
<!-- MM: I used a type of enhanced funnel here that I believe helps visualize publication bias better than standard funnels. Mike, up to you if you want regular funnels instead. I've included commented code for those as well. I didn't describe the purpose of the grey diamonds for now. Text snippet if we decide to briefly mention that: "the grey diamonds represent worst-case point estimate within only the studies with $p \ge 0.05$ or negative estimates." -->
<!-- NC: I like the funnel plot, but it keeps duplicating the plots for some reason" -->
<div class="figure"><span style="display:block;" id="fig:meta-funnel-unbiased-1"></span>
<p class="caption marginnote shownote">
Figure 17.3: Significance funnel plot for a meta-analysis simulated to have no publication bias. Orange points: studies with <span class="math inline">\(p&lt;0.05\)</span> and positive estimates. Grey points: studies with <span class="math inline">\(p ge 0.05\)</span> or negative estimates. Black diamond: random-effects estimate of <span class="math inline">\(Mhat\)</span> within all studies.
</p>
<img src="experimentology_files/figure-html/meta-funnel-unbiased-1.png" alt="Significance funnel plot for a meta-analysis simulated to have no publication bias. Orange points: studies with $p&lt;0.05$ and positive estimates. Grey points: studies with $p ge 0.05$ or negative estimates. Black diamond: random-effects estimate of $Mhat$ within all studies." width="\linewidth"  />
</div>
<div class="figure"><span style="display:block;" id="fig:meta-funnel-unbiased-2"></span>
<p class="caption marginnote shownote">
Figure 17.4: Significance funnel plot for a meta-analysis simulated to have no publication bias. Orange points: studies with <span class="math inline">\(p&lt;0.05\)</span> and positive estimates. Grey points: studies with <span class="math inline">\(p ge 0.05\)</span> or negative estimates. Black diamond: random-effects estimate of <span class="math inline">\(Mhat\)</span> within all studies.
</p>
<img src="experimentology_files/figure-html/meta-funnel-unbiased-2.png" alt="Significance funnel plot for a meta-analysis simulated to have no publication bias. Orange points: studies with $p&lt;0.05$ and positive estimates. Grey points: studies with $p ge 0.05$ or negative estimates. Black diamond: random-effects estimate of $Mhat$ within all studies." width="\linewidth"  />
</div>
<p>Notice that larger studies (those with smaller standard errors) cluster more closely around the mean of 0.34 than do smaller studies, but large and small studies alikehave point estimates centered around the mean. That is, the funnel plot is symmetric. In contrast, a funnel plot might look asymmetric. Here is what happens to our hypothetical meta-analysis if all studies with <span class="math inline">\(p&lt;0.05\)</span> and positive estimates are published, but only 10% of studies with <span class="math inline">\(p \ge 0.05\)</span> or with negative estimates are published:</p>
<div class="figure"><span style="display:block;" id="fig:meta-funnel-biased-1"></span>
<p class="caption marginnote shownote">
Figure 17.5: Significance funnel plot for the same simulated meta-analysis after publication bias has occurred. Orange points: studies with <span class="math inline">\(p&lt;0.05\)</span> and positive estimates. Grey points: studies with <span class="math inline">\(p ge 0.05\)</span> or negative estimates. Black diamond: random-effects estimate of <span class="math inline">\(Mhat\)</span> within all studies.
</p>
<img src="experimentology_files/figure-html/meta-funnel-biased-1.png" alt="Significance funnel plot for the same simulated meta-analysis after publication bias has occurred. Orange points: studies with $p&lt;0.05$ and positive estimates. Grey points: studies with $p ge 0.05$ or negative estimates. Black diamond: random-effects estimate of $Mhat$ within all studies." width="\linewidth"  />
</div>
<div class="figure"><span style="display:block;" id="fig:meta-funnel-biased-2"></span>
<p class="caption marginnote shownote">
Figure 17.6: Significance funnel plot for the same simulated meta-analysis after publication bias has occurred. Orange points: studies with <span class="math inline">\(p&lt;0.05\)</span> and positive estimates. Grey points: studies with <span class="math inline">\(p ge 0.05\)</span> or negative estimates. Black diamond: random-effects estimate of <span class="math inline">\(Mhat\)</span> within all studies.
</p>
<img src="experimentology_files/figure-html/meta-funnel-biased-2.png" alt="Significance funnel plot for the same simulated meta-analysis after publication bias has occurred. Orange points: studies with $p&lt;0.05$ and positive estimates. Grey points: studies with $p ge 0.05$ or negative estimates. Black diamond: random-effects estimate of $Mhat$ within all studies." width="\linewidth"  />
</div>
<p>Notice that the introduction of publication has dramatically inflated the pooled estimate from 0.34 to 1.15.[^8] Also, there appears to be a correlation between studies‚Äô estimates and their standard errors, such that smaller studies tend to have larger estimates than do larger studies. This is often called <em>funnel plot asymmetry</em>.</p>
<p>Several popular statistical methods, such as Trim-and-Fill <span class="citation">(<a href="#ref-duval2000trim" role="doc-biblioref">Duval &amp; Tweedie, 2000</a>)</span> and Egger‚Äôs regression <span class="citation">(<a href="#ref-egger1997bias" role="doc-biblioref">Egger et al., 1997</a>)</span> are designed to quantify funnel plot asymmetry. However, one limitation is that funnel plot asymmetry is not always representative of publication bias. Indeed, sometimes funnel plot asymmetry is driven by genuine differences in the effects being studied in small and large studies <span class="citation">(<a href="#ref-egger1997bias" role="doc-biblioref">Egger et al., 1997</a>; <a href="#ref-lau2006case" role="doc-biblioref">Lau et al., 2006</a>)</span>. For example, in a meta-analysis of intervention studies, if the most effective interventions are also the most expensive or difficult to implement, these highly effective interventions might be used primarily in the smallest studies. Thus, funnel plot methods may be indicative of publication bias‚Äìbut they may more simply driven by small-study effects <span class="citation">(<a href="#ref-smt" role="doc-biblioref">Maier et al., 2021</a>)</span>.<label for="tufte-sn-7" class="margin-toggle sidenote-number">7</label><input type="checkbox" id="tufte-sn-7" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">7</span> Essentially, funnel plots and most related methods can detect publication bias in which (1) small studies with large positive point estimates are more likely to be published than small studies with small or negative point estimates; and (2) the largest studies are published regardless of the magnitude of their point estimates. Funnel plots may not detect publication bias that favors significant results. For more detail on these points, see <span class="citation"><a href="#ref-smt" role="doc-biblioref">Maier et al.</a> (<a href="#ref-smt" role="doc-biblioref">2021</a>)</span>.</span> For these reasons, assessments of publication bias in meta-analysis should rely not only on funnel plots <span class="citation">(<a href="#ref-smt" role="doc-biblioref">Maier et al., 2021</a>)</span>. There are numerous other methods that can be applied as well, such as <strong>selection models</strong>.<label for="tufte-sn-8" class="margin-toggle sidenote-number">8</label><input type="checkbox" id="tufte-sn-8" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">8</span> High-level overviews of selection models are given in <span class="citation"><a href="#ref-mcshane2016adjusting" role="doc-biblioref">McShane et al.</a> (<a href="#ref-mcshane2016adjusting" role="doc-biblioref">2016</a>)</span> and <span class="citation"><a href="#ref-smt" role="doc-biblioref">Maier et al.</a> (<a href="#ref-smt" role="doc-biblioref">2021</a>)</span>. For more methodological detail, see <span class="citation"><a href="#ref-hedges1984estimation" role="doc-biblioref">Hedges</a> (<a href="#ref-hedges1984estimation" role="doc-biblioref">1984</a>)</span>, <span class="citation"><a href="#ref-iyengar1988" role="doc-biblioref">Iyengar &amp; Greenhouse</a> (<a href="#ref-iyengar1988" role="doc-biblioref">1988</a>)</span>, and <span class="citation"><a href="#ref-vevea1995" role="doc-biblioref">Vevea &amp; Hedges</a> (<a href="#ref-vevea1995" role="doc-biblioref">1995</a>)</span>. For a tutorial on fitting and interpreting selection models, see <span class="citation"><a href="#ref-smt" role="doc-biblioref">Maier et al.</a> (<a href="#ref-smt" role="doc-biblioref">2021</a>)</span>.</span> These models can detect other forms of publication bias that funnel plots may not detect, such as publication bias that favors significant results.</p>
<p>You may also have heard of ‚Äú<span class="math inline">\(p\)</span>-methods‚Äù to detect across-study biases such as <span class="math inline">\(p\)</span>-curve and <span class="math inline">\(p\)</span>-uniform <span class="citation">(<a href="#ref-simonsohn2014p" role="doc-biblioref">Simonsohn et al., 2014b</a>, <a href="#ref-simonsohn2014b" role="doc-biblioref">2014a</a>; <a href="#ref-van2015meta" role="doc-biblioref">Van Assen et al., 2015</a>)</span>. These methods essentially assess whether the significant <span class="math inline">\(p\)</span>-values ‚Äúbunch up‚Äù just under 0.05, which is taken to indicate publication bias. These methods are increasingly popular in psychology and have merits. However, it is important to note that these methods are actually simplified versions of selection models <span class="citation">(e.g., <a href="#ref-hedges1984estimation" role="doc-biblioref">Hedges, 1984</a>)</span> that work only under considerably more restrictive settings than do the original selection models [for example, when there is not heterogeneity across studies; <span class="citation"><a href="#ref-mcshane2016adjusting" role="doc-biblioref">McShane et al.</a> (<a href="#ref-mcshane2016adjusting" role="doc-biblioref">2016</a>)</span>]. For this reason, it is usually (although not always) better to use selection models in place of the more restrictive <span class="math inline">\(p\)</span>-methods.</p>
<div id="applied-example-1" class="section level4" number="17.2.2.1">
<h4><span class="header-section-number">17.2.2.1</span> Applied example</h4>
<p>In Paluck and colleagues‚Äô meta-analysis, they used a regression-based approach to assess and correct for publication bias. This approach provided significant evidence of a relationship between the standard error and effect size (i.e., evidence of an asymmetric funnel plot). This asymettric funnel plot, of course, could be driven by small study effects, but it may be indicative of publication bias. Paluck and colleague subsequently used this same regression-based approach to try to correct for potential publication bias. Results from this model indicated that the bias-corrected effect size estimate was close to zero. Consequently, Paluck and colleagues concluded that publication bias may be a problem. In other words, even though all studies estimated that intergroup contact decreased prejudice, it is possible that there are studies that did not find this (or found that intergroup contact increased prejudice).</p>
<p>Taken together, Paluck and colleagues‚Äô use of meta-analysis provided several important insights that would have been easy to miss in a non-quantitative review:
1. Despite a preponderance of non-significant findings, real-world intergroup contact interventions are estimated to significant decrease prejudice by 0.4 standard deviations.
2. There is significant heterogeneity in intergroup contact effects, suggesting that there are important moderators of the effectiveness of these interventions.
3. Publication bias is a concern, suggesting the need for follow-up research that will be published regardless of the outcome.</p>
<div class="accident-report">
<p>‚ö†Ô∏è Garbage in, garbage out? Meta-analyzing bad research (Coles et al.¬†2019)</p>
<p>You may have heard that Botox can help eliminate wrinkles. But some researchers have also suggested that it may help treat clinical depression when used to paralyze the muscles associated with frowning. As crazy as they may sound, a quick examination of the literature would lead many to conclude that this treatment works. Studies that randomly assign depressed patients to either receive Botox injections or saline injections do indeed find that Botox recipients exhibit decreases in depression. And when you combine all available evidence in a meta-analysis, you find that this difference is quite large: <span class="math inline">\(\widehat{d}\)</span> = 0.83, 95% CI [0.52, 1.14].</p>
<p>However, <span class="citation"><a href="#ref-coles2019does" role="doc-biblioref">Coles et al.</a> (<a href="#ref-coles2019does" role="doc-biblioref">2019</a>)</span> pointed out that there is a problem with within-study bias. Participants are not supposed to know whether they have been randomly assigned to receive Botox or a control saline injections. However, only one of these treatments leads the upper half of your face to be paralyzed, and after a couple of weeks you‚Äôre likely to figure out whether you received the Botox treatment or control saline injection. Thus, it is possible that the apparent effect of Botox on depression is actually an effect of placebo. The meta-analytic conclusions are potentially undermined by within-study bias.</p>
<p><span class="citation"><a href="#ref-coles2019does" role="doc-biblioref">Coles et al.</a> (<a href="#ref-coles2019does" role="doc-biblioref">2019</a>)</span> also found evidence of between-study bias. When coding the effect sizes in the literature, they found that 51% of the outcomes measured were not reported by the study authors. For example, researchers may have collected two measures of depression, but only reported one in the manuscript. This raises concerns about selective reporting: that researchers examining the effects of Botox on depression are only reporting the outcomes that demonstrate an effect, while hiding away the outcomes that do not. In this scenario, any meta-analytic conclusions are potentially undermined by between-study bias.</p>
<p><span class="citation"><a href="#ref-coles2019does" role="doc-biblioref">Coles et al.</a> (<a href="#ref-coles2019does" role="doc-biblioref">2019</a>)</span> provides a great example of the ‚Äúgarbage in, garbage out‚Äù problem we alluded to throughout this chapter. If within- and between-study bias is not properly mitigated, it is difficult to conclude make valid inferences in a meta-analysis.</p>
</div>
<!-- ## Summary -->
<div class="exercise">
<p><span id="exr:unlabeled-div-1" class="exercise"><strong>Exercise 17.1  </strong></span>Imagine that you read the following result in the abstract of a meta-analysis: ‚ÄúIn 83 randomized studies of middle school children, replacing one hour of class time with mindfulness meditation significantly improved standardized test scores (standardized mean difference <span class="math inline">\(\widehat{\mu} = 0.05\)</span>; 95% confidence interval: <span class="math display">\[$0.01, 0.09$\]</span>; <span class="math inline">\(p&lt;0.05\)</span>).‚Äù Why is this a problematic way to report on meta-analysis results? Suggest a better sentence to replace this one.</p>
<p>As you read the rest of the meta-analysis, you find that the authors conclude that ‚ÄúThese findings demonstrate robust benefits of meditation for children, suggesting that test scores improve even when the meditation is introduced as a replacement for normal class time.‚Äù You recall that the heterogeneity estimate was <span class="math inline">\(\widehat{\tau} = 0.90\)</span>. Do you think that this result regarding the heterogeneity tends to support, or rather tends to undermine, the concluding sentence of the meta-analysis? Why?</p>
<p>What kinds of within-study biases would concern you in this meta-analysis? How might you assess the credibility of the meta-analyzed studies and of the meta-analysis as whole in light of these possible biases?</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-2" class="exercise"><strong>Exercise 17.2  </strong></span>Imagine you conduct a meta-analysis on a literature in which statistically significant results in either direction are much more likely to be published that nonsignificant results. Draw the funnel plot you would expect to see. Is the plot funnel symmetric or asymmetric?</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-3" class="exercise"><strong>Exercise 17.3  </strong></span>Why do you think small studies receive more weight in random-effects meta-analysis than in fixed-effects meta-analysis? Can you see why this is true mathematically based on the equations given above, and can you also explain the intuition in simple language?</p>
</div>

</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-allport1954nature" class="csl-entry">
Allport, G. W. (1954). <em>The nature of prejudice</em>.
</div>
<div id="ref-boisjoly2006empathy" class="csl-entry">
Boisjoly, J., Duncan, G. J., Kremer, M., Levy, D. M., &amp; Eccles, J. (2006). Empathy or antipathy? The impact of diversity. <em>American Economic Review</em>, <em>96</em>(5), 1890‚Äì1905.
</div>
<div id="ref-borenstein2021introduction" class="csl-entry">
Borenstein, M., Hedges, L. V., Higgins, J. P., &amp; Rothstein, H. R. (2021). <em>Introduction to meta-analysis</em>. John Wiley &amp;amp; Sons.
</div>
<div id="ref-brockwell2001comparison" class="csl-entry">
Brockwell, S. E., &amp; Gordon, I. R. (2001). A comparison of statistical methods for meta-analysis. <em>Statistics in Medicine</em>, <em>20</em>(6), 825‚Äì840.
</div>
<div id="ref-clunies1989changing" class="csl-entry">
Clunies-Ross, G., &amp; O‚Äômeara, K. (1989). Changing the attitudes of students towards peers with disabilities. <em>Australian Psychologist</em>, <em>24</em>(2), 273‚Äì284.
</div>
<div id="ref-coles2019does" class="csl-entry">
Coles, N. A., Larsen, J. T., Kuribayashi, J., &amp; Kuelz, A. (2019). Does blocking facial feedback via botulinum toxin injections decrease depression? A critical review and meta-analysis. <em>Emotion Review</em>, <em>11</em>(4), 294‚Äì309.
</div>
<div id="ref-dersimonian1986meta" class="csl-entry">
DerSimonian, R., &amp; Laird, N. (1986). Meta-analysis in clinical trials. <em>Controlled Clinical Trials</em>, <em>7</em>(3), 177‚Äì188.
</div>
<div id="ref-duval2000trim" class="csl-entry">
Duval, S., &amp; Tweedie, R. (2000). Trim and fill: A simple funnel-plot‚Äìbased method of testing and adjusting for publication bias in meta-analysis. <em>Biometrics</em>, <em>56</em>(2), 455‚Äì463. <a href="https://doi.org/10.1111/j.0006-341X.2000.00455.x">https://doi.org/10.1111/j.0006-341X.2000.00455.x</a>
</div>
<div id="ref-egger1997bias" class="csl-entry">
Egger, M., Smith, G. D., Schneider, M., &amp; Minder, C. (1997). Bias in meta-analysis detected by a simple, graphical test. <em>BMJ</em>, <em>315</em>(7109), 629‚Äì634. <a href="https://doi.org/10.1136/bmj.315.7109.629">https://doi.org/10.1136/bmj.315.7109.629</a>
</div>
<div id="ref-franco2014" class="csl-entry">
Franco, A., Malhotra, N., &amp; Simonovits, G. (2014). Publication bias in the social sciences: Unlocking the file drawer. In <em>Science</em> (No. 6203; Vol. 345, pp. 1502‚Äì1505).
</div>
<div id="ref-goldstein2008room" class="csl-entry">
Goldstein, N. J., Cialdini, R. B., &amp; Griskevicius, V. (2008). A room with a viewpoint: Using social norms to motivate environmental conservation in hotels. <em>Journal of Consumer Research</em>, <em>35</em>(3), 472‚Äì482.
</div>
<div id="ref-grant2009typology" class="csl-entry">
Grant, M. J., &amp; Booth, A. (2009). A typology of reviews: An analysis of 14 review types and associated methodologies. <em>Health Information &amp; Libraries Journal</em>, <em>26</em>(2), 91‚Äì108.
</div>
<div id="ref-hedges1984estimation" class="csl-entry">
Hedges, L. V. (1984). Estimation of effect size under nonrandom sampling: The effects of censoring studies yielding statistically insignificant mean differences. <em>Journal of Educational Statistics</em>, <em>9</em>(1), 61‚Äì85. <a href="https://doi.org/10.3102/10769986009001061">https://doi.org/10.3102/10769986009001061</a>
</div>
<div id="ref-hedges2010robust" class="csl-entry">
Hedges, L. V., Tipton, E., &amp; Johnson, M. C. (2010). Robust variance estimation in meta-regression with dependent effect size estimates. <em>Research Synthesis Methods</em>, <em>1</em>(1), 39‚Äì65.
</div>
<div id="ref-iyengar1988" class="csl-entry">
Iyengar, S., &amp; Greenhouse, J. B. (1988). Selection models and the file drawer problem. <em>Statistical Science</em>, 109‚Äì117.
</div>
<div id="ref-knapp2003improved" class="csl-entry">
Knapp, G., &amp; Hartung, J. (2003). Improved tests for a random effects meta-regression with a single covariate. <em>Statistics in Medicine</em>, <em>22</em>(17), 2693‚Äì2710.
</div>
<div id="ref-kotzeva2015eurostat" class="csl-entry">
Kotzeva, M., Brandm√ºller, T., &amp; √ñnnerfors, √Ö. (2015). <em>Eurostat regional yearbook 2015</em>. Publications Office of the European Union.
</div>
<div id="ref-lau2006case" class="csl-entry">
Lau, J., Ioannidis, J. P., Terrin, N., Schmid, C. H., &amp; Olkin, I. (2006). The case of the misleading funnel plot. <em>BMJ</em>, <em>333</em>(7568), 597‚Äì600. <a href="https://doi.org/10.1136/bmj.333.7568.597">https://doi.org/10.1136/bmj.333.7568.597</a>
</div>
<div id="ref-lefebvre2019searching" class="csl-entry">
Lefebvre, C., Glanville, J., Briscoe, S., Littlewood, A., Marshall, C., Metzendorf, M.-I., Noel-Storr, A., Rader, T., Shokraneh, F., Thomas, J.others. (2019). Searching for and selecting studies. <em>Cochrane Handbook for Systematic Reviews of Interventions</em>, 67‚Äì107.
</div>
<div id="ref-smt" class="csl-entry">
Maier, M., VanderWeele, T. J., &amp; Mathur, M. B. (2021). Using selection models to assess sensitivity to publication bias: A tutorial and call for more routine use. <em>Under Review</em>.
</div>
<div id="ref-mathur_mam" class="csl-entry">
Mathur, M. B., &amp; VanderWeele, T. J. (2019). New metrics for meta-analyses of heterogeneous effects. <em>Statistics in Medicine</em>, <em>38</em>(8), 1336‚Äì1342.
</div>
<div id="ref-npphat" class="csl-entry">
Mathur, M. B., &amp; VanderWeele, T. J. (2020a). Robust metrics and sensitivity analyses for meta-analyses of heterogeneous effects. <em>Epidemiology</em>, <em>31</em>(3), 356‚Äì358.
</div>
<div id="ref-sapb" class="csl-entry">
Mathur, M. B., &amp; VanderWeele, T. J. (2020b). Sensitivity analysis for publication bias in meta-analyses. <em>Journal of the Royal Statistical Society: Series C</em>, <em>5</em>(69), 1091‚Äì1119.
</div>
<div id="ref-sapbe" class="csl-entry">
Mathur, M. B., &amp; VanderWeele, T. J. (2021). Estimating publication bias in meta-analyses of peer-reviewed studies: A meta-meta-analysis across disciplines and journal tiers. <em>Research Synthesis Methods</em>, <em>12</em>(2), 176‚Äì191.
</div>
<div id="ref-art" class="csl-entry">
Mathur, M. B., &amp; VanderWeele, T. J. (2022). Methods to address confounding and other biases in meta-analyses: Review and recommendations. <em>Annual Review of Public Health</em>.
</div>
<div id="ref-mcshane2016adjusting" class="csl-entry">
McShane, B. B., B√∂ckenholt, U., &amp; Hansen, K. T. (2016). Adjusting for publication bias in meta-analysis: An evaluation of selection methods and some cautionary notes. <em>Perspectives on Psychological Science</em>, <em>11</em>(5), 730‚Äì749. <a href="https://doi.org/10.1177/1745691616662243">https://doi.org/10.1177/1745691616662243</a>
</div>
<div id="ref-mcshane2017statistical" class="csl-entry">
McShane, B. B., &amp; Gal, D. (2017). Statistical significance and the dichotomization of evidence. <em>Journal of the American Statistical Association</em>, <em>112</em>(519), 885‚Äì895. <a href="https://doi.org/10.1080/01621459.2017.1289846">https://doi.org/10.1080/01621459.2017.1289846</a>
</div>
<div id="ref-nelson1986interpretation" class="csl-entry">
Nelson, N., Rosenthal, R., &amp; Rosnow, R. L. (1986). Interpretation of significance levels and effect sizes by psychological researchers. <em>American Psychologist</em>, <em>41</em>(11), 1299.
</div>
<div id="ref-paluck2019contact" class="csl-entry">
Paluck, E. L., Green, S. A., &amp; Green, D. P. (2019). The contact hypothesis re-evaluated. <em>Behavioural Public Policy</em>, <em>3</em>(2), 129‚Äì158.
</div>
<div id="ref-pustejovsky2021meta" class="csl-entry">
Pustejovsky, J. E., &amp; Tipton, E. (2021). Meta-analysis with robust variance estimation: Expanding the range of working models. <em>Prevention Science</em>, 1‚Äì14.
</div>
<div id="ref-riley2011interpretation" class="csl-entry">
Riley, R. D., Higgins, J. P., &amp; Deeks, J. J. (2011). Interpretation of random effects meta-analyses. <em>BMJ</em>, <em>342</em>.
</div>
<div id="ref-scheibehenne2016" class="csl-entry">
Scheibehenne, B., Jamil, T., &amp; Wagenmakers, E.-J. (2016). Bayesian evidence synthesis can reconcile seemingly inconsistent results: The case of hotel towel reuse. <em>Psychol. Sci.</em>, <em>27</em>(7), 1043‚Äì1046.
</div>
<div id="ref-simonsohn2014b" class="csl-entry">
Simonsohn, U., Nelson, L. D., &amp; Simmons, J. P. (2014a). P-curve and effect size: Correcting for publication bias using only significant results. <em>Perspectives on Psychological Science</em>, <em>9</em>(6), 666‚Äì681.
</div>
<div id="ref-simonsohn2014p" class="csl-entry">
Simonsohn, U., Nelson, L. D., &amp; Simmons, J. P. (2014b). P-curve: A key to the file-drawer. <em>Journal of Experimental Psychology: General</em>, <em>143</em>(2), 534.
</div>
<div id="ref-thompson2002should" class="csl-entry">
Thompson, S. G., &amp; Higgins, J. P. (2002). How should meta-regression analyses be undertaken and interpreted? <em>Statistics in Medicine</em>, <em>21</em>(11), 1559‚Äì1573.
</div>
<div id="ref-tipton2015small" class="csl-entry">
Tipton, E. (2015). Small sample adjustments for robust variance estimation with meta-regression. <em>Psychological Methods</em>, <em>20</em>(3), 375.
</div>
<div id="ref-tsuji2020addressing" class="csl-entry">
Tsuji, S., Cristia, A., Frank, M. C., &amp; Bergmann, C. (2020). Addressing publication bias in meta-analysis. <em>Zeitschrift f<span>√º</span>r Psychologie</em>.
</div>
<div id="ref-van2015meta" class="csl-entry">
Van Assen, M. A., Aert, R. van, &amp; Wicherts, J. M. (2015). Meta-analysis using effect size distributions of only statistically significant studies. <em>Psychological Methods</em>, <em>20</em>(3), 293.
</div>
<div id="ref-vevea1995" class="csl-entry">
Vevea, J. L., &amp; Hedges, L. V. (1995). A general linear model for estimating effect size in the presence of publication bias. <em>Psychometrika</em>, <em>60</em>(3), 419‚Äì435.
</div>
<div id="ref-wang2019simple" class="csl-entry">
Wang, C.-C., &amp; Lee, W.-C. (2019). A simple method to estimate prediction intervals and predictive distributions: Summarizing meta-analyses beyond means and confidence intervals. <em>Research Synthesis Methods</em>, <em>10</em>(2), 255‚Äì266.
</div>
</div>
<p style="text-align: center;">
<a href="16-writing.html"><button class="btn btn-default">Previous</button></a>
<a href="18-conclusions.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="toc/toc.js"></script>


</body>
</html>
