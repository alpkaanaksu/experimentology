<!DOCTYPE html>
<html>

<head>


<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 16 Meta-analysis | Experimentology" />
<meta property="og:type" content="book" />




<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 16 Meta-analysis | Experimentology">

<title>Chapter 16 Meta-analysis | Experimentology</title>

<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.18/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>



<link rel="stylesheet" type="text/css" href="/assets/index.page.c5e7dffa.css"><link rel="modulepreload" as="script" type="text/javascript" href="/assets/src/index.page.client.jsx.49dc7b35.js"><link rel="modulepreload" as="script" type="text/javascript" href="/assets/contents.d440020f.js"></head>

<body>




<div class="row">
<div class="col-sm-12">
<header class="_toc_1lnsy_1" id="toc"><a class="_book_title_1lnsy_24" href="/">Experimentology: An Open Science Approach to Experimental Psychology Methods</a><nav><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Preliminaries</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="1-experiments">Experiments</a><a class="_chapter_title_1lnsy_32" href="2-theories">Theories</a><a class="_chapter_title_1lnsy_32" href="3-replication">Replication and reproducibility</a><a class="_chapter_title_1lnsy_32" href="4-ethics">Ethics</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Statistics</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="5-estimation">Estimation</a><a class="_chapter_title_1lnsy_32" href="6-inference">Inference</a><a class="_chapter_title_1lnsy_32" href="7-models">Models</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Design</div><div class="_part_title_rest_1lnsy_32"> and Planning</div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="8-measurement">Measurement</a><a class="_chapter_title_1lnsy_32" href="9-design">Design of experiments</a><a class="_chapter_title_1lnsy_32" href="10-sampling">Sampling</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Execution</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="11-prereg">Preregistration</a><a class="_chapter_title_1lnsy_32" href="12-collection">Data collection</a><a class="_chapter_title_1lnsy_32" href="13-management">Project management</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Analysis</div><div class="_part_title_rest_1lnsy_32"> and Reporting</div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="14-viz">Visualization</a><a class="_chapter_title_1lnsy_32" href="15-writing">Writing</a><a class="_chapter_title_1lnsy_32" href="16-meta">Meta-analysis</a><a class="_chapter_title_1lnsy_32" href="17-conclusions">Conclusions</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Appendices</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="A-git">GitHub Tutorial</a><a class="_chapter_title_1lnsy_32" href="B-rmarkdown">R Markdown Tutorial</a><a class="_chapter_title_1lnsy_32" href="C-tidyverse">Tidyverse Tutorial</a><a class="_chapter_title_1lnsy_32" href="D-ggplot">ggplot Tutorial</a><a class="_chapter_title_1lnsy_32" href="E-instructors">Instructor’s guide</a></div></div></nav></header>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="meta" class="section level1" number="16">
<h1><span class="header-section-number">Chapter 16</span> Meta-analysis</h1>
<div class="box learning_goals"><div class="Collapsible"><span id="collapsible-trigger-1655314617939" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1655314617939" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="apple-whole" class="svg-inline--fa fa-apple-whole " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M336 128c-32 0-80.02 16.03-112 32.03c-32.01-16-79.1-32.02-111.1-32.03C32 128 .4134 210.5 .0033 288c-.5313 99.97 63.99 224 159.1 224c32 0 48-16 64-16c16 0 32 16 64 16c96 0 160.4-122.8 159.1-224C447.7 211.6 416 128 336 128zM320 32V0h-32C243.8 0 208 35.82 208 80v32h32C284.2 112 320 76.18 320 32z"></path></svg>Learning goals<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M169.4 278.6C175.6 284.9 183.8 288 192 288s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25s-32.75-12.5-45.25 0L192 210.8L54.63 73.38c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25L169.4 278.6zM329.4 265.4L192 402.8L54.63 265.4c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25l160 160C175.6 476.9 183.8 480 192 480s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25S341.9 252.9 329.4 265.4z"></path></svg></span><div id="collapsible-content-1655314617939" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1655314617939"><div class="Collapsible__contentInner">
<ul>
<li>Discuss the benefits of synthesizing evidence across studies</li>
<li>Conduct a simple fixed- or random-effects meta analysis</li>
<li>Reason about the role of within- and across-study biases in meta-analysis</li>
</ul>
</div></div></div></div>
<p>Throughout this book, we have focused on how to design individual experiments that maximize the precision of our effect size estimates and minimize their bias. But even when we do our best to get a precise, unbiased estimate in an individual experiment, we must also acknowledge that one study can never be definitive. For example, variability in participant demographics, stimuli, and experimental methods may limit the generalizability of our findings. Additionally, even well-powered individual studies have some amount of statistical error, limiting their precision.</p>
<p>For this reason, synthesizing evidence across studies is critical for developing a balanced and appropriately evolving view of the overall evidence on an effect of interest and for understanding sources of variation in the effect. Synthesizing evidence rigorously is not simply a matter of throwing a search term into Google Scholar, downloading articles that look topical or interesting, and qualitatively summarizing your impressions of those studies. While this ad hoc method can be an essential first step in performing a review of the literature <span class="citation">(<a href="#ref-grant2009typology" role="doc-biblioref">Grant &amp; Booth, 2009</a>)</span>, it is not systematic, and it doesn’t provide any kind of quantitative summary of a particular effect. Further, it doesn’t tell you anything about potential biases in the literature – for example, a bias for the publication of positive effects.</p>
<p>To address these issues, a more systematic, quantitative review of the literature is often more informative. This chapter focuses on a specific type of quantitative review called <strong>meta-analysis</strong>. Meta-analysis is a method for combining effect sizes across different measurements. (If you need a refresher on effect size, see Chapter <a href="5-estimation.html#estimation">5</a>, where we introduce the concept).<a href="#fn240" class="footnote-ref" id="fnref240"><sup>240</sup></a></p>
<p>By combining information from multiple studies, meta-analysis often provides more precise estimates of an effect size than any single study. In addition, meta-analysis also allows the researcher to look at the extent to which an effect varies across studies. If an effect does vary across studies, meta-analysis also can be used to test whether certain study characteristics systematically produce different results (e.g., whether an effect is larger in certain populations).</p>
<div class="box case_study"><div class="Collapsible"><span id="collapsible-trigger-1655314617939" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1655314617939" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="microscope" class="svg-inline--fa fa-microscope " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M160 320h12v16c0 8.875 7.125 16 16 16h40c8.875 0 16-7.125 16-16V320H256c17.62 0 32-14.38 32-32V64c0-17.62-14.38-32-32-32V16C256 7.125 248.9 0 240 0h-64C167.1 0 160 7.125 160 16V32C142.4 32 128 46.38 128 64v224C128 305.6 142.4 320 160 320zM464 448h-1.25C493.2 414 512 369.2 512 320c0-105.9-86.13-192-192-192v64c70.63 0 128 57.38 128 128s-57.38 128-128 128H48C21.5 448 0 469.5 0 496C0 504.9 7.125 512 16 512h480c8.875 0 16-7.125 16-16C512 469.5 490.5 448 464 448zM104 416h208c4.375 0 8-3.625 8-8v-16c0-4.375-3.625-8-8-8h-208C99.63 384 96 387.6 96 392v16C96 412.4 99.63 416 104 416z"></path></svg>Case study<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M169.4 278.6C175.6 284.9 183.8 288 192 288s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25s-32.75-12.5-45.25 0L192 210.8L54.63 73.38c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25L169.4 278.6zM329.4 265.4L192 402.8L54.63 265.4c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25l160 160C175.6 476.9 183.8 480 192 480s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25S341.9 252.9 329.4 265.4z"></path></svg></span><div id="collapsible-content-1655314617939" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1655314617939"><div class="Collapsible__contentInner"><p class="title">Towel reuse by hotel guests</p>
<p></p>
<p>Imagine you are staying in a hotel and you have just taken a shower. Do you throw the towels on the floor or hang them back up again? In a widely-cited study on the power of social norms, <span class="citation">Goldstein et al. (<a href="#ref-goldstein2008room" role="doc-biblioref">2008</a>)</span> manipulated whether a sign encouraging guests to reuse towels focused environmental impacts (e.g., “help reduce water use”) or social norms (e.g., “most guests re-use their towels”). Across two studies, they found that guests were significantly more likely to reuse their towels after receiving the social norm message (Study 1: odds ratio [OR] = 1.46, 95% CI [1.00, 2.16], <span class="math inline">\(p = .05\)</span>; Study 2: OR = 1.35, 95% CI [1.04, 1.77], <span class="math inline">\(p = .03\)</span>).</p>
<p>However, five subsequent studies by other researchers did not find significant evidence of that social norm messaging increased towel reuse (with ORs ranging from 0.22 to 1.34, and no hypothesis-consistent <span class="math inline">\(p\)</span>-values was less than .05). This caused many researchers to wonder if there is any effect at all. To examine this question, <span class="citation">Scheibehenne et al. (<a href="#ref-scheibehenne2016" role="doc-biblioref">2016</a>)</span> statistically combined all the evidence via meta-analysis. When they did, they found that the social norm messages did have a significant average effect on hotel towel reuse (OR = 1.26, 95% CI [1.07, 1.46], <span class="math inline">\(p &lt; .005\)</span>). This study demonstrates an important strength of meta-analysis: by pooling evidence from multiple studies, meta-analysis can generate more powerful insights than any one study alone. We will also see how meta-analysis can be used to assess variability in effects across studies.</p>
</div></div></div></div>

<p>Meta-analysis often teaches us something about a body of evidence that we do not intuitively grasp when we casually read through a bunch of articles. In this example, merely reading the individual studies might give the impression that social norm messages do not increase hotel towel re-use. But meta-analysis indicated that the average effect is beneficial, although there might be substantial variation in effect sizes across studies.<a href="#fn241" class="footnote-ref" id="fnref241"><sup>241</sup></a></p>
<div id="the-basics-of-evidence-synthesis" class="section level2" number="16.1">
<h2><span class="header-section-number">16.1</span> The basics of evidence synthesis</h2>
<p>As we explore the details of conducting a meta-analysis, we’ll turn to another example: a meta-analysis of studies investigating the “contact hypothesis” on intergroup relations.</p>
<p>According to the contact hypothesis, prejudice towards members of minority groups can be reduced through intergroup contact interventions, in which members of majority and minority groups work together to pursue a common goal <span class="citation">(<a href="#ref-allport1954nature" role="doc-biblioref">Allport, 1954</a>)</span>.To test this, <span class="citation">Paluck et al. (<a href="#ref-paluck2019contact" role="doc-biblioref">2019</a>)</span> meta-analyzed randomized studies that tested the effects of intergroup contact interventions on long-term prejudice-related outcomes. Using a systematic literature search, they tried to identify all published papers that tested these effects and then extracted effect size estimates from each paper. Because not every paper reports standardized effect sizes – or even means and standard deviations for every group – this process can often involve scraping information from plots, tables, and statistical tests to try to reconstruct effect sizes. This book will not cover the process of conducting a systematic literature search and extracting effect sizes, but these topics are critical to understand if you plan to conduct a meta-analysis or other evidence synthesis.</p>
<p>As we’ve seen throughout this book, visualizing data before and after analysis helps benchmark and check our intuitions about the formal statistical results. In a meta-analysis, a common way to do so is the <strong>forest plot</strong>, which depicts individual studies’ estimates and confidence intervals.<a href="#fn242" class="footnote-ref" id="fnref242"><sup>242</sup></a> In the forest plot in Figure <a href="16-meta.html#fig:meta-forest">16.1</a>, the larger squares correspond to more precise studies; notice how much narrower their confidence intervals are than the confidence intervals of less precise studies.</p>
<div class="figure"><span style="display: block;" id="fig:meta-forest"></span>
<p class="caption marginnote shownote">
Figure 16.1: Forest plot for Paluck et al. meta-analysis. Studies are ordered from smallest to largest standard error.
</p>
<img src="experimentology_files/figure-html/meta-forest-1.png" alt="Forest plot for Paluck et al. meta-analysis. Studies are ordered from smallest to largest standard error." width="\linewidth" />
</div>

<div id="how-not-to-synthesize-evidence" class="section level3" number="16.1.1">
<h3><span class="header-section-number">16.1.1</span> How not to synthesize evidence</h3>
<p>Many people’s first instinct to synthesize evidence is to count how many studies support versus did not support the hypothesis under investigation. This technique usually amounts to counting the number of studies with “significant” <span class="math inline">\(p\)</span>-values, since – for better or for worse – “significance” is largely what drives the take-home conclusions researchers report <span class="citation">(<a href="#ref-mcshane2017statistical" role="doc-biblioref">McShane &amp; Gal, 2017</a>; <a href="#ref-nelson1986interpretation" role="doc-biblioref">N. Nelson et al., 1986</a>)</span>. In meta-analysis, we call this practice of counting the number of significant <span class="math inline">\(p\)</span>-values <strong>vote-counting</strong> <span class="citation">(<a href="#ref-borenstein2021introduction" role="doc-biblioref">Borenstein et al., 2021</a>)</span>. For example, in the <span class="citation">Paluck et al. (<a href="#ref-paluck2019contact" role="doc-biblioref">2019</a>)</span> meta-analysis, all studies had positive effect sizes, but only 12 of 27 were significant. So, based on this vote-count, we would have the impression that most studies do not support the contact hypothesis.</p>
<p>Many qualitative literature reviews use this vote-counting approach, although often not explicitly. Despite its intuitive appeal, vote-counting can be very misleading because it characterizes evidence solely in terms of dichotomized <span class="math inline">\(p\)</span>-values, while entirely ignoring effect sizes. In Chapter <a href="3-replication.html#replication">3</a>, we saw how fetishizing statistical significance can mislead us when we consider individual studies. These problems also apply when considering multiple studies.</p>
<p>For example, small studies may consistently produce non-significant effects due to their limited power. But when many such studies are combined in a meta-analysis, the meta-analysis may provide strong evidence of a positive average effect. Inversely, many studies might have statistically significant effects, but if their effect sizes are small, then a meta-analysis might might indicate that the average effect size is too small to be practically meaningful. In these cases, vote-counting based on statistical significance can lead us badly astray <span class="citation">(<a href="#ref-borenstein2021introduction" role="doc-biblioref">Borenstein et al., 2021</a>)</span>. To avoid these pitfalls, meta-analysis combines the effect sizes estimates from each study (not just their <span class="math inline">\(p\)</span>-values), weighting them in a principled way.</p>
</div>
<div id="fixed-effects-meta-analysis" class="section level3" number="16.1.2">
<h3><span class="header-section-number">16.1.2</span> Fixed-effects meta-analysis</h3>
<p>If vote-counting is a bad idea, how should we combine results across studies? Another intuitive approach might be to average effect sizes from each study. For example, in Paluck et al.’s meta-analysis, the mean of the studies’ effect size estimates is 0.44. This averaging approach is a step in the right direction, but it has an important limitation: averaging effect size estimates gives equal weight to each study. A small study <span class="citation">(e.g., <a href="#ref-clunies1989changing" role="doc-biblioref">Clunies-Ross &amp; O’meara, 1989</a> with N=30)</span> contributes as much to the mean effect size as a large study <span class="citation">(e.g., <a href="#ref-boisjoly2006empathy" role="doc-biblioref">Boisjoly et al., 2006</a> with N=1243)</span>. Larger studies provide more precise estimates of effect sizes than small studies, so weighting all studies equally is not ideal. Instead, larger studies should carry more weight in the analysis.</p>
<p>To address this issue, <strong>fixed-effects meta-analysis</strong> uses a <strong>weighted average</strong> approach. Larger, more precise studies are given more weight in the calculation of the overall effect size. Specifically, each study is weighted by the inverse of its variance (i.e., the inverse of its squared standard error). This makes sense because larger, more precise studies have smaller variances, and thus get more weight in the analysis.</p>
<p>In general terms, the fixed-effect pooled estimate is:</p>
<p><span class="math display">\[\widehat{\mu} = \frac{ \sum_{i=1}^k w_i \widehat{\theta}_i}{\sum_{i=1}^k w_i}\]</span> where <span class="math inline">\(k\)</span> is the number of studies, <span class="math inline">\(\widehat{\theta}_i\)</span> is the point estimate of the <span class="math inline">\(i^{th}\)</span> study, and <span class="math inline">\(w_i = 1/\widehat{\sigma}^2_i\)</span> is study <span class="math inline">\(i\)</span>’s weight in the analysis (i.e., the inverse of its variance).<a href="#fn243" class="footnote-ref" id="fnref243"><sup>243</sup></a></p>




<p>We can use the fixed-effects formula to estimate that the overall effect size in Paluck et al.’s meta-analysis studies is a standardized mean difference of <span class="math inline">\(\widehat{\mu}\)</span> = 0.28; 95% confidence interval [0.23, 0.34]; <span class="math inline">\(p &lt; .001\)</span>. Because Cohen’s <span class="math inline">\(d\)</span> is our effect size index, this estimate means that intergroup contact decreased prejudice by 0.28 standard deviations.</p>
</div>
<div id="limitations-of-fixed-effects-meta-analysis" class="section level3" number="16.1.3">
<h3><span class="header-section-number">16.1.3</span> Limitations of fixed-effects meta-analysis</h3>
<p>One of the limitations of fixed-effect meta-analysis is that it assumes that the true effect size is, well, <em>fixed</em>! In other words, fixed-effect meta-analysis assumes that there is a single effect size that all studies are estimating. This is a stringent assumption. For example, imagine that intergroup contact decreases prejudice when the group succeeds at its goal, but <em>increases</em> prejudice when the group fails at its goal. If we meta-analyzed two studies, one in which intergroup contact substantially increased prejudice, and one in which intergroup contact substantially decreased prejudice, it might appear that the true effect of intergroup contact is close to zero, when in fact both of the meta-analyzed studies have large effects.</p>
<p>In Paluck et al.’s meta-analysis, studies differed in several ways that could lead to different true effects. For example, some studies recruited adult participants while others recruited children. If intergroup contact is more or less effective for adults versus children, then it is misleading to talk about a single (i.e., fixed) intergroup contact effect. Instead, we would say that the effects of intergroup contact vary across studies, an idea called <strong>heterogeneity</strong>.</p>
<p>Does the concept of heterogeneity remind you of anything from when we analyzed repeated-measures data in Chapter <a href="7-models.html#models">7</a> on models? Yes! Recall that, with repeated-measures data, we had to deal with the possibility of heterogeneity across participants – and of the ways we did so was by introducing participant-level random intercepts to our regression model. It turns out that we can do a similar thing in meta-analysis, dealing with heterogeneity across studies.</p>
</div>
<div id="random-effects-meta-analysis" class="section level3" number="16.1.4">
<h3><span class="header-section-number">16.1.4</span> Random-effects meta-analysis</h3>
<p>While fixed-effect meta-analysis essentially assumes that all studies in the meta-analysis have the same population effect size, <span class="math inline">\(\mu\)</span>, random-effects meta-analysis instead postulates that studies’ population effects come from a normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\tau\)</span>.<a href="#fn244" class="footnote-ref" id="fnref244"><sup>244</sup></a> The larger the standard deviation, <span class="math inline">\(\tau\)</span>, the more heterogeneous the effects are across studies. A random-effects model then estimates both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span>, for example by maximum likelihood <span class="citation">(<a href="#ref-brockwell2001comparison" role="doc-biblioref">Brockwell &amp; Gordon, 2001</a>; <a href="#ref-dersimonian1986meta" role="doc-biblioref">DerSimonian &amp; Laird, 1986</a>)</span>.<a href="#fn245" class="footnote-ref" id="fnref245"><sup>245</sup></a></p>
<p>Like fixed-effect meta-analysis, the random-effects estimate of <span class="math inline">\(\widehat{\mu}\)</span> is still a weighted average of studies’ effect size estimates: <span class="math display">\[\widehat{\mu} = \frac{ \sum_{i=1}^k w_i \widehat{\theta}_i}{\sum_{i=1}^k w_i}\]</span></p>
<p>However, in random-effects meta-analysis, the inverse-variance weights now incorporate heterogeneity: <span class="math inline">\(w_i = 1/\left(\widehat{\tau}^2 + \widehat{\sigma}^2_i \right)\)</span>. Where before we had one term in our weights, now we have two. That is because these weights represent the inverse of studies’ <em>marginal</em> variances, taking into account both statistical error due to their finite sample sizes (<span class="math inline">\(\widehat{\sigma}^2_i\)</span>) and also genuine effect heterogeneity (<span class="math inline">\(\widehat{\tau}^2\)</span>).<a href="#fn246" class="footnote-ref" id="fnref246"><sup>246</sup></a></p>
<p>Conducting a random-effects meta-analysis of Paluck et al.’s dataset yields <span class="math inline">\(\widehat{\mu}\)</span> = 0.4; 95% confidence interval [0.2, 0.61]; <span class="math inline">\(p &lt; .001\)</span>. That is, they estimated that, <em>on average across studies</em>, intergroup contact was associated with a decrease in prejudice of 0.4 standard deviations. This meta-analytic estimate is shown as the bottom line of Figure <a href="16-meta.html#fig:meta-forest">16.1</a>.</p>
<p>However, these effects appeared to differ across studies. Paluck et al. estimated that the standard deviation of effects across studies was <span class="math inline">\(\widehat{\tau}\)</span> = 0.44 ; 95% confidence interval [0.25, 0.57]. This estimate indicates a substantial amount of heterogeneity! To conveniently visualize these results, we can plot the estimated density of the population effects, which is just a normal distribution with mean <span class="math inline">\(\widehat{\mu}\)</span> and standard deviation <span class="math inline">\(\widehat{\tau}\)</span> (Figure <a href="16-meta.html#fig:meta-densities">16.2</a>).</p>
<p>This meta-analysis highlights an important point:that the overall effect size estimate <span class="math inline">\(\widehat{\mu}\)</span> represents only the <em>mean</em> population effect across studies. It tells us nothing about how much the effects <em>vary</em> across studies. Thus, we recommend always reporting the heterogeneity estimate <span class="math inline">\(\widehat{\tau}\)</span>, preferably along with other related metrics that help summarize the distribution of effect sizes across studies <span class="citation">(<a href="#ref-mathur_mam" role="doc-biblioref">Mathur &amp; VanderWeele, 2019</a>, <a href="#ref-npphat" role="doc-biblioref">2020b</a>; <a href="#ref-riley2011interpretation" role="doc-biblioref">Riley et al., 2011</a>; <a href="#ref-wang2019simple" role="doc-biblioref">Wang &amp; Lee, 2019</a>)</span>. Reporting the heterogeneity helps readers know how consistent or inconsistent the effects are across studies, which may point to the need to investigate <em>moderators</em> of the effect (i.e., factors that are associated with larger or smaller effects, such as whether participants were adults or children).<a href="#fn247" class="footnote-ref" id="fnref247"><sup>247</sup></a></p>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:meta-densities"></span>
<img src="experimentology_files/figure-html/meta-densities-1.png" alt="Estimated distribution of population effects from random-effects meta-analysis of Paluck et. al's dataset (heavy red curve) and estimated density of studies' point estimates (thin black curve)." width="\linewidth" />
Figure 16.2: Estimated distribution of population effects from random-effects meta-analysis of Paluck et. al’s dataset (heavy red curve) and estimated density of studies’ point estimates (thin black curve).
</span>
</p>
</div>
</div>
<div id="bias-in-meta-analysis" class="section level2" number="16.2">
<h2><span class="header-section-number">16.2</span> Bias in meta-analysis</h2>
<p>Meta-analysis is an invaluable tool for synthesizing evidence across studies. However, the accuracy of meta-analysis can be compromised by two categories of bias: <strong>within-study biases</strong> and <strong>across-study biases</strong>. Either type can lead to meta-analysis estimates that are too large, too small, or in the wrong direction. We will now discuss examples of each type of bias as well as ways to address these biases when conducting a meta-analysis. This includes mitigating the biases at the outset through sound meta-analysis design and also assessing the robustness of the ultimate conclusions to possible remaining bias.</p>
<div id="within-study-biases" class="section level3" number="16.2.1">
<h3><span class="header-section-number">16.2.1</span> Within-study biases</h3>
<p>Within-study biases – such as demand characteristics, confounds, and order effects – not only impact the validity of individual studies, but also any attempt to synthesize those studies. In other words: garbage in, garbage out. For example, <span class="citation">Paluck et al. (<a href="#ref-paluck2019contact" role="doc-biblioref">2019</a>)</span> noted that early studies on intergroup contact almost exclusively used non-randomized designs. Imagine a hypothetical study where researchers studied a completely ineffective intergroup contact intervention, and non-randomly assigned low-prejudice people to the intergroup contact condition and high-prejudice people to the control condition. In a scenario like this, the researcher will, of course, find that the prejudice was lower in the intergroup contact condition. However, this is not a true effect of the contact intervention, but rather a spurious effect of non-random assignment (i.e., confounding). Now imagine meta-analyzing many studies with similarly poor designs. The meta-analyst would find impressive evidence of an intergroup contact effect, but this is simply driven by systematic non-random assignment.</p>
<p>To mitigate this problem, meta-analysts often exclude studies that may be affected by within-study bias. For example, <span class="citation">Paluck et al. (<a href="#ref-paluck2019contact" role="doc-biblioref">2019</a>)</span> excluded non-randomized studies to minimize concerns about confounding. Preferably, inclusion and exclusion criteria for meta-analyses should be preregistered to ensure that these decisions are applied in a systematic and uniform way, rather than being applied after looking at the results.</p>
<p>Sometimes, though, certain sources of bias cannot be eliminated through exclusion criteria, for example because it is not possible or ethical to randomize the independent variable. After data have been collected, meta-analysts should also assess studies’ risks of bias qualitatively using established rating tools <span class="citation">(<a href="#ref-sterne2016robins" role="doc-biblioref">Sterne et al., 2016</a>)</span>. Doing so allows the meta-analyst to communicate how much within-study bias there may be.<a href="#fn248" class="footnote-ref" id="fnref248"><sup>248</sup></a> Meta-analysts can also conduct sensitivity analyses to assess how much results might be affected by different within-study biases or by excluding certain types of studies <span class="citation">(<a href="#ref-art" role="doc-biblioref">Mathur &amp; VanderWeele, 2022</a>)</span>. For example, if nonrandom assignment is a concern, a meta-analysts may run the analyses including only randomized studies, versus including all studies, in order to determine if including nonrandomized studies changes the meta-analysis results. These two options parallel our discussion of experimental preregistration in Chapter <a href="11-prereg.html#prereg">11</a>: To allay concerns about results-dependent meta-analysis, researchers can either pre-register their analyses ahead of time or else be transparent about their choices after the fact. Sensitivity analyses can allay concerns that a specific choice of exclusion criteria is critically related to the reported results.</p>

</div>
<div id="across-study-biases" class="section level3" number="16.2.2">
<h3><span class="header-section-number">16.2.2</span> Across-study biases</h3>
<p>Across-study biases occur if, for example, researchers <strong>selectively report</strong> certain types of findings or selectively publishing certain types of findings (<strong>publication bias</strong>). Often, these across-study biases favor statistically significant positive results, which means the meta-analysis of the available results will be inflated. For example, if researchers publish only the studies that yield statistically significant positive results and hide the studies that don’t, statistically combining the published studies via meta-analysis will obviously lead to exaggerated effect size estimates.</p>
<div class="box accident_report"><div class="Collapsible"><span id="collapsible-trigger-1655314617940" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1655314617940" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="person-falling-burst" class="svg-inline--fa fa-person-falling-burst " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><path fill="currentColor" d="M256 41.84C256 96.45 228.1 146.5 183.5 175.4L183.7 175.8L240.5 255.1H311.1C327.1 255.1 341.3 263.1 350.4 275.2L393.6 332.8C404.2 346.9 401.3 366.1 387.2 377.6C373.1 388.2 353 385.3 342.4 371.2L303.1 319.1H222.6L314.9 462.6C324.5 477.5 320.2 497.3 305.4 506.9C290.5 516.5 270.7 512.2 261.1 497.4L100.5 249.2C97.57 258.4 95.1 268.1 95.1 278.2V351.1C95.1 369.7 81.67 383.1 63.1 383.1C46.33 383.1 31.1 369.7 31.1 351.1V278.2C31.1 213 71.65 154.5 132.1 130.3C168.3 115.8 191.1 80.79 191.1 41.84V32C191.1 14.33 206.3 0 223.1 0C241.7 0 255.1 14.33 255.1 32L256 41.84zM96 79.1C96 106.5 74.51 127.1 48 127.1C21.49 127.1 0 106.5 0 79.1C0 53.49 21.49 31.1 48 31.1C74.51 31.1 96 53.49 96 79.1zM464 286.1L424.7 322.2C423.1 319.3 421.3 316.4 419.2 313.6L382.1 265.3L384.2 247.6L365.8 244.8C351.2 231.5 332.1 223.1 311.1 223.1H292.6C292.5 223.7 292.5 223.4 292.4 223.2C290.1 216.8 293.5 210.1 298.9 206.4L364.5 161.3L325 92.18C321.8 86.49 322.3 79.39 326.4 74.27C330.5 69.14 337.3 67.03 343.6 68.93L419.7 92.05L449.1 18.09C451.6 11.1 457.4 8 464 8C470.6 8 476.4 11.1 478.9 18.09L508.3 92.05L584.4 68.93C590.7 67.03 597.5 69.14 601.6 74.27C605.7 79.39 606.2 86.49 602.1 92.18L563.5 161.3L629.1 206.4C634.5 210.1 637 216.8 635.6 223.2C634.1 229.6 628.9 234.4 622.4 235.4L543.8 247.6L549.4 327C549.8 333.6 546.3 339.7 540.4 342.6C534.5 345.4 527.4 344.4 522.6 339.9L464 286.1z"></path></svg>Accident report<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M169.4 278.6C175.6 284.9 183.8 288 192 288s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25s-32.75-12.5-45.25 0L192 210.8L54.63 73.38c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25L169.4 278.6zM329.4 265.4L192 402.8L54.63 265.4c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25l160 160C175.6 476.9 183.8 480 192 480s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25S341.9 252.9 329.4 265.4z"></path></svg></span><div id="collapsible-content-1655314617940" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1655314617940"><div class="Collapsible__contentInner"><p class="title">Quantifying publication bias in the social sciences</p>
<p></p>
<p>In 2014, <span class="citation">Franco et al. (<a href="#ref-franco2014" role="doc-biblioref">2014</a>)</span> and colleagues examined the population of 221 studies conducted through a funding initiative called TESS (“time-sharing experiments in the social sciences”) that helps researchers run experiments on nationally-representative samples in the U.S. This sample of studies was unique in that everyone that used TESS had to register their study with the group. That meant that Franco et al. knew the whole universe of studies that had been conducted using TESS – a key piece of information that meta-analysts almost never have available.</p>
<p>Using this information, Franco and colleagues examined the records of these studies to determine whether the researchers found statistically significant results, a mixture of statistically significant and non-significant results, or only non-significant results. Then, they examined the likelihood that these results were published in the scientific literature.</p>
<p>Which results do you think were most likely to be published? If you guessed “significant results,” then you have a good intuition about publication bias. Over 60% of studies with statistically significant results were published, compared to the mere 25% of studies that produced only statistically non-significant results. This finding was important because it quantified how strong that bias was, at least for one sample of studies.</p>
</div></div></div></div>
<p>Like within-study biases, meta-analysts often try to mitigate across-study biases by being careful about what studies make it into the meta-analysis. Meta-analysts don’t only want to capture high-profile, published studies on their effect of interest, but also studies published in low-profile journals and the so-called “gray literature” [i.e., unpublished dissertations and theses; <span class="citation">Lefebvre et al. (<a href="#ref-lefebvre2019searching" role="doc-biblioref">2019</a>)</span>].<a href="#fn249" class="footnote-ref" id="fnref249"><sup>249</sup></a></p>
<p>There are also statistical methods to help assess how robust the results may be to across-study biases. Among the most popular tools to assess and correct for publication bias is the <strong>funnel plot</strong> <span class="citation">(<a href="#ref-duval2000trim" role="doc-biblioref">Duval &amp; Tweedie, 2000</a>; <a href="#ref-egger1997bias" role="doc-biblioref">Egger et al., 1997</a>)</span>. A funnel plot shows the relationship between studies’ effect estimates and their precision (usually their standard error). They are called “funnel plots” because if there is no publication bias, then as precision increases, the effects “funnel” towards the meta-analytic estimate. As the precision is smaller, they spread out more because of greater measurement error.</p>
<p>Here is an example of one type of funnel plot <span class="citation">(<a href="#ref-sapb" role="doc-biblioref">Mathur &amp; VanderWeele, 2020c</a>)</span> for a simulated meta-analysis of 100 studies with no publication bias:</p>
<div class="figure"><span style="display: block;" id="fig:meta-funnel-unbiased"></span>
<p class="caption marginnote shownote">
Figure 16.3: Significance funnel plot for a meta-analysis simulated to have no publication bias. Orange points: studies with <span class="math inline">\(p &lt; 0.05\)</span> and positive estimates. Grey points: studies with <span class="math inline">\(p\)</span> <span class="math inline">\(\ge\)</span> <span class="math inline">\(0.05\)</span> or negative estimates. Black diamond: random-effects estimate of <span class="math inline">\(\widehat{\mu}\)</span>.
</p>
<img src="experimentology_files/figure-html/meta-funnel-unbiased-1.png" alt="Significance funnel plot for a meta-analysis simulated to have no publication bias. Orange points: studies with $p &lt; 0.05$ and positive estimates. Grey points: studies with $p$ $\ge$ $0.05$ or negative estimates. Black diamond: random-effects estimate of $\widehat{\mu}$." width="\linewidth" />
</div>
<p>As implied by the “funnel” moniker, our plot looks a little like a funnel. Larger studies (those with smaller standard errors) cluster more closely around the mean of 0.34 than do smaller studies, but large and small studies alike have point estimates centered around the mean. That is, the funnel plot is symmetric.<a href="#fn250" class="footnote-ref" id="fnref250"><sup>250</sup></a></p>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:meta-classic-funnel"></span>
<img src="experimentology_files/figure-html/meta-classic-funnel-1.png" alt="Classic funnel plot." width="\linewidth" />
Figure 16.4: Classic funnel plot.
</span>
</p>
<p>Not all funnel plots are symmetric! Figure <a href="16-meta.html#fig:meta-funnel-biased">16.5</a> is what happens to our hypothetical meta-analysis if all studies with <span class="math inline">\(p&lt;0.05\)</span> and positive estimates are published, but only 10% of studies with <span class="math inline">\(p \ge 0.05\)</span> or with negative estimates are published. The introduction of publication bias dramatically inflates the pooled estimate from 0.34 to 1.15. Also, there appears to be a correlation between studies’ estimates and their standard errors, such that smaller studies tend to have larger estimates than do larger studies. This correlation is often called <strong>funnel plot asymmetry</strong> because the funnel plot starts to look like a right triangle rather than a funnel. Funnel plot asymmetry <em>can</em> be a diagnostic for publication bias, though it isn’t always a perfect indicator, as we’ll see in the next subsection.</p>
<div class="figure"><span style="display: block;" id="fig:meta-funnel-biased"></span>
<p class="caption marginnote shownote">
Figure 16.5: Significance funnel plot for the same simulated meta-analysis after publication bias has occurred. Orange points: studies with <span class="math inline">\(p &lt; 0.05\)</span> and positive estimates. Grey points: studies with <span class="math inline">\(p\)</span> <span class="math inline">\(\ge\)</span> <span class="math inline">\(0.05\)</span> or negative estimates. Black diamond: random-effects estimate of <span class="math inline">\(\widehat{\mu}\)</span>.
</p>
<img src="experimentology_files/figure-html/meta-funnel-biased-1.png" alt="Significance funnel plot for the same simulated meta-analysis after publication bias has occurred. Orange points: studies with $p &lt; 0.05$ and positive estimates. Grey points: studies with $p$ $\ge$ $0.05$ or negative estimates. Black diamond: random-effects estimate of $\widehat{\mu}$." width="\linewidth" />
</div>
</div>
<div id="across-study-bias-correction" class="section level3" number="16.2.3">
<h3><span class="header-section-number">16.2.3</span> Across-study bias correction</h3>
<p>How do we identify and correct bias across studies? Given that some forms of publication bias induce a correlation between studies’ point estimates and their standard errors, several popular statistical methods, such as Trim-and-Fill <span class="citation">(<a href="#ref-duval2000trim" role="doc-biblioref">Duval &amp; Tweedie, 2000</a>)</span> and Egger’s regression <span class="citation">(<a href="#ref-egger1997bias" role="doc-biblioref">Egger et al., 1997</a>)</span> are designed to quantify funnel plot asymmetry.</p>
<p>However, it is important to note that funnel plot asymmetry does not always imply that there is publication bias, nor does publication bias always cause funnel plot asymmetry. Sometimes funnel plot asymmetry is driven by genuine differences in the effects being studied in small and large studies <span class="citation">(<a href="#ref-egger1997bias" role="doc-biblioref">Egger et al., 1997</a>; <a href="#ref-lau2006case" role="doc-biblioref">Lau et al., 2006</a>)</span>. For example, in a meta-analysis of intervention studies, if the most effective interventions are also the most expensive or difficult to implement, these highly effective interventions might be used primarily in the smallest studies (“small study effects”). Essentially, funnel plots and related methods are best suited to detecting publication bias in which (1) small studies with large positive point estimates are more likely to be published than small studies with small or negative point estimates; and (2) the largest studies are published regardless of the magnitude of their point estimates. That model of publication bias is sometimes what is happening, but not always!</p>
<p>A more flexible approach for detecting publication bias uses <strong>selection models</strong>. These models can detect other forms of publication bias that funnel plots may not detect, such as publication bias that favors <em>significant</em> results. We won’t cover these methods in detail here, but we think they are a better approach to the question, along with related sensitivity analyses.<a href="#fn251" class="footnote-ref" id="fnref251"><sup>251</sup></a></p>
<p>You may also have heard of “<span class="math inline">\(p\)</span>-methods” to detect across-study biases such as <span class="math inline">\(p\)</span>-curve and <span class="math inline">\(p\)</span>-uniform <span class="citation">(<a href="#ref-simonsohn2014p" role="doc-biblioref">Simonsohn et al., 2014</a>; <a href="#ref-van2015meta" role="doc-biblioref">Van Assen et al., 2015</a>)</span>. These methods essentially assess whether the significant <span class="math inline">\(p\)</span>-values “bunch up” just under 0.05, which is taken to indicate publication bias. These methods are increasingly popular in psychology and have their merits. However, it is important to note that these methods are actually simplified versions of selection models <span class="citation">(e.g., <a href="#ref-hedges1984estimation" role="doc-biblioref">Hedges, 1984</a>)</span> that work only under considerably more restrictive settings than do the original selection models [for example, when there is not heterogeneity across studies; <span class="citation">McShane et al. (<a href="#ref-mcshane2016adjusting" role="doc-biblioref">2016</a>)</span>]. For this reason, it is usually (although not always) better to use selection models in place of the more restrictive <span class="math inline">\(p\)</span>-methods.</p>
<p>Paluck et al. used a regression-based approach to assess and correct for publication bias. This approach provided significant evidence of a relationship between the standard error and effect size (i.e., an asymmetric funnel plot). Again, this asymmetry could reflect publication bias or other sources of correlation between studies’ estimates and their standard errors. Paluck et al. also used this same regression-based approach to try to correct for potential publication bias. Results from this model indicated that the bias-corrected effect size estimate was close to zero. In other words, even though all studies estimated that intergroup contact decreased prejudice, it is possible that there are unpublished studies that did not find this (or found that intergroup contact increased prejudice).</p>
<div class="box accident_report"><div class="Collapsible"><span id="collapsible-trigger-1655314617941" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1655314617941" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="person-falling-burst" class="svg-inline--fa fa-person-falling-burst " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><path fill="currentColor" d="M256 41.84C256 96.45 228.1 146.5 183.5 175.4L183.7 175.8L240.5 255.1H311.1C327.1 255.1 341.3 263.1 350.4 275.2L393.6 332.8C404.2 346.9 401.3 366.1 387.2 377.6C373.1 388.2 353 385.3 342.4 371.2L303.1 319.1H222.6L314.9 462.6C324.5 477.5 320.2 497.3 305.4 506.9C290.5 516.5 270.7 512.2 261.1 497.4L100.5 249.2C97.57 258.4 95.1 268.1 95.1 278.2V351.1C95.1 369.7 81.67 383.1 63.1 383.1C46.33 383.1 31.1 369.7 31.1 351.1V278.2C31.1 213 71.65 154.5 132.1 130.3C168.3 115.8 191.1 80.79 191.1 41.84V32C191.1 14.33 206.3 0 223.1 0C241.7 0 255.1 14.33 255.1 32L256 41.84zM96 79.1C96 106.5 74.51 127.1 48 127.1C21.49 127.1 0 106.5 0 79.1C0 53.49 21.49 31.1 48 31.1C74.51 31.1 96 53.49 96 79.1zM464 286.1L424.7 322.2C423.1 319.3 421.3 316.4 419.2 313.6L382.1 265.3L384.2 247.6L365.8 244.8C351.2 231.5 332.1 223.1 311.1 223.1H292.6C292.5 223.7 292.5 223.4 292.4 223.2C290.1 216.8 293.5 210.1 298.9 206.4L364.5 161.3L325 92.18C321.8 86.49 322.3 79.39 326.4 74.27C330.5 69.14 337.3 67.03 343.6 68.93L419.7 92.05L449.1 18.09C451.6 11.1 457.4 8 464 8C470.6 8 476.4 11.1 478.9 18.09L508.3 92.05L584.4 68.93C590.7 67.03 597.5 69.14 601.6 74.27C605.7 79.39 606.2 86.49 602.1 92.18L563.5 161.3L629.1 206.4C634.5 210.1 637 216.8 635.6 223.2C634.1 229.6 628.9 234.4 622.4 235.4L543.8 247.6L549.4 327C549.8 333.6 546.3 339.7 540.4 342.6C534.5 345.4 527.4 344.4 522.6 339.9L464 286.1z"></path></svg>Accident report<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M169.4 278.6C175.6 284.9 183.8 288 192 288s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25s-32.75-12.5-45.25 0L192 210.8L54.63 73.38c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25L169.4 278.6zM329.4 265.4L192 402.8L54.63 265.4c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25l160 160C175.6 476.9 183.8 480 192 480s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25S341.9 252.9 329.4 265.4z"></path></svg></span><div id="collapsible-content-1655314617941" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1655314617941"><div class="Collapsible__contentInner"><p class="title">Garbage in, garbage out? Meta-analyzing bad research</p>
<p></p>
<p>Botox can help eliminate wrinkles. But some researchers have also suggested that it may help treat clinical depression when used to paralyze the muscles associated with frowning. As crazy as they may sound, a quick examination of the literature would lead many to conclude that this treatment works. Studies that randomly assign depressed patients to either receive Botox injections or saline injections do indeed find that Botox recipients exhibit decreases in depression. And when you combine all available evidence in a meta-analysis, you find that this difference is quite large: <span class="math inline">\(\widehat{d}\)</span> = 0.83, 95% CI [0.52, 1.14]. Unfortunately, this meta-analytic result has problems with both within- and between-study bias <span class="citation">(<a href="#ref-coles2019does" role="doc-biblioref">Coles et al., 2019</a>)</span>.</p>
<p>First, participants are not supposed to know whether they have been randomly assigned to receive Botox or a control saline injections. However, only one of these treatments leads the upper half of your face to be paralyzed! After a couple of weeks you’re pretty likely to figure out whether you received the Botox treatment or control saline injection. Thus, the apparent effect of Botox on depression could very well be a placebo effect, a clear within-study bias issue.</p>
<p>Second, 51% of the outcomes measured were found not to have been reported by the study authors. This finding raises concerns about selective reporting: perhaps researchers examining the effects of Botox on depression only report the outcomes that demonstrate an effect, but not those that do not. In this scenario, any meta-analytic conclusions are potentially undermined by between-study bias.</p>
</div></div></div></div>
</div>
</div>
<div id="chapter-summary-meta-analysis" class="section level2" number="16.3">
<h2><span class="header-section-number">16.3</span> Chapter summary: Meta-analysis</h2>
<p>Taken together, Paluck and colleagues’ use of meta-analysis provided several important insights that would have been easy to miss in a non-quantitative review. First, despite a preponderance of non-significant findings, intergroup contact interventions were estimated to decrease prejudice by on average 0.4 standard deviations. On the other hand, there was considerable heterogeneity in intergroup contact effects, suggesting that there are important moderators of the effectiveness of these interventions. And finally, publication bias was a substantial concern, suggesting the need for follow-up research that will be published regardless of the outcome.</p>
<p>Overall, meta-analysis is a key technique for aggregating evidence across studies. Meta-analysis allows researchers to move beyond the bias of naive techniques like vote counting and towards a more quantitative summary of an experimental effect. Unfortunately, a meta-analysis is only as good as the literature it’s based on, so the aspiring meta-analyst must be aware of both within- and between-study biases!</p>
<div class="box exercises"><div class="Collapsible"><span id="collapsible-trigger-1655314617941" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1655314617941" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="pen-ruler" class="svg-inline--fa fa-pen-ruler " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M492.7 42.75C517.7 67.74 517.7 108.3 492.7 133.3L436.3 189.7L322.3 75.72L378.7 19.32C403.7-5.678 444.3-5.678 469.3 19.32L492.7 42.75zM44.89 353.2L299.7 98.34L413.7 212.3L158.8 467.1C152.1 473.8 143.8 478.7 134.6 481.4L30.59 511.1C22.21 513.5 13.19 511.1 7.03 504.1C.8669 498.8-1.47 489.8 .9242 481.4L30.65 377.4C33.26 368.2 38.16 359.9 44.89 353.2zM249.4 103.4L103.4 249.4L16 161.9C-2.745 143.2-2.745 112.8 16 94.06L94.06 16C112.8-2.745 143.2-2.745 161.9 16L181.7 35.76C181.4 36.05 181 36.36 180.7 36.69L116.7 100.7C110.4 106.9 110.4 117.1 116.7 123.3C122.9 129.6 133.1 129.6 139.3 123.3L203.3 59.31C203.6 58.99 203.1 58.65 204.2 58.3L249.4 103.4zM453.7 307.8C453.4 308 453 308.4 452.7 308.7L388.7 372.7C382.4 378.9 382.4 389.1 388.7 395.3C394.9 401.6 405.1 401.6 411.3 395.3L475.3 331.3C475.6 330.1 475.1 330.6 476.2 330.3L496 350.1C514.7 368.8 514.7 399.2 496 417.9L417.9 496C399.2 514.7 368.8 514.7 350.1 496L262.6 408.6L408.6 262.6L453.7 307.8z"></path></svg>Exercises<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M169.4 278.6C175.6 284.9 183.8 288 192 288s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25s-32.75-12.5-45.25 0L192 210.8L54.63 73.38c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25L169.4 278.6zM329.4 265.4L192 402.8L54.63 265.4c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25l160 160C175.6 476.9 183.8 480 192 480s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25S341.9 252.9 329.4 265.4z"></path></svg></span><div id="collapsible-content-1655314617941" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1655314617941"><div class="Collapsible__contentInner">
<ol style="list-style-type: decimal;">
<li><p>Imagine that you read the following result in the abstract of a meta-analysis: “In 83 randomized studies of middle school children, replacing one hour of class time with mindfulness meditation significantly improved standardized test scores (standardized mean difference <span class="math inline">\(\widehat{\mu} = 0.05\)</span>; 95% confidence interval: [<span class="math inline">\(0.01, 0.09\)</span>]; <span class="math inline">\(p&lt;0.05\)</span>).” Why is this a problematic way to report on meta-analysis results? Suggest a better sentence to replace this one.</p>
<p>As you read the rest of the meta-analysis, you find that the authors conclude that “These findings demonstrate robust benefits of meditation for children, suggesting that test scores improve even when the meditation is introduced as a replacement for normal class time.” You recall that the heterogeneity estimate was <span class="math inline">\(\widehat{\tau} = 0.90\)</span>. Do you think that this result regarding the heterogeneity tends to support, or rather tends to undermine, the concluding sentence of the meta-analysis? Why?</p>
<p>What kinds of within-study biases would concern you in this meta-analysis? How might you assess the credibility of the meta-analyzed studies and of the meta-analysis as whole in light of these possible biases?</p></li>
<li><p>Imagine you conduct a meta-analysis on a literature in which statistically significant results in either direction are much more likely to be published that non-significant results. Draw the funnel plot you would expect to see. Is the plot funnel symmetric or asymmetric?</p></li>
<li><p>Why do you think small studies receive more weight in random-effects meta-analysis than in fixed-effects meta-analysis? Can you see why this is true mathematically based on the equations given above, and can you also explain the intuition in simple language?</p></li>
</ol>
</div></div></div></div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-allport1954nature" class="csl-entry">
Allport, G. W. (1954). <em>The nature of prejudice</em>.
</div>
<div id="ref-boisjoly2006empathy" class="csl-entry">
Boisjoly, J., Duncan, G. J., Kremer, M., Levy, D. M., &amp; Eccles, J. (2006). Empathy or antipathy? The impact of diversity. <em>American Economic Review</em>, <em>96</em>(5), 1890–1905.
</div>
<div id="ref-borenstein2021introduction" class="csl-entry">
Borenstein, M., Hedges, L. V., Higgins, J. P., &amp; Rothstein, H. R. (2021). <em>Introduction to meta-analysis</em>. John Wiley &amp;amp; Sons.
</div>
<div id="ref-brockwell2001comparison" class="csl-entry">
Brockwell, S. E., &amp; Gordon, I. R. (2001). A comparison of statistical methods for meta-analysis. <em>Statistics in Medicine</em>, <em>20</em>(6), 825–840.
</div>
<div id="ref-clunies1989changing" class="csl-entry">
Clunies-Ross, G., &amp; O’meara, K. (1989). Changing the attitudes of students towards peers with disabilities. <em>Australian Psychologist</em>, <em>24</em>(2), 273–284.
</div>
<div id="ref-coles2019does" class="csl-entry">
Coles, N. A., Larsen, J. T., Kuribayashi, J., &amp; Kuelz, A. (2019). Does blocking facial feedback via botulinum toxin injections decrease depression? A critical review and meta-analysis. <em>Emotion Review</em>, <em>11</em>(4), 294–309.
</div>
<div id="ref-dersimonian1986meta" class="csl-entry">
DerSimonian, R., &amp; Laird, N. (1986). Meta-analysis in clinical trials. <em>Controlled Clinical Trials</em>, <em>7</em>(3), 177–188.
</div>
<div id="ref-duval2000trim" class="csl-entry">
Duval, S., &amp; Tweedie, R. (2000). Trim and fill: A simple funnel-plot–based method of testing and adjusting for publication bias in meta-analysis. <em>Biometrics</em>, <em>56</em>(2), 455–463. <a href="https://doi.org/10.1111/j.0006-341X.2000.00455.x">https://doi.org/10.1111/j.0006-341X.2000.00455.x</a>
</div>
<div id="ref-egger1997bias" class="csl-entry">
Egger, M., Smith, G. D., Schneider, M., &amp; Minder, C. (1997). Bias in meta-analysis detected by a simple, graphical test. <em>BMJ</em>, <em>315</em>(7109), 629–634. <a href="https://doi.org/10.1136/bmj.315.7109.629">https://doi.org/10.1136/bmj.315.7109.629</a>
</div>
<div id="ref-franco2014" class="csl-entry">
Franco, A., Malhotra, N., &amp; Simonovits, G. (2014). Publication bias in the social sciences: Unlocking the file drawer. In <em>Science</em> (No. 6203; Vol. 345, pp. 1502–1505).
</div>
<div id="ref-goldstein2008room" class="csl-entry">
Goldstein, N. J., Cialdini, R. B., &amp; Griskevicius, V. (2008). A room with a viewpoint: Using social norms to motivate environmental conservation in hotels. <em>Journal of Consumer Research</em>, <em>35</em>(3), 472–482.
</div>
<div id="ref-grant2009typology" class="csl-entry">
Grant, M. J., &amp; Booth, A. (2009). A typology of reviews: An analysis of 14 review types and associated methodologies. <em>Health Information &amp; Libraries Journal</em>, <em>26</em>(2), 91–108.
</div>
<div id="ref-hedges1984estimation" class="csl-entry">
Hedges, L. V. (1984). Estimation of effect size under nonrandom sampling: The effects of censoring studies yielding statistically insignificant mean differences. <em>Journal of Educational Statistics</em>, <em>9</em>(1), 61–85. <a href="https://doi.org/10.3102/10769986009001061">https://doi.org/10.3102/10769986009001061</a>
</div>
<div id="ref-hedges2010robust" class="csl-entry">
Hedges, L. V., Tipton, E., &amp; Johnson, M. C. (2010). Robust variance estimation in meta-regression with dependent effect size estimates. <em>Research Synthesis Methods</em>, <em>1</em>(1), 39–65.
</div>
<div id="ref-iyengar1988" class="csl-entry">
Iyengar, S., &amp; Greenhouse, J. B. (1988). Selection models and the file drawer problem. <em>Statistical Science</em>, 109–117.
</div>
<div id="ref-knapp2003improved" class="csl-entry">
Knapp, G., &amp; Hartung, J. (2003). Improved tests for a random effects meta-regression with a single covariate. <em>Statistics in Medicine</em>, <em>22</em>(17), 2693–2710.
</div>
<div id="ref-kotzeva2015eurostat" class="csl-entry">
Kotzeva, M., Brandmüller, T., &amp; Önnerfors, Å. (2015). <em>Eurostat regional yearbook 2015</em>. Publications Office of the European Union.
</div>
<div id="ref-lau2006case" class="csl-entry">
Lau, J., Ioannidis, J. P., Terrin, N., Schmid, C. H., &amp; Olkin, I. (2006). The case of the misleading funnel plot. <em>BMJ</em>, <em>333</em>(7568), 597–600. <a href="https://doi.org/10.1136/bmj.333.7568.597">https://doi.org/10.1136/bmj.333.7568.597</a>
</div>
<div id="ref-lefebvre2019searching" class="csl-entry">
Lefebvre, C., Glanville, J., Briscoe, S., Littlewood, A., Marshall, C., Metzendorf, M.-I., Noel-Storr, A., Rader, T., Shokraneh, F., Thomas, J.others. (2019). Searching for and selecting studies. <em>Cochrane Handbook for Systematic Reviews of Interventions</em>, 67–107.
</div>
<div id="ref-smt" class="csl-entry">
Maier, M., VanderWeele, T. J., &amp; Mathur, M. B. (in press). Using selection models to assess sensitivity to publication bias: A tutorial and call for more routine use. <em>Campbell Systematic Reviews</em>.
</div>
<div id="ref-mathur_mam" class="csl-entry">
Mathur, M. B., &amp; VanderWeele, T. J. (2019). New metrics for meta-analyses of heterogeneous effects. <em>Statistics in Medicine</em>, <em>38</em>(8), 1336–1342.
</div>
<div id="ref-npphat" class="csl-entry">
Mathur, M. B., &amp; VanderWeele, T. J. (2020b). Robust metrics and sensitivity analyses for meta-analyses of heterogeneous effects. <em>Epidemiology</em>, <em>31</em>(3), 356–358.
</div>
<div id="ref-sapb" class="csl-entry">
Mathur, M. B., &amp; VanderWeele, T. J. (2020c). Sensitivity analysis for publication bias in meta-analyses. <em>Journal of the Royal Statistical Society: Series C</em>, <em>5</em>(69), 1091–1119.
</div>
<div id="ref-sapbe" class="csl-entry">
Mathur, M. B., &amp; VanderWeele, T. J. (2021). Estimating publication bias in meta-analyses of peer-reviewed studies: A meta-meta-analysis across disciplines and journal tiers. <em>Research Synthesis Methods</em>, <em>12</em>(2), 176–191.
</div>
<div id="ref-art" class="csl-entry">
Mathur, M. B., &amp; VanderWeele, T. J. (2022). Methods to address confounding and other biases in meta-analyses: Review and recommendations. <em>Annual Review of Public Health</em>, <em>1</em>(43).
</div>
<div id="ref-mcshane2016adjusting" class="csl-entry">
McShane, B. B., Böckenholt, U., &amp; Hansen, K. T. (2016). Adjusting for publication bias in meta-analysis: An evaluation of selection methods and some cautionary notes. <em>Perspectives on Psychological Science</em>, <em>11</em>(5), 730–749. <a href="https://doi.org/10.1177/1745691616662243">https://doi.org/10.1177/1745691616662243</a>
</div>
<div id="ref-mcshane2017statistical" class="csl-entry">
McShane, B. B., &amp; Gal, D. (2017). Statistical significance and the dichotomization of evidence. <em>Journal of the American Statistical Association</em>, <em>112</em>(519), 885–895. <a href="https://doi.org/10.1080/01621459.2017.1289846">https://doi.org/10.1080/01621459.2017.1289846</a>
</div>
<div id="ref-nelson1986interpretation" class="csl-entry">
Nelson, N., Rosenthal, R., &amp; Rosnow, R. L. (1986). Interpretation of significance levels and effect sizes by psychological researchers. <em>American Psychologist</em>, <em>41</em>(11), 1299.
</div>
<div id="ref-paluck2019contact" class="csl-entry">
Paluck, E. L., Green, S. A., &amp; Green, D. P. (2019). The contact hypothesis re-evaluated. <em>Behavioural Public Policy</em>, <em>3</em>(2), 129–158.
</div>
<div id="ref-pustejovsky2021meta" class="csl-entry">
Pustejovsky, J. E., &amp; Tipton, E. (2021). Meta-analysis with robust variance estimation: Expanding the range of working models. <em>Prevention Science</em>, 1–14.
</div>
<div id="ref-riley2011interpretation" class="csl-entry">
Riley, R. D., Higgins, J. P., &amp; Deeks, J. J. (2011). Interpretation of random effects meta-analyses. <em>BMJ</em>, <em>342</em>.
</div>
<div id="ref-scheibehenne2016" class="csl-entry">
Scheibehenne, B., Jamil, T., &amp; Wagenmakers, E.-J. (2016). Bayesian evidence synthesis can reconcile seemingly inconsistent results: The case of hotel towel reuse. <em>Psychol. Sci.</em>, <em>27</em>(7), 1043–1046.
</div>
<div id="ref-simonsohn2014p" class="csl-entry">
Simonsohn, U., Nelson, L. D., &amp; Simmons, J. P. (2014). P-curve: A key to the file-drawer. <em>Journal of Experimental Psychology: General</em>, <em>143</em>(2), 534.
</div>
<div id="ref-sterne2016robins" class="csl-entry">
Sterne, J. A., Hernán, M. A., Reeves, B. C., Savović, J., Berkman, N. D., Viswanathan, M., Henry, D., Altman, D. G., Ansari, M. T., Boutron, I.others. (2016). ROBINS-i: A tool for assessing risk of bias in non-randomised studies of interventions. <em>Bmj</em>, <em>355</em>.
</div>
<div id="ref-thompson2002should" class="csl-entry">
Thompson, S. G., &amp; Higgins, J. P. (2002). How should meta-regression analyses be undertaken and interpreted? <em>Statistics in Medicine</em>, <em>21</em>(11), 1559–1573.
</div>
<div id="ref-tipton2015small" class="csl-entry">
Tipton, E. (2015). Small sample adjustments for robust variance estimation with meta-regression. <em>Psychological Methods</em>, <em>20</em>(3), 375.
</div>
<div id="ref-tsuji2020addressing" class="csl-entry">
Tsuji, S., Cristia, A., Frank, M. C., &amp; Bergmann, C. (2020). Addressing publication bias in meta-analysis. <em>Zeitschrift f<span>ü</span>r Psychologie</em>.
</div>
<div id="ref-van2015meta" class="csl-entry">
Van Assen, M. A., Aert, R. van, &amp; Wicherts, J. M. (2015). Meta-analysis using effect size distributions of only statistically significant studies. <em>Psychological Methods</em>, <em>20</em>(3), 293.
</div>
<div id="ref-vevea1995" class="csl-entry">
Vevea, J. L., &amp; Hedges, L. V. (1995). A general linear model for estimating effect size in the presence of publication bias. <em>Psychometrika</em>, <em>60</em>(3), 419–435.
</div>
<div id="ref-wang2019simple" class="csl-entry">
Wang, C.-C., &amp; Lee, W.-C. (2019). A simple method to estimate prediction intervals and predictive distributions: Summarizing meta-analyses beyond means and confidence intervals. <em>Research Synthesis Methods</em>, <em>10</em>(2), 255–266.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="240">
<li id="fn240"><p>We’ll primarily be using Cohen’s <span class="math inline">\(d\)</span>, the standardized difference between means, which we introduced in Chapter <a href="5-estimation.html#estimation">5</a>. There are many more varieties of effect size available, but we focus here on <span class="math inline">\(d\)</span> because it’s common and easy to reason about in the context of the statistical tools we introduced in the earlier sections of the book.<a href="16-meta.html#fnref240" class="footnote-back">↩︎</a></p></li>
<li id="fn241"><p>Given the number of hotel bookings worldwide – 1.7 billion in the European Union alone in 2013 <span class="citation">(<a href="#ref-kotzeva2015eurostat" role="doc-biblioref">Kotzeva et al., 2015</a>)</span> – the insight provided by the meta-analysis is not trivial!<a href="16-meta.html#fnref241" class="footnote-back">↩︎</a></p></li>
<li id="fn242"><p>You can ignore for now the column of percentages and the final line, “RE Model”; we will return to these later.<a href="16-meta.html#fnref242" class="footnote-back">↩︎</a></p></li>
<li id="fn243"><p>If you are curious, the standard error of the fixed-effect <span class="math inline">\(\widehat{\mu}\)</span> is <span class="math inline">\(\frac{1}{\sum_{i=1}^k w_i}\)</span>. This standard error can be used to construct a confidence interval or <span class="math inline">\(p\)</span>-value, as described in Chapter <a href="6-inference.html#inference">6</a>.<a href="16-meta.html#fnref243" class="footnote-back">↩︎</a></p></li>
<li id="fn244"><p>Technically, other specifications of random-effects meta-analysis are possible. For example, robust variance estimation does not require making assumptions about the distribution of effects across studies <span class="citation">(<a href="#ref-hedges2010robust" role="doc-biblioref">Hedges et al., 2010</a>)</span>. These approaches also have other substantial advantages, like their ability to handle effects that are clustered [e.g., because some papers contribute multiple estimates; <span class="citation">Hedges et al. (<a href="#ref-hedges2010robust" role="doc-biblioref">2010</a>)</span>; <span class="citation">Pustejovsky &amp; Tipton (<a href="#ref-pustejovsky2021meta" role="doc-biblioref">2021</a>)</span>] and their ability to provide better inference in meta-analyses with relatively few studies <span class="citation">(<a href="#ref-tipton2015small" role="doc-biblioref">Tipton, 2015</a>)</span>. For these reasons, the authors of this book prefer to use these robust methods by default when conducting meta-analyses.<a href="16-meta.html#fnref244" class="footnote-back">↩︎</a></p></li>
<li id="fn245"><p>A confidence interval and <span class="math inline">\(p\)</span>-value for the random-effects estimate <span class="math inline">\(\widehat{\mu}\)</span> can be obtained using standard theory for maximum likelihood estimates with an additional adjustment that helps account for uncertainty in estimating <span class="math inline">\(\tau\)</span> <span class="citation">(<a href="#ref-knapp2003improved" role="doc-biblioref">Knapp &amp; Hartung, 2003</a>)</span>.<a href="16-meta.html#fnref245" class="footnote-back">↩︎</a></p></li>
<li id="fn246"><p>The estimate of <span class="math inline">\(\widehat{\tau}^2\)</span> is a bit more complicated, but is essentially a weighted average of studies’ residuals, <span class="math inline">\(\widehat{\theta_i} - \widehat{\mu}\)</span>, while subtracting away variation due to statistical error, <span class="math inline">\(\widehat{\sigma}^2_i\)</span> <span class="citation">(<a href="#ref-brockwell2001comparison" role="doc-biblioref">Brockwell &amp; Gordon, 2001</a>; <a href="#ref-dersimonian1986meta" role="doc-biblioref">DerSimonian &amp; Laird, 1986</a>)</span>.<a href="16-meta.html#fnref246" class="footnote-back">↩︎</a></p></li>
<li id="fn247"><p>One common approach to investigating moderators in meta-analysis is meta-regression, in which moderators (e.g., type of intergroup contact) are included as covariates in a random-effects meta-analysis model <span class="citation">(<a href="#ref-thompson2002should" role="doc-biblioref">Thompson &amp; Higgins, 2002</a>)</span>. As in standard regression, coefficients can then be estimated for each moderator, representing the mean difference in population effect between studies with versus without the moderator.<a href="16-meta.html#fnref247" class="footnote-back">↩︎</a></p></li>
<li id="fn248"><p><span class="citation">Paluck et al. (<a href="#ref-paluck2019contact" role="doc-biblioref">2019</a>)</span> did not use this tool, but they could have used it to communicate, for example, the extent to which participants might have differentially dropped out of the study.<a href="16-meta.html#fnref248" class="footnote-back">↩︎</a></p></li>
<li id="fn249"><p>Evidence is mixed regarding whether including gray literature actually reduces across-study biases in meta-analysis <span class="citation">(<a href="#ref-sapbe" role="doc-biblioref">Mathur &amp; VanderWeele, 2021</a>; <a href="#ref-tsuji2020addressing" role="doc-biblioref">Tsuji et al., 2020</a>)</span>, but it is still common practice to try to include this literature.<a href="16-meta.html#fnref249" class="footnote-back">↩︎</a></p></li>
<li id="fn250"><p>Classic funnel plots look more like Figure <a href="16-meta.html#fig:meta-classic-funnel">16.4</a>. Our version is different in a couple of ways. Most prominently, we don’t have the vertical axis reversed (which we think is confusing), and we don’t have the left boundary highlighted (because we think folks don’t typically select for negative studies).<a href="16-meta.html#fnref250" class="footnote-back">↩︎</a></p></li>
<li id="fn251"><p>High-level overviews of selection models are given in <span class="citation">McShane et al. (<a href="#ref-mcshane2016adjusting" role="doc-biblioref">2016</a>)</span> and <span class="citation">Maier et al. (<a href="#ref-smt" role="doc-biblioref">in press</a>)</span>. For more methodological detail, see <span class="citation">Hedges (<a href="#ref-hedges1984estimation" role="doc-biblioref">1984</a>)</span>, <span class="citation">Iyengar &amp; Greenhouse (<a href="#ref-iyengar1988" role="doc-biblioref">1988</a>)</span>, and <span class="citation">Vevea &amp; Hedges (<a href="#ref-vevea1995" role="doc-biblioref">1995</a>)</span>. For a tutorial on fitting and interpreting selection models, see <span class="citation">Maier et al. (<a href="#ref-smt" role="doc-biblioref">in press</a>)</span>. For sensitivity analyses, see <span class="citation">Mathur &amp; VanderWeele (<a href="#ref-sapb" role="doc-biblioref">2020c</a>)</span>.<a href="16-meta.html#fnref251" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

</div>
</div>




<script type="module" src="/assets/src/index.page.client.jsx.49dc7b35.js"></script><script id="vite-plugin-ssr_pageContext" type="application/json">{"pageContext":{"_pageId":"/src/index","url":"/16-meta","body":"\n\n\n\n\u003cdiv class=\"row\">\n\u003cdiv class=\"col-sm-12\">\n\u003cdiv id=\"TOC\">\n\u003cul>\n\u003cli class=\"part\">\u003cspan>\u003cb>I Preliminaries\u003c/b>\u003c/span>\u003c/li>\n\u003cli>\u003ca href=\"1-experiments.html#experiments\">\u003cspan class=\"toc-section-number\">1\u003c/span> Experiments\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"2-theories.html#theories\">\u003cspan class=\"toc-section-number\">2\u003c/span> Theories\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"3-replication.html#replication\">\u003cspan class=\"toc-section-number\">3\u003c/span> Replication and reproducibility\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"4-ethics.html#ethics\">\u003cspan class=\"toc-section-number\">4\u003c/span> Ethics\u003c/a>\u003c/li>\n\u003cli class=\"part\">\u003cspan>\u003cb>II Statistics\u003c/b>\u003c/span>\u003c/li>\n\u003cli>\u003ca href=\"5-estimation.html#estimation\">\u003cspan class=\"toc-section-number\">5\u003c/span> Estimation\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"6-inference.html#inference\">\u003cspan class=\"toc-section-number\">6\u003c/span> Inference\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"7-models.html#models\">\u003cspan class=\"toc-section-number\">7\u003c/span> Models\u003c/a>\u003c/li>\n\u003cli class=\"part\">\u003cspan>\u003cb>III Design and Planning\u003c/b>\u003c/span>\u003c/li>\n\u003cli>\u003ca href=\"8-measurement.html#measurement\">\u003cspan class=\"toc-section-number\">8\u003c/span> Measurement\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"9-design.html#design\">\u003cspan class=\"toc-section-number\">9\u003c/span> Design of experiments\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"10-sampling.html#sampling\">\u003cspan class=\"toc-section-number\">10\u003c/span> Sampling\u003c/a>\u003c/li>\n\u003cli class=\"part\">\u003cspan>\u003cb>IV Execution\u003c/b>\u003c/span>\u003c/li>\n\u003cli>\u003ca href=\"11-prereg.html#prereg\">\u003cspan class=\"toc-section-number\">11\u003c/span> Preregistration\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"12-collection.html#collection\">\u003cspan class=\"toc-section-number\">12\u003c/span> Data collection\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"13-management.html#management\">\u003cspan class=\"toc-section-number\">13\u003c/span> Project management\u003c/a>\u003c/li>\n\u003cli class=\"part\">\u003cspan>\u003cb>V Analysis and Reporting\u003c/b>\u003c/span>\u003c/li>\n\u003cli>\u003ca href=\"14-viz.html#viz\">\u003cspan class=\"toc-section-number\">14\u003c/span> Visualization\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"15-writing.html#writing\">\u003cspan class=\"toc-section-number\">15\u003c/span> Writing\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"16-meta.html#meta\">\u003cspan class=\"toc-section-number\">16\u003c/span> Meta-analysis\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"17-conclusions.html#conclusions\">\u003cspan class=\"toc-section-number\">17\u003c/span> Conclusions\u003c/a>\u003c/li>\n\u003cli class=\"appendix\">\u003cspan>\u003cb>Appendices\u003c/b>\u003c/span>\u003c/li>\n\u003cli>\u003ca href=\"A-git.html#git\">\u003cspan class=\"toc-section-number\">A\u003c/span> GitHub Tutorial\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"B-rmarkdown.html#rmarkdown\">\u003cspan class=\"toc-section-number\">B\u003c/span> R Markdown Tutorial\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"C-tidyverse.html#tidyverse\">\u003cspan class=\"toc-section-number\">C\u003c/span> Tidyverse Tutorial\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"D-ggplot.html#ggplot\">\u003cspan class=\"toc-section-number\">D\u003c/span> ggplot Tutorial\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"E-instructors.html#instructors\">\u003cspan class=\"toc-section-number\">E\u003c/span> Instructor’s guide\u003c/a>\u003c/li>\n\u003c/ul>\n\u003c/div>\n\u003c/div>\n\u003c/div>\n\u003cdiv class=\"row\">\n\u003cdiv class=\"col-sm-12\">\n\u003cdiv id=\"meta\" class=\"section level1\" number=\"16\">\n\u003ch1>\u003cspan class=\"header-section-number\">Chapter 16\u003c/span> Meta-analysis\u003c/h1>\n\u003cdiv class=\"box learning_goals\">\n\u003cul>\n\u003cli>Discuss the benefits of synthesizing evidence across studies\u003c/li>\n\u003cli>Conduct a simple fixed- or random-effects meta analysis\u003c/li>\n\u003cli>Reason about the role of within- and across-study biases in meta-analysis\u003c/li>\n\u003c/ul>\n\u003c/div>\n\u003cp>Throughout this book, we have focused on how to design individual experiments that maximize the precision of our effect size estimates and minimize their bias. But even when we do our best to get a precise, unbiased estimate in an individual experiment, we must also acknowledge that one study can never be definitive. For example, variability in participant demographics, stimuli, and experimental methods may limit the generalizability of our findings. Additionally, even well-powered individual studies have some amount of statistical error, limiting their precision.\u003c/p>\n\u003cp>For this reason, synthesizing evidence across studies is critical for developing a balanced and appropriately evolving view of the overall evidence on an effect of interest and for understanding sources of variation in the effect. Synthesizing evidence rigorously is not simply a matter of throwing a search term into Google Scholar, downloading articles that look topical or interesting, and qualitatively summarizing your impressions of those studies. While this ad hoc method can be an essential first step in performing a review of the literature \u003cspan class=\"citation\">(\u003ca href=\"#ref-grant2009typology\" role=\"doc-biblioref\">Grant &amp; Booth, 2009\u003c/a>)\u003c/span>, it is not systematic, and it doesn’t provide any kind of quantitative summary of a particular effect. Further, it doesn’t tell you anything about potential biases in the literature – for example, a bias for the publication of positive effects.\u003c/p>\n\u003cp>To address these issues, a more systematic, quantitative review of the literature is often more informative. This chapter focuses on a specific type of quantitative review called \u003cstrong>meta-analysis\u003c/strong>. Meta-analysis is a method for combining effect sizes across different measurements. (If you need a refresher on effect size, see Chapter \u003ca href=\"5-estimation.html#estimation\">5\u003c/a>, where we introduce the concept).\u003ca href=\"#fn240\" class=\"footnote-ref\" id=\"fnref240\">\u003csup>240\u003c/sup>\u003c/a>\u003c/p>\n\u003cp>By combining information from multiple studies, meta-analysis often provides more precise estimates of an effect size than any single study. In addition, meta-analysis also allows the researcher to look at the extent to which an effect varies across studies. If an effect does vary across studies, meta-analysis also can be used to test whether certain study characteristics systematically produce different results (e.g., whether an effect is larger in certain populations).\u003c/p>\n\u003cdiv class=\"box case_study\">\n\u003cp>(TITLE) Towel reuse by hotel guests\u003c/p>\n\u003cp>Imagine you are staying in a hotel and you have just taken a shower. Do you throw the towels on the floor or hang them back up again? In a widely-cited study on the power of social norms, \u003cspan class=\"citation\">Goldstein et al. (\u003ca href=\"#ref-goldstein2008room\" role=\"doc-biblioref\">2008\u003c/a>)\u003c/span> manipulated whether a sign encouraging guests to reuse towels focused environmental impacts (e.g., “help reduce water use”) or social norms (e.g., “most guests re-use their towels”). Across two studies, they found that guests were significantly more likely to reuse their towels after receiving the social norm message (Study 1: odds ratio [OR] = 1.46, 95% CI [1.00, 2.16], \u003cspan class=\"math inline\">\\(p = .05\\)\u003c/span>; Study 2: OR = 1.35, 95% CI [1.04, 1.77], \u003cspan class=\"math inline\">\\(p = .03\\)\u003c/span>).\u003c/p>\n\u003cp>However, five subsequent studies by other researchers did not find significant evidence of that social norm messaging increased towel reuse (with ORs ranging from 0.22 to 1.34, and no hypothesis-consistent \u003cspan class=\"math inline\">\\(p\\)\u003c/span>-values was less than .05). This caused many researchers to wonder if there is any effect at all. To examine this question, \u003cspan class=\"citation\">Scheibehenne et al. (\u003ca href=\"#ref-scheibehenne2016\" role=\"doc-biblioref\">2016\u003c/a>)\u003c/span> statistically combined all the evidence via meta-analysis. When they did, they found that the social norm messages did have a significant average effect on hotel towel reuse (OR = 1.26, 95% CI [1.07, 1.46], \u003cspan class=\"math inline\">\\(p &lt; .005\\)\u003c/span>). This study demonstrates an important strength of meta-analysis: by pooling evidence from multiple studies, meta-analysis can generate more powerful insights than any one study alone. We will also see how meta-analysis can be used to assess variability in effects across studies.\u003c/p>\n\u003c/div>\n\u003c!-- All ES's come from Wagenmakers et al. SI -->\n\u003cp>Meta-analysis often teaches us something about a body of evidence that we do not intuitively grasp when we casually read through a bunch of articles. In this example, merely reading the individual studies might give the impression that social norm messages do not increase hotel towel re-use. But meta-analysis indicated that the average effect is beneficial, although there might be substantial variation in effect sizes across studies.\u003ca href=\"#fn241\" class=\"footnote-ref\" id=\"fnref241\">\u003csup>241\u003c/sup>\u003c/a>\u003c/p>\n\u003cdiv id=\"the-basics-of-evidence-synthesis\" class=\"section level2\" number=\"16.1\">\n\u003ch2>\u003cspan class=\"header-section-number\">16.1\u003c/span> The basics of evidence synthesis\u003c/h2>\n\u003cp>As we explore the details of conducting a meta-analysis, we’ll turn to another example: a meta-analysis of studies investigating the “contact hypothesis” on intergroup relations.\u003c/p>\n\u003cp>According to the contact hypothesis, prejudice towards members of minority groups can be reduced through intergroup contact interventions, in which members of majority and minority groups work together to pursue a common goal \u003cspan class=\"citation\">(\u003ca href=\"#ref-allport1954nature\" role=\"doc-biblioref\">Allport, 1954\u003c/a>)\u003c/span>.To test this, \u003cspan class=\"citation\">Paluck et al. (\u003ca href=\"#ref-paluck2019contact\" role=\"doc-biblioref\">2019\u003c/a>)\u003c/span> meta-analyzed randomized studies that tested the effects of intergroup contact interventions on long-term prejudice-related outcomes. Using a systematic literature search, they tried to identify all published papers that tested these effects and then extracted effect size estimates from each paper. Because not every paper reports standardized effect sizes – or even means and standard deviations for every group – this process can often involve scraping information from plots, tables, and statistical tests to try to reconstruct effect sizes. This book will not cover the process of conducting a systematic literature search and extracting effect sizes, but these topics are critical to understand if you plan to conduct a meta-analysis or other evidence synthesis.\u003c/p>\n\u003cp>As we’ve seen throughout this book, visualizing data before and after analysis helps benchmark and check our intuitions about the formal statistical results. In a meta-analysis, a common way to do so is the \u003cstrong>forest plot\u003c/strong>, which depicts individual studies’ estimates and confidence intervals.\u003ca href=\"#fn242\" class=\"footnote-ref\" id=\"fnref242\">\u003csup>242\u003c/sup>\u003c/a> In the forest plot in Figure \u003ca href=\"16-meta.html#fig:meta-forest\">16.1\u003c/a>, the larger squares correspond to more precise studies; notice how much narrower their confidence intervals are than the confidence intervals of less precise studies.\u003c/p>\n\u003cdiv class=\"figure\">\u003cspan style=\"display:block;\" id=\"fig:meta-forest\">\u003c/span>\n\u003cp class=\"caption marginnote shownote\">\nFigure 16.1: Forest plot for Paluck et al. meta-analysis. Studies are ordered from smallest to largest standard error.\n\u003c/p>\n\u003cimg src=\"experimentology_files/figure-html/meta-forest-1.png\" alt=\"Forest plot for Paluck et al. meta-analysis. Studies are ordered from smallest to largest standard error.\" width=\"\\linewidth\"  />\n\u003c/div>\n\u003c!-- Cohen's $d$ -- which, if you recall from Chapter \\@ref(estimation), represents the standardized mean difference -- was used as the effect size index. As we show in the remainder of the chapter, the meta-analytic tools @paluck2019contact used provide several useful insights about this proposed prejudice-reduction intervention. -->\n\u003cdiv id=\"how-not-to-synthesize-evidence\" class=\"section level3\" number=\"16.1.1\">\n\u003ch3>\u003cspan class=\"header-section-number\">16.1.1\u003c/span> How not to synthesize evidence\u003c/h3>\n\u003cp>Many people’s first instinct to synthesize evidence is to count how many studies support versus did not support the hypothesis under investigation. This technique usually amounts to counting the number of studies with “significant” \u003cspan class=\"math inline\">\\(p\\)\u003c/span>-values, since – for better or for worse – “significance” is largely what drives the take-home conclusions researchers report \u003cspan class=\"citation\">(\u003ca href=\"#ref-mcshane2017statistical\" role=\"doc-biblioref\">McShane &amp; Gal, 2017\u003c/a>; \u003ca href=\"#ref-nelson1986interpretation\" role=\"doc-biblioref\">N. Nelson et al., 1986\u003c/a>)\u003c/span>. In meta-analysis, we call this practice of counting the number of significant \u003cspan class=\"math inline\">\\(p\\)\u003c/span>-values \u003cstrong>vote-counting\u003c/strong> \u003cspan class=\"citation\">(\u003ca href=\"#ref-borenstein2021introduction\" role=\"doc-biblioref\">Borenstein et al., 2021\u003c/a>)\u003c/span>. For example, in the \u003cspan class=\"citation\">Paluck et al. (\u003ca href=\"#ref-paluck2019contact\" role=\"doc-biblioref\">2019\u003c/a>)\u003c/span> meta-analysis, all studies had positive effect sizes, but only 12 of 27 were significant. So, based on this vote-count, we would have the impression that most studies do not support the contact hypothesis.\u003c/p>\n\u003cp>Many qualitative literature reviews use this vote-counting approach, although often not explicitly. Despite its intuitive appeal, vote-counting can be very misleading because it characterizes evidence solely in terms of dichotomized \u003cspan class=\"math inline\">\\(p\\)\u003c/span>-values, while entirely ignoring effect sizes. In Chapter \u003ca href=\"3-replication.html#replication\">3\u003c/a>, we saw how fetishizing statistical significance can mislead us when we consider individual studies. These problems also apply when considering multiple studies.\u003c/p>\n\u003cp>For example, small studies may consistently produce non-significant effects due to their limited power. But when many such studies are combined in a meta-analysis, the meta-analysis may provide strong evidence of a positive average effect. Inversely, many studies might have statistically significant effects, but if their effect sizes are small, then a meta-analysis might might indicate that the average effect size is too small to be practically meaningful. In these cases, vote-counting based on statistical significance can lead us badly astray \u003cspan class=\"citation\">(\u003ca href=\"#ref-borenstein2021introduction\" role=\"doc-biblioref\">Borenstein et al., 2021\u003c/a>)\u003c/span>. To avoid these pitfalls, meta-analysis combines the effect sizes estimates from each study (not just their \u003cspan class=\"math inline\">\\(p\\)\u003c/span>-values), weighting them in a principled way.\u003c/p>\n\u003c/div>\n\u003cdiv id=\"fixed-effects-meta-analysis\" class=\"section level3\" number=\"16.1.2\">\n\u003ch3>\u003cspan class=\"header-section-number\">16.1.2\u003c/span> Fixed-effects meta-analysis\u003c/h3>\n\u003cp>If vote-counting is a bad idea, how should we combine results across studies? Another intuitive approach might be to average effect sizes from each study. For example, in Paluck et al.’s meta-analysis, the mean of the studies’ effect size estimates is 0.44. This averaging approach is a step in the right direction, but it has an important limitation: averaging effect size estimates gives equal weight to each study. A small study \u003cspan class=\"citation\">(e.g., \u003ca href=\"#ref-clunies1989changing\" role=\"doc-biblioref\">Clunies-Ross &amp; O’meara, 1989\u003c/a> with N=30)\u003c/span> contributes as much to the mean effect size as a large study \u003cspan class=\"citation\">(e.g., \u003ca href=\"#ref-boisjoly2006empathy\" role=\"doc-biblioref\">Boisjoly et al., 2006\u003c/a> with N=1243)\u003c/span>. Larger studies provide more precise estimates of effect sizes than small studies, so weighting all studies equally is not ideal. Instead, larger studies should carry more weight in the analysis.\u003c/p>\n\u003cp>To address this issue, \u003cstrong>fixed-effects meta-analysis\u003c/strong> uses a \u003cstrong>weighted average\u003c/strong> approach. Larger, more precise studies are given more weight in the calculation of the overall effect size. Specifically, each study is weighted by the inverse of its variance (i.e., the inverse of its squared standard error). This makes sense because larger, more precise studies have smaller variances, and thus get more weight in the analysis.\u003c/p>\n\u003cp>In general terms, the fixed-effect pooled estimate is:\u003c/p>\n\u003cp>\u003cspan class=\"math display\">\\[\\widehat{\\mu} = \\frac{ \\sum_{i=1}^k w_i \\widehat{\\theta}_i}{\\sum_{i=1}^k w_i}\\]\u003c/span> where \u003cspan class=\"math inline\">\\(k\\)\u003c/span> is the number of studies, \u003cspan class=\"math inline\">\\(\\widehat{\\theta}_i\\)\u003c/span> is the point estimate of the \u003cspan class=\"math inline\">\\(i^{th}\\)\u003c/span> study, and \u003cspan class=\"math inline\">\\(w_i = 1/\\widehat{\\sigma}^2_i\\)\u003c/span> is study \u003cspan class=\"math inline\">\\(i\\)\u003c/span>’s weight in the analysis (i.e., the inverse of its variance).\u003ca href=\"#fn243\" class=\"footnote-ref\" id=\"fnref243\">\u003csup>243\u003c/sup>\u003c/a>\u003c/p>\n\u003c!-- In Paluck et al.'s meta-analysis, we would calculate the fixed-effect estimate, $\\widehat{\\mu}$, as: -->\n\u003c!-- \u003c!-- hard-coded from DF$d[1:2] and DF$se_d[1:2] -->\n\u003c!-- $$\\widehat{\\mu} = \\frac{ \\frac{\\widehat{\\theta}_{study1}}{\\widehat{\\sigma}^2_{study1}} + \\frac{\\widehat{\\theta}_{study2}}{\\widehat{\\sigma}^2_{study2}} + \\cdots}{ \\frac{1}{\\widehat{\\sigma}^2_{study1}} + \\frac{1}{\\widehat{\\sigma}^2_{study2}} + \\cdots } = -->\n\u003c!-- \\frac{ \\frac{0.03}{0.08^2} + \\frac{0.30}{0.08^2} + \\cdots }{ \\frac{1}{0.08^2} + \\frac{1}{0.08^2} + \\cdots }$$ -->\n\u003cp>We can use the fixed-effects formula to estimate that the overall effect size in Paluck et al.’s meta-analysis studies is a standardized mean difference of \u003cspan class=\"math inline\">\\(\\widehat{\\mu}\\)\u003c/span> = 0.28; 95% confidence interval [0.23, 0.34]; \u003cspan class=\"math inline\">\\(p &lt; .001\\)\u003c/span>. Because Cohen’s \u003cspan class=\"math inline\">\\(d\\)\u003c/span> is our effect size index, this estimate means that intergroup contact decreased prejudice by 0.28 standard deviations.\u003c/p>\n\u003c/div>\n\u003cdiv id=\"limitations-of-fixed-effects-meta-analysis\" class=\"section level3\" number=\"16.1.3\">\n\u003ch3>\u003cspan class=\"header-section-number\">16.1.3\u003c/span> Limitations of fixed-effects meta-analysis\u003c/h3>\n\u003cp>One of the limitations of fixed-effect meta-analysis is that it assumes that the true effect size is, well, \u003cem>fixed\u003c/em>! In other words, fixed-effect meta-analysis assumes that there is a single effect size that all studies are estimating. This is a stringent assumption. For example, imagine that intergroup contact decreases prejudice when the group succeeds at its goal, but \u003cem>increases\u003c/em> prejudice when the group fails at its goal. If we meta-analyzed two studies, one in which intergroup contact substantially increased prejudice, and one in which intergroup contact substantially decreased prejudice, it might appear that the true effect of intergroup contact is close to zero, when in fact both of the meta-analyzed studies have large effects.\u003c/p>\n\u003cp>In Paluck et al.’s meta-analysis, studies differed in several ways that could lead to different true effects. For example, some studies recruited adult participants while others recruited children. If intergroup contact is more or less effective for adults versus children, then it is misleading to talk about a single (i.e., fixed) intergroup contact effect. Instead, we would say that the effects of intergroup contact vary across studies, an idea called \u003cstrong>heterogeneity\u003c/strong>.\u003c/p>\n\u003cp>Does the concept of heterogeneity remind you of anything from when we analyzed repeated-measures data in Chapter \u003ca href=\"7-models.html#models\">7\u003c/a> on models? Yes! Recall that, with repeated-measures data, we had to deal with the possibility of heterogeneity across participants – and of the ways we did so was by introducing participant-level random intercepts to our regression model. It turns out that we can do a similar thing in meta-analysis, dealing with heterogeneity across studies.\u003c/p>\n\u003c/div>\n\u003cdiv id=\"random-effects-meta-analysis\" class=\"section level3\" number=\"16.1.4\">\n\u003ch3>\u003cspan class=\"header-section-number\">16.1.4\u003c/span> Random-effects meta-analysis\u003c/h3>\n\u003cp>While fixed-effect meta-analysis essentially assumes that all studies in the meta-analysis have the same population effect size, \u003cspan class=\"math inline\">\\(\\mu\\)\u003c/span>, random-effects meta-analysis instead postulates that studies’ population effects come from a normal distribution with mean \u003cspan class=\"math inline\">\\(\\mu\\)\u003c/span> and standard deviation \u003cspan class=\"math inline\">\\(\\tau\\)\u003c/span>.\u003ca href=\"#fn244\" class=\"footnote-ref\" id=\"fnref244\">\u003csup>244\u003c/sup>\u003c/a> The larger the standard deviation, \u003cspan class=\"math inline\">\\(\\tau\\)\u003c/span>, the more heterogeneous the effects are across studies. A random-effects model then estimates both \u003cspan class=\"math inline\">\\(\\mu\\)\u003c/span> and \u003cspan class=\"math inline\">\\(\\tau\\)\u003c/span>, for example by maximum likelihood \u003cspan class=\"citation\">(\u003ca href=\"#ref-brockwell2001comparison\" role=\"doc-biblioref\">Brockwell &amp; Gordon, 2001\u003c/a>; \u003ca href=\"#ref-dersimonian1986meta\" role=\"doc-biblioref\">DerSimonian &amp; Laird, 1986\u003c/a>)\u003c/span>.\u003ca href=\"#fn245\" class=\"footnote-ref\" id=\"fnref245\">\u003csup>245\u003c/sup>\u003c/a>\u003c/p>\n\u003cp>Like fixed-effect meta-analysis, the random-effects estimate of \u003cspan class=\"math inline\">\\(\\widehat{\\mu}\\)\u003c/span> is still a weighted average of studies’ effect size estimates: \u003cspan class=\"math display\">\\[\\widehat{\\mu} = \\frac{ \\sum_{i=1}^k w_i \\widehat{\\theta}_i}{\\sum_{i=1}^k w_i}\\]\u003c/span>\u003c/p>\n\u003cp>However, in random-effects meta-analysis, the inverse-variance weights now incorporate heterogeneity: \u003cspan class=\"math inline\">\\(w_i = 1/\\left(\\widehat{\\tau}^2 + \\widehat{\\sigma}^2_i \\right)\\)\u003c/span>. Where before we had one term in our weights, now we have two. That is because these weights represent the inverse of studies’ \u003cem>marginal\u003c/em> variances, taking into account both statistical error due to their finite sample sizes (\u003cspan class=\"math inline\">\\(\\widehat{\\sigma}^2_i\\)\u003c/span>) and also genuine effect heterogeneity (\u003cspan class=\"math inline\">\\(\\widehat{\\tau}^2\\)\u003c/span>).\u003ca href=\"#fn246\" class=\"footnote-ref\" id=\"fnref246\">\u003csup>246\u003c/sup>\u003c/a>\u003c/p>\n\u003cp>Conducting a random-effects meta-analysis of Paluck et al.’s dataset yields \u003cspan class=\"math inline\">\\(\\widehat{\\mu}\\)\u003c/span> = 0.4; 95% confidence interval [0.2, 0.61]; \u003cspan class=\"math inline\">\\(p &lt; .001\\)\u003c/span>. That is, they estimated that, \u003cem>on average across studies\u003c/em>, intergroup contact was associated with a decrease in prejudice of 0.4 standard deviations. This meta-analytic estimate is shown as the bottom line of Figure \u003ca href=\"16-meta.html#fig:meta-forest\">16.1\u003c/a>.\u003c/p>\n\u003cp>However, these effects appeared to differ across studies. Paluck et al. estimated that the standard deviation of effects across studies was \u003cspan class=\"math inline\">\\(\\widehat{\\tau}\\)\u003c/span> = 0.44 ; 95% confidence interval [0.25, 0.57]. This estimate indicates a substantial amount of heterogeneity! To conveniently visualize these results, we can plot the estimated density of the population effects, which is just a normal distribution with mean \u003cspan class=\"math inline\">\\(\\widehat{\\mu}\\)\u003c/span> and standard deviation \u003cspan class=\"math inline\">\\(\\widehat{\\tau}\\)\u003c/span> (Figure \u003ca href=\"16-meta.html#fig:meta-densities\">16.2\u003c/a>).\u003c/p>\n\u003cp>This meta-analysis highlights an important point:that the overall effect size estimate \u003cspan class=\"math inline\">\\(\\widehat{\\mu}\\)\u003c/span> represents only the \u003cem>mean\u003c/em> population effect across studies. It tells us nothing about how much the effects \u003cem>vary\u003c/em> across studies. Thus, we recommend always reporting the heterogeneity estimate \u003cspan class=\"math inline\">\\(\\widehat{\\tau}\\)\u003c/span>, preferably along with other related metrics that help summarize the distribution of effect sizes across studies \u003cspan class=\"citation\">(\u003ca href=\"#ref-mathur_mam\" role=\"doc-biblioref\">Mathur &amp; VanderWeele, 2019\u003c/a>, \u003ca href=\"#ref-npphat\" role=\"doc-biblioref\">2020b\u003c/a>; \u003ca href=\"#ref-riley2011interpretation\" role=\"doc-biblioref\">Riley et al., 2011\u003c/a>; \u003ca href=\"#ref-wang2019simple\" role=\"doc-biblioref\">Wang &amp; Lee, 2019\u003c/a>)\u003c/span>. Reporting the heterogeneity helps readers know how consistent or inconsistent the effects are across studies, which may point to the need to investigate \u003cem>moderators\u003c/em> of the effect (i.e., factors that are associated with larger or smaller effects, such as whether participants were adults or children).\u003ca href=\"#fn247\" class=\"footnote-ref\" id=\"fnref247\">\u003csup>247\u003c/sup>\u003c/a>\u003c/p>\n\u003cp>\n\u003cspan class=\"marginnote shownote\">\n\u003c!--\n\u003cdiv class=\"figure\">-->\u003cspan style=\"display:block;\" id=\"fig:meta-densities\">\u003c/span>\n\u003cimg src=\"experimentology_files/figure-html/meta-densities-1.png\" alt=\"Estimated distribution of population effects from random-effects meta-analysis of Paluck et. al's dataset (heavy red curve) and estimated density of studies' point estimates (thin black curve).\" width=\"\\linewidth\"  />\n\u003c!--\n\u003cp class=\"caption marginnote\">-->Figure 16.2: Estimated distribution of population effects from random-effects meta-analysis of Paluck et. al’s dataset (heavy red curve) and estimated density of studies’ point estimates (thin black curve).\u003c!--\u003c/p>-->\n\u003c!--\u003c/div>-->\u003c/span>\n\u003c/p>\n\u003c/div>\n\u003c/div>\n\u003cdiv id=\"bias-in-meta-analysis\" class=\"section level2\" number=\"16.2\">\n\u003ch2>\u003cspan class=\"header-section-number\">16.2\u003c/span> Bias in meta-analysis\u003c/h2>\n\u003cp>Meta-analysis is an invaluable tool for synthesizing evidence across studies. However, the accuracy of meta-analysis can be compromised by two categories of bias: \u003cstrong>within-study biases\u003c/strong> and \u003cstrong>across-study biases\u003c/strong>. Either type can lead to meta-analysis estimates that are too large, too small, or in the wrong direction. We will now discuss examples of each type of bias as well as ways to address these biases when conducting a meta-analysis. This includes mitigating the biases at the outset through sound meta-analysis design and also assessing the robustness of the ultimate conclusions to possible remaining bias.\u003c/p>\n\u003cdiv id=\"within-study-biases\" class=\"section level3\" number=\"16.2.1\">\n\u003ch3>\u003cspan class=\"header-section-number\">16.2.1\u003c/span> Within-study biases\u003c/h3>\n\u003cp>Within-study biases – such as demand characteristics, confounds, and order effects – not only impact the validity of individual studies, but also any attempt to synthesize those studies. In other words: garbage in, garbage out. For example, \u003cspan class=\"citation\">Paluck et al. (\u003ca href=\"#ref-paluck2019contact\" role=\"doc-biblioref\">2019\u003c/a>)\u003c/span> noted that early studies on intergroup contact almost exclusively used non-randomized designs. Imagine a hypothetical study where researchers studied a completely ineffective intergroup contact intervention, and non-randomly assigned low-prejudice people to the intergroup contact condition and high-prejudice people to the control condition. In a scenario like this, the researcher will, of course, find that the prejudice was lower in the intergroup contact condition. However, this is not a true effect of the contact intervention, but rather a spurious effect of non-random assignment (i.e., confounding). Now imagine meta-analyzing many studies with similarly poor designs. The meta-analyst would find impressive evidence of an intergroup contact effect, but this is simply driven by systematic non-random assignment.\u003c/p>\n\u003cp>To mitigate this problem, meta-analysts often exclude studies that may be affected by within-study bias. For example, \u003cspan class=\"citation\">Paluck et al. (\u003ca href=\"#ref-paluck2019contact\" role=\"doc-biblioref\">2019\u003c/a>)\u003c/span> excluded non-randomized studies to minimize concerns about confounding. Preferably, inclusion and exclusion criteria for meta-analyses should be preregistered to ensure that these decisions are applied in a systematic and uniform way, rather than being applied after looking at the results.\u003c/p>\n\u003cp>Sometimes, though, certain sources of bias cannot be eliminated through exclusion criteria, for example because it is not possible or ethical to randomize the independent variable. After data have been collected, meta-analysts should also assess studies’ risks of bias qualitatively using established rating tools \u003cspan class=\"citation\">(\u003ca href=\"#ref-sterne2016robins\" role=\"doc-biblioref\">Sterne et al., 2016\u003c/a>)\u003c/span>. Doing so allows the meta-analyst to communicate how much within-study bias there may be.\u003ca href=\"#fn248\" class=\"footnote-ref\" id=\"fnref248\">\u003csup>248\u003c/sup>\u003c/a> Meta-analysts can also conduct sensitivity analyses to assess how much results might be affected by different within-study biases or by excluding certain types of studies \u003cspan class=\"citation\">(\u003ca href=\"#ref-art\" role=\"doc-biblioref\">Mathur &amp; VanderWeele, 2022\u003c/a>)\u003c/span>. For example, if nonrandom assignment is a concern, a meta-analysts may run the analyses including only randomized studies, versus including all studies, in order to determine if including nonrandomized studies changes the meta-analysis results. These two options parallel our discussion of experimental preregistration in Chapter \u003ca href=\"11-prereg.html#prereg\">11\u003c/a>: To allay concerns about results-dependent meta-analysis, researchers can either pre-register their analyses ahead of time or else be transparent about their choices after the fact. Sensitivity analyses can allay concerns that a specific choice of exclusion criteria is critically related to the reported results.\u003c/p>\n\u003c!-- It might be nice to show the reader an example of a risk of bias rating scale. Doing so would also help clarify what types of within-study biases meta-analysts are typically concerned about. https://drive.google.com/file/d/1Q4Fk3HCuBRwIDWTGZa5oH11OdR4Gbhdo/view-->\n\u003c/div>\n\u003cdiv id=\"across-study-biases\" class=\"section level3\" number=\"16.2.2\">\n\u003ch3>\u003cspan class=\"header-section-number\">16.2.2\u003c/span> Across-study biases\u003c/h3>\n\u003cp>Across-study biases occur if, for example, researchers \u003cstrong>selectively report\u003c/strong> certain types of findings or selectively publishing certain types of findings (\u003cstrong>publication bias\u003c/strong>). Often, these across-study biases favor statistically significant positive results, which means the meta-analysis of the available results will be inflated. For example, if researchers publish only the studies that yield statistically significant positive results and hide the studies that don’t, statistically combining the published studies via meta-analysis will obviously lead to exaggerated effect size estimates.\u003c/p>\n\u003cdiv class=\"box accident_report\">\n\u003cp>(TITLE) Quantifying publication bias in the social sciences\u003c/p>\n\u003cp>In 2014, \u003cspan class=\"citation\">Franco et al. (\u003ca href=\"#ref-franco2014\" role=\"doc-biblioref\">2014\u003c/a>)\u003c/span> and colleagues examined the population of 221 studies conducted through a funding initiative called TESS (“time-sharing experiments in the social sciences”) that helps researchers run experiments on nationally-representative samples in the U.S. This sample of studies was unique in that everyone that used TESS had to register their study with the group. That meant that Franco et al. knew the whole universe of studies that had been conducted using TESS – a key piece of information that meta-analysts almost never have available.\u003c/p>\n\u003cp>Using this information, Franco and colleagues examined the records of these studies to determine whether the researchers found statistically significant results, a mixture of statistically significant and non-significant results, or only non-significant results. Then, they examined the likelihood that these results were published in the scientific literature.\u003c/p>\n\u003cp>Which results do you think were most likely to be published? If you guessed “significant results,” then you have a good intuition about publication bias. Over 60% of studies with statistically significant results were published, compared to the mere 25% of studies that produced only statistically non-significant results. This finding was important because it quantified how strong that bias was, at least for one sample of studies.\u003c/p>\n\u003c/div>\n\u003cp>Like within-study biases, meta-analysts often try to mitigate across-study biases by being careful about what studies make it into the meta-analysis. Meta-analysts don’t only want to capture high-profile, published studies on their effect of interest, but also studies published in low-profile journals and the so-called “gray literature” [i.e., unpublished dissertations and theses; \u003cspan class=\"citation\">Lefebvre et al. (\u003ca href=\"#ref-lefebvre2019searching\" role=\"doc-biblioref\">2019\u003c/a>)\u003c/span>].\u003ca href=\"#fn249\" class=\"footnote-ref\" id=\"fnref249\">\u003csup>249\u003c/sup>\u003c/a>\u003c/p>\n\u003cp>There are also statistical methods to help assess how robust the results may be to across-study biases. Among the most popular tools to assess and correct for publication bias is the \u003cstrong>funnel plot\u003c/strong> \u003cspan class=\"citation\">(\u003ca href=\"#ref-duval2000trim\" role=\"doc-biblioref\">Duval &amp; Tweedie, 2000\u003c/a>; \u003ca href=\"#ref-egger1997bias\" role=\"doc-biblioref\">Egger et al., 1997\u003c/a>)\u003c/span>. A funnel plot shows the relationship between studies’ effect estimates and their precision (usually their standard error). They are called “funnel plots” because if there is no publication bias, then as precision increases, the effects “funnel” towards the meta-analytic estimate. As the precision is smaller, they spread out more because of greater measurement error.\u003c/p>\n\u003cp>Here is an example of one type of funnel plot \u003cspan class=\"citation\">(\u003ca href=\"#ref-sapb\" role=\"doc-biblioref\">Mathur &amp; VanderWeele, 2020c\u003c/a>)\u003c/span> for a simulated meta-analysis of 100 studies with no publication bias:\u003c/p>\n\u003cdiv class=\"figure\">\u003cspan style=\"display:block;\" id=\"fig:meta-funnel-unbiased\">\u003c/span>\n\u003cp class=\"caption marginnote shownote\">\nFigure 16.3: Significance funnel plot for a meta-analysis simulated to have no publication bias. Orange points: studies with \u003cspan class=\"math inline\">\\(p &lt; 0.05\\)\u003c/span> and positive estimates. Grey points: studies with \u003cspan class=\"math inline\">\\(p\\)\u003c/span> \u003cspan class=\"math inline\">\\(\\ge\\)\u003c/span> \u003cspan class=\"math inline\">\\(0.05\\)\u003c/span> or negative estimates. Black diamond: random-effects estimate of \u003cspan class=\"math inline\">\\(\\widehat{\\mu}\\)\u003c/span>.\n\u003c/p>\n\u003cimg src=\"experimentology_files/figure-html/meta-funnel-unbiased-1.png\" alt=\"Significance funnel plot for a meta-analysis simulated to have no publication bias. Orange points: studies with $p &lt; 0.05$ and positive estimates. Grey points: studies with $p$ $\\ge$ $0.05$ or negative estimates. Black diamond: random-effects estimate of $\\widehat{\\mu}$.\" width=\"\\linewidth\"  />\n\u003c/div>\n\u003cp>As implied by the “funnel” moniker, our plot looks a little like a funnel. Larger studies (those with smaller standard errors) cluster more closely around the mean of 0.34 than do smaller studies, but large and small studies alike have point estimates centered around the mean. That is, the funnel plot is symmetric.\u003ca href=\"#fn250\" class=\"footnote-ref\" id=\"fnref250\">\u003csup>250\u003c/sup>\u003c/a>\u003c/p>\n\u003cp>\n\u003cspan class=\"marginnote shownote\">\n\u003c!--\n\u003cdiv class=\"figure\">-->\u003cspan style=\"display:block;\" id=\"fig:meta-classic-funnel\">\u003c/span>\n\u003cimg src=\"experimentology_files/figure-html/meta-classic-funnel-1.png\" alt=\"Classic funnel plot.\" width=\"\\linewidth\"  />\n\u003c!--\n\u003cp class=\"caption marginnote\">-->Figure 16.4: Classic funnel plot.\u003c!--\u003c/p>-->\n\u003c!--\u003c/div>-->\u003c/span>\n\u003c/p>\n\u003cp>Not all funnel plots are symmetric! Figure \u003ca href=\"16-meta.html#fig:meta-funnel-biased\">16.5\u003c/a> is what happens to our hypothetical meta-analysis if all studies with \u003cspan class=\"math inline\">\\(p&lt;0.05\\)\u003c/span> and positive estimates are published, but only 10% of studies with \u003cspan class=\"math inline\">\\(p \\ge 0.05\\)\u003c/span> or with negative estimates are published. The introduction of publication bias dramatically inflates the pooled estimate from 0.34 to 1.15. Also, there appears to be a correlation between studies’ estimates and their standard errors, such that smaller studies tend to have larger estimates than do larger studies. This correlation is often called \u003cstrong>funnel plot asymmetry\u003c/strong> because the funnel plot starts to look like a right triangle rather than a funnel. Funnel plot asymmetry \u003cem>can\u003c/em> be a diagnostic for publication bias, though it isn’t always a perfect indicator, as we’ll see in the next subsection.\u003c/p>\n\u003cdiv class=\"figure\">\u003cspan style=\"display:block;\" id=\"fig:meta-funnel-biased\">\u003c/span>\n\u003cp class=\"caption marginnote shownote\">\nFigure 16.5: Significance funnel plot for the same simulated meta-analysis after publication bias has occurred. Orange points: studies with \u003cspan class=\"math inline\">\\(p &lt; 0.05\\)\u003c/span> and positive estimates. Grey points: studies with \u003cspan class=\"math inline\">\\(p\\)\u003c/span> \u003cspan class=\"math inline\">\\(\\ge\\)\u003c/span> \u003cspan class=\"math inline\">\\(0.05\\)\u003c/span> or negative estimates. Black diamond: random-effects estimate of \u003cspan class=\"math inline\">\\(\\widehat{\\mu}\\)\u003c/span>.\n\u003c/p>\n\u003cimg src=\"experimentology_files/figure-html/meta-funnel-biased-1.png\" alt=\"Significance funnel plot for the same simulated meta-analysis after publication bias has occurred. Orange points: studies with $p &lt; 0.05$ and positive estimates. Grey points: studies with $p$ $\\ge$ $0.05$ or negative estimates. Black diamond: random-effects estimate of $\\widehat{\\mu}$.\" width=\"\\linewidth\"  />\n\u003c/div>\n\u003c/div>\n\u003cdiv id=\"across-study-bias-correction\" class=\"section level3\" number=\"16.2.3\">\n\u003ch3>\u003cspan class=\"header-section-number\">16.2.3\u003c/span> Across-study bias correction\u003c/h3>\n\u003cp>How do we identify and correct bias across studies? Given that some forms of publication bias induce a correlation between studies’ point estimates and their standard errors, several popular statistical methods, such as Trim-and-Fill \u003cspan class=\"citation\">(\u003ca href=\"#ref-duval2000trim\" role=\"doc-biblioref\">Duval &amp; Tweedie, 2000\u003c/a>)\u003c/span> and Egger’s regression \u003cspan class=\"citation\">(\u003ca href=\"#ref-egger1997bias\" role=\"doc-biblioref\">Egger et al., 1997\u003c/a>)\u003c/span> are designed to quantify funnel plot asymmetry.\u003c/p>\n\u003cp>However, it is important to note that funnel plot asymmetry does not always imply that there is publication bias, nor does publication bias always cause funnel plot asymmetry. Sometimes funnel plot asymmetry is driven by genuine differences in the effects being studied in small and large studies \u003cspan class=\"citation\">(\u003ca href=\"#ref-egger1997bias\" role=\"doc-biblioref\">Egger et al., 1997\u003c/a>; \u003ca href=\"#ref-lau2006case\" role=\"doc-biblioref\">Lau et al., 2006\u003c/a>)\u003c/span>. For example, in a meta-analysis of intervention studies, if the most effective interventions are also the most expensive or difficult to implement, these highly effective interventions might be used primarily in the smallest studies (“small study effects”). Essentially, funnel plots and related methods are best suited to detecting publication bias in which (1) small studies with large positive point estimates are more likely to be published than small studies with small or negative point estimates; and (2) the largest studies are published regardless of the magnitude of their point estimates. That model of publication bias is sometimes what is happening, but not always!\u003c/p>\n\u003cp>A more flexible approach for detecting publication bias uses \u003cstrong>selection models\u003c/strong>. These models can detect other forms of publication bias that funnel plots may not detect, such as publication bias that favors \u003cem>significant\u003c/em> results. We won’t cover these methods in detail here, but we think they are a better approach to the question, along with related sensitivity analyses.\u003ca href=\"#fn251\" class=\"footnote-ref\" id=\"fnref251\">\u003csup>251\u003c/sup>\u003c/a>\u003c/p>\n\u003cp>You may also have heard of “\u003cspan class=\"math inline\">\\(p\\)\u003c/span>-methods” to detect across-study biases such as \u003cspan class=\"math inline\">\\(p\\)\u003c/span>-curve and \u003cspan class=\"math inline\">\\(p\\)\u003c/span>-uniform \u003cspan class=\"citation\">(\u003ca href=\"#ref-simonsohn2014p\" role=\"doc-biblioref\">Simonsohn et al., 2014\u003c/a>; \u003ca href=\"#ref-van2015meta\" role=\"doc-biblioref\">Van Assen et al., 2015\u003c/a>)\u003c/span>. These methods essentially assess whether the significant \u003cspan class=\"math inline\">\\(p\\)\u003c/span>-values “bunch up” just under 0.05, which is taken to indicate publication bias. These methods are increasingly popular in psychology and have their merits. However, it is important to note that these methods are actually simplified versions of selection models \u003cspan class=\"citation\">(e.g., \u003ca href=\"#ref-hedges1984estimation\" role=\"doc-biblioref\">Hedges, 1984\u003c/a>)\u003c/span> that work only under considerably more restrictive settings than do the original selection models [for example, when there is not heterogeneity across studies; \u003cspan class=\"citation\">McShane et al. (\u003ca href=\"#ref-mcshane2016adjusting\" role=\"doc-biblioref\">2016\u003c/a>)\u003c/span>]. For this reason, it is usually (although not always) better to use selection models in place of the more restrictive \u003cspan class=\"math inline\">\\(p\\)\u003c/span>-methods.\u003c/p>\n\u003cp>Paluck et al. used a regression-based approach to assess and correct for publication bias. This approach provided significant evidence of a relationship between the standard error and effect size (i.e., an asymmetric funnel plot). Again, this asymmetry could reflect publication bias or other sources of correlation between studies’ estimates and their standard errors. Paluck et al. also used this same regression-based approach to try to correct for potential publication bias. Results from this model indicated that the bias-corrected effect size estimate was close to zero. In other words, even though all studies estimated that intergroup contact decreased prejudice, it is possible that there are unpublished studies that did not find this (or found that intergroup contact increased prejudice).\u003c/p>\n\u003cdiv class=\"box accident_report\">\n\u003cp>(TITLE) Garbage in, garbage out? Meta-analyzing bad research\u003c/p>\n\u003cp>Botox can help eliminate wrinkles. But some researchers have also suggested that it may help treat clinical depression when used to paralyze the muscles associated with frowning. As crazy as they may sound, a quick examination of the literature would lead many to conclude that this treatment works. Studies that randomly assign depressed patients to either receive Botox injections or saline injections do indeed find that Botox recipients exhibit decreases in depression. And when you combine all available evidence in a meta-analysis, you find that this difference is quite large: \u003cspan class=\"math inline\">\\(\\widehat{d}\\)\u003c/span> = 0.83, 95% CI [0.52, 1.14]. Unfortunately, this meta-analytic result has problems with both within- and between-study bias \u003cspan class=\"citation\">(\u003ca href=\"#ref-coles2019does\" role=\"doc-biblioref\">Coles et al., 2019\u003c/a>)\u003c/span>.\u003c/p>\n\u003cp>First, participants are not supposed to know whether they have been randomly assigned to receive Botox or a control saline injections. However, only one of these treatments leads the upper half of your face to be paralyzed! After a couple of weeks you’re pretty likely to figure out whether you received the Botox treatment or control saline injection. Thus, the apparent effect of Botox on depression could very well be a placebo effect, a clear within-study bias issue.\u003c/p>\n\u003cp>Second, 51% of the outcomes measured were found not to have been reported by the study authors. This finding raises concerns about selective reporting: perhaps researchers examining the effects of Botox on depression only report the outcomes that demonstrate an effect, but not those that do not. In this scenario, any meta-analytic conclusions are potentially undermined by between-study bias.\u003c/p>\n\u003c/div>\n\u003c/div>\n\u003c/div>\n\u003cdiv id=\"chapter-summary-meta-analysis\" class=\"section level2\" number=\"16.3\">\n\u003ch2>\u003cspan class=\"header-section-number\">16.3\u003c/span> Chapter summary: Meta-analysis\u003c/h2>\n\u003cp>Taken together, Paluck and colleagues’ use of meta-analysis provided several important insights that would have been easy to miss in a non-quantitative review. First, despite a preponderance of non-significant findings, intergroup contact interventions were estimated to decrease prejudice by on average 0.4 standard deviations. On the other hand, there was considerable heterogeneity in intergroup contact effects, suggesting that there are important moderators of the effectiveness of these interventions. And finally, publication bias was a substantial concern, suggesting the need for follow-up research that will be published regardless of the outcome.\u003c/p>\n\u003cp>Overall, meta-analysis is a key technique for aggregating evidence across studies. Meta-analysis allows researchers to move beyond the bias of naive techniques like vote counting and towards a more quantitative summary of an experimental effect. Unfortunately, a meta-analysis is only as good as the literature it’s based on, so the aspiring meta-analyst must be aware of both within- and between-study biases!\u003c/p>\n\u003cdiv class=\"box exercises\">\n\u003col style=\"list-style-type: decimal\">\n\u003cli>\u003cp>Imagine that you read the following result in the abstract of a meta-analysis: “In 83 randomized studies of middle school children, replacing one hour of class time with mindfulness meditation significantly improved standardized test scores (standardized mean difference \u003cspan class=\"math inline\">\\(\\widehat{\\mu} = 0.05\\)\u003c/span>; 95% confidence interval: [\u003cspan class=\"math inline\">\\(0.01, 0.09\\)\u003c/span>]; \u003cspan class=\"math inline\">\\(p&lt;0.05\\)\u003c/span>).” Why is this a problematic way to report on meta-analysis results? Suggest a better sentence to replace this one.\u003c/p>\n\u003cp>As you read the rest of the meta-analysis, you find that the authors conclude that “These findings demonstrate robust benefits of meditation for children, suggesting that test scores improve even when the meditation is introduced as a replacement for normal class time.” You recall that the heterogeneity estimate was \u003cspan class=\"math inline\">\\(\\widehat{\\tau} = 0.90\\)\u003c/span>. Do you think that this result regarding the heterogeneity tends to support, or rather tends to undermine, the concluding sentence of the meta-analysis? Why?\u003c/p>\n\u003cp>What kinds of within-study biases would concern you in this meta-analysis? How might you assess the credibility of the meta-analyzed studies and of the meta-analysis as whole in light of these possible biases?\u003c/p>\u003c/li>\n\u003cli>\u003cp>Imagine you conduct a meta-analysis on a literature in which statistically significant results in either direction are much more likely to be published that non-significant results. Draw the funnel plot you would expect to see. Is the plot funnel symmetric or asymmetric?\u003c/p>\u003c/li>\n\u003cli>\u003cp>Why do you think small studies receive more weight in random-effects meta-analysis than in fixed-effects meta-analysis? Can you see why this is true mathematically based on the equations given above, and can you also explain the intuition in simple language?\u003c/p>\u003c/li>\n\u003c/ol>\n\u003c/div>\n\n\u003c/div>\n\u003c/div>\n\u003ch3>References\u003c/h3>\n\u003cdiv id=\"refs\" class=\"references csl-bib-body hanging-indent\" line-spacing=\"2\">\n\u003cdiv id=\"ref-allport1954nature\" class=\"csl-entry\">\nAllport, G. W. (1954). \u003cem>The nature of prejudice\u003c/em>.\n\u003c/div>\n\u003cdiv id=\"ref-boisjoly2006empathy\" class=\"csl-entry\">\nBoisjoly, J., Duncan, G. J., Kremer, M., Levy, D. M., &amp; Eccles, J. (2006). Empathy or antipathy? The impact of diversity. \u003cem>American Economic Review\u003c/em>, \u003cem>96\u003c/em>(5), 1890–1905.\n\u003c/div>\n\u003cdiv id=\"ref-borenstein2021introduction\" class=\"csl-entry\">\nBorenstein, M., Hedges, L. V., Higgins, J. P., &amp; Rothstein, H. R. (2021). \u003cem>Introduction to meta-analysis\u003c/em>. John Wiley &amp;amp; Sons.\n\u003c/div>\n\u003cdiv id=\"ref-brockwell2001comparison\" class=\"csl-entry\">\nBrockwell, S. E., &amp; Gordon, I. R. (2001). A comparison of statistical methods for meta-analysis. \u003cem>Statistics in Medicine\u003c/em>, \u003cem>20\u003c/em>(6), 825–840.\n\u003c/div>\n\u003cdiv id=\"ref-clunies1989changing\" class=\"csl-entry\">\nClunies-Ross, G., &amp; O’meara, K. (1989). Changing the attitudes of students towards peers with disabilities. \u003cem>Australian Psychologist\u003c/em>, \u003cem>24\u003c/em>(2), 273–284.\n\u003c/div>\n\u003cdiv id=\"ref-coles2019does\" class=\"csl-entry\">\nColes, N. A., Larsen, J. T., Kuribayashi, J., &amp; Kuelz, A. (2019). Does blocking facial feedback via botulinum toxin injections decrease depression? A critical review and meta-analysis. \u003cem>Emotion Review\u003c/em>, \u003cem>11\u003c/em>(4), 294–309.\n\u003c/div>\n\u003cdiv id=\"ref-dersimonian1986meta\" class=\"csl-entry\">\nDerSimonian, R., &amp; Laird, N. (1986). Meta-analysis in clinical trials. \u003cem>Controlled Clinical Trials\u003c/em>, \u003cem>7\u003c/em>(3), 177–188.\n\u003c/div>\n\u003cdiv id=\"ref-duval2000trim\" class=\"csl-entry\">\nDuval, S., &amp; Tweedie, R. (2000). Trim and fill: A simple funnel-plot–based method of testing and adjusting for publication bias in meta-analysis. \u003cem>Biometrics\u003c/em>, \u003cem>56\u003c/em>(2), 455–463. \u003ca href=\"https://doi.org/10.1111/j.0006-341X.2000.00455.x\">https://doi.org/10.1111/j.0006-341X.2000.00455.x\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-egger1997bias\" class=\"csl-entry\">\nEgger, M., Smith, G. D., Schneider, M., &amp; Minder, C. (1997). Bias in meta-analysis detected by a simple, graphical test. \u003cem>BMJ\u003c/em>, \u003cem>315\u003c/em>(7109), 629–634. \u003ca href=\"https://doi.org/10.1136/bmj.315.7109.629\">https://doi.org/10.1136/bmj.315.7109.629\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-franco2014\" class=\"csl-entry\">\nFranco, A., Malhotra, N., &amp; Simonovits, G. (2014). Publication bias in the social sciences: Unlocking the file drawer. In \u003cem>Science\u003c/em> (No. 6203; Vol. 345, pp. 1502–1505).\n\u003c/div>\n\u003cdiv id=\"ref-goldstein2008room\" class=\"csl-entry\">\nGoldstein, N. J., Cialdini, R. B., &amp; Griskevicius, V. (2008). A room with a viewpoint: Using social norms to motivate environmental conservation in hotels. \u003cem>Journal of Consumer Research\u003c/em>, \u003cem>35\u003c/em>(3), 472–482.\n\u003c/div>\n\u003cdiv id=\"ref-grant2009typology\" class=\"csl-entry\">\nGrant, M. J., &amp; Booth, A. (2009). A typology of reviews: An analysis of 14 review types and associated methodologies. \u003cem>Health Information &amp; Libraries Journal\u003c/em>, \u003cem>26\u003c/em>(2), 91–108.\n\u003c/div>\n\u003cdiv id=\"ref-hedges1984estimation\" class=\"csl-entry\">\nHedges, L. V. (1984). Estimation of effect size under nonrandom sampling: The effects of censoring studies yielding statistically insignificant mean differences. \u003cem>Journal of Educational Statistics\u003c/em>, \u003cem>9\u003c/em>(1), 61–85. \u003ca href=\"https://doi.org/10.3102/10769986009001061\">https://doi.org/10.3102/10769986009001061\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-hedges2010robust\" class=\"csl-entry\">\nHedges, L. V., Tipton, E., &amp; Johnson, M. C. (2010). Robust variance estimation in meta-regression with dependent effect size estimates. \u003cem>Research Synthesis Methods\u003c/em>, \u003cem>1\u003c/em>(1), 39–65.\n\u003c/div>\n\u003cdiv id=\"ref-iyengar1988\" class=\"csl-entry\">\nIyengar, S., &amp; Greenhouse, J. B. (1988). Selection models and the file drawer problem. \u003cem>Statistical Science\u003c/em>, 109–117.\n\u003c/div>\n\u003cdiv id=\"ref-knapp2003improved\" class=\"csl-entry\">\nKnapp, G., &amp; Hartung, J. (2003). Improved tests for a random effects meta-regression with a single covariate. \u003cem>Statistics in Medicine\u003c/em>, \u003cem>22\u003c/em>(17), 2693–2710.\n\u003c/div>\n\u003cdiv id=\"ref-kotzeva2015eurostat\" class=\"csl-entry\">\nKotzeva, M., Brandmüller, T., &amp; Önnerfors, Å. (2015). \u003cem>Eurostat regional yearbook 2015\u003c/em>. Publications Office of the European Union.\n\u003c/div>\n\u003cdiv id=\"ref-lau2006case\" class=\"csl-entry\">\nLau, J., Ioannidis, J. P., Terrin, N., Schmid, C. H., &amp; Olkin, I. (2006). The case of the misleading funnel plot. \u003cem>BMJ\u003c/em>, \u003cem>333\u003c/em>(7568), 597–600. \u003ca href=\"https://doi.org/10.1136/bmj.333.7568.597\">https://doi.org/10.1136/bmj.333.7568.597\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-lefebvre2019searching\" class=\"csl-entry\">\nLefebvre, C., Glanville, J., Briscoe, S., Littlewood, A., Marshall, C., Metzendorf, M.-I., Noel-Storr, A., Rader, T., Shokraneh, F., Thomas, J.others. (2019). Searching for and selecting studies. \u003cem>Cochrane Handbook for Systematic Reviews of Interventions\u003c/em>, 67–107.\n\u003c/div>\n\u003cdiv id=\"ref-smt\" class=\"csl-entry\">\nMaier, M., VanderWeele, T. J., &amp; Mathur, M. B. (in press). Using selection models to assess sensitivity to publication bias: A tutorial and call for more routine use. \u003cem>Campbell Systematic Reviews\u003c/em>.\n\u003c/div>\n\u003cdiv id=\"ref-mathur_mam\" class=\"csl-entry\">\nMathur, M. B., &amp; VanderWeele, T. J. (2019). New metrics for meta-analyses of heterogeneous effects. \u003cem>Statistics in Medicine\u003c/em>, \u003cem>38\u003c/em>(8), 1336–1342.\n\u003c/div>\n\u003cdiv id=\"ref-npphat\" class=\"csl-entry\">\nMathur, M. B., &amp; VanderWeele, T. J. (2020b). Robust metrics and sensitivity analyses for meta-analyses of heterogeneous effects. \u003cem>Epidemiology\u003c/em>, \u003cem>31\u003c/em>(3), 356–358.\n\u003c/div>\n\u003cdiv id=\"ref-sapb\" class=\"csl-entry\">\nMathur, M. B., &amp; VanderWeele, T. J. (2020c). Sensitivity analysis for publication bias in meta-analyses. \u003cem>Journal of the Royal Statistical Society: Series C\u003c/em>, \u003cem>5\u003c/em>(69), 1091–1119.\n\u003c/div>\n\u003cdiv id=\"ref-sapbe\" class=\"csl-entry\">\nMathur, M. B., &amp; VanderWeele, T. J. (2021). Estimating publication bias in meta-analyses of peer-reviewed studies: A meta-meta-analysis across disciplines and journal tiers. \u003cem>Research Synthesis Methods\u003c/em>, \u003cem>12\u003c/em>(2), 176–191.\n\u003c/div>\n\u003cdiv id=\"ref-art\" class=\"csl-entry\">\nMathur, M. B., &amp; VanderWeele, T. J. (2022). Methods to address confounding and other biases in meta-analyses: Review and recommendations. \u003cem>Annual Review of Public Health\u003c/em>, \u003cem>1\u003c/em>(43).\n\u003c/div>\n\u003cdiv id=\"ref-mcshane2016adjusting\" class=\"csl-entry\">\nMcShane, B. B., Böckenholt, U., &amp; Hansen, K. T. (2016). Adjusting for publication bias in meta-analysis: An evaluation of selection methods and some cautionary notes. \u003cem>Perspectives on Psychological Science\u003c/em>, \u003cem>11\u003c/em>(5), 730–749. \u003ca href=\"https://doi.org/10.1177/1745691616662243\">https://doi.org/10.1177/1745691616662243\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-mcshane2017statistical\" class=\"csl-entry\">\nMcShane, B. B., &amp; Gal, D. (2017). Statistical significance and the dichotomization of evidence. \u003cem>Journal of the American Statistical Association\u003c/em>, \u003cem>112\u003c/em>(519), 885–895. \u003ca href=\"https://doi.org/10.1080/01621459.2017.1289846\">https://doi.org/10.1080/01621459.2017.1289846\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-nelson1986interpretation\" class=\"csl-entry\">\nNelson, N., Rosenthal, R., &amp; Rosnow, R. L. (1986). Interpretation of significance levels and effect sizes by psychological researchers. \u003cem>American Psychologist\u003c/em>, \u003cem>41\u003c/em>(11), 1299.\n\u003c/div>\n\u003cdiv id=\"ref-paluck2019contact\" class=\"csl-entry\">\nPaluck, E. L., Green, S. A., &amp; Green, D. P. (2019). The contact hypothesis re-evaluated. \u003cem>Behavioural Public Policy\u003c/em>, \u003cem>3\u003c/em>(2), 129–158.\n\u003c/div>\n\u003cdiv id=\"ref-pustejovsky2021meta\" class=\"csl-entry\">\nPustejovsky, J. E., &amp; Tipton, E. (2021). Meta-analysis with robust variance estimation: Expanding the range of working models. \u003cem>Prevention Science\u003c/em>, 1–14.\n\u003c/div>\n\u003cdiv id=\"ref-riley2011interpretation\" class=\"csl-entry\">\nRiley, R. D., Higgins, J. P., &amp; Deeks, J. J. (2011). Interpretation of random effects meta-analyses. \u003cem>BMJ\u003c/em>, \u003cem>342\u003c/em>.\n\u003c/div>\n\u003cdiv id=\"ref-scheibehenne2016\" class=\"csl-entry\">\nScheibehenne, B., Jamil, T., &amp; Wagenmakers, E.-J. (2016). Bayesian evidence synthesis can reconcile seemingly inconsistent results: The case of hotel towel reuse. \u003cem>Psychol. Sci.\u003c/em>, \u003cem>27\u003c/em>(7), 1043–1046.\n\u003c/div>\n\u003cdiv id=\"ref-simonsohn2014p\" class=\"csl-entry\">\nSimonsohn, U., Nelson, L. D., &amp; Simmons, J. P. (2014). P-curve: A key to the file-drawer. \u003cem>Journal of Experimental Psychology: General\u003c/em>, \u003cem>143\u003c/em>(2), 534.\n\u003c/div>\n\u003cdiv id=\"ref-sterne2016robins\" class=\"csl-entry\">\nSterne, J. A., Hernán, M. A., Reeves, B. C., Savović, J., Berkman, N. D., Viswanathan, M., Henry, D., Altman, D. G., Ansari, M. T., Boutron, I.others. (2016). ROBINS-i: A tool for assessing risk of bias in non-randomised studies of interventions. \u003cem>Bmj\u003c/em>, \u003cem>355\u003c/em>.\n\u003c/div>\n\u003cdiv id=\"ref-thompson2002should\" class=\"csl-entry\">\nThompson, S. G., &amp; Higgins, J. P. (2002). How should meta-regression analyses be undertaken and interpreted? \u003cem>Statistics in Medicine\u003c/em>, \u003cem>21\u003c/em>(11), 1559–1573.\n\u003c/div>\n\u003cdiv id=\"ref-tipton2015small\" class=\"csl-entry\">\nTipton, E. (2015). Small sample adjustments for robust variance estimation with meta-regression. \u003cem>Psychological Methods\u003c/em>, \u003cem>20\u003c/em>(3), 375.\n\u003c/div>\n\u003cdiv id=\"ref-tsuji2020addressing\" class=\"csl-entry\">\nTsuji, S., Cristia, A., Frank, M. C., &amp; Bergmann, C. (2020). Addressing publication bias in meta-analysis. \u003cem>Zeitschrift f\u003cspan>ü\u003c/span>r Psychologie\u003c/em>.\n\u003c/div>\n\u003cdiv id=\"ref-van2015meta\" class=\"csl-entry\">\nVan Assen, M. A., Aert, R. van, &amp; Wicherts, J. M. (2015). Meta-analysis using effect size distributions of only statistically significant studies. \u003cem>Psychological Methods\u003c/em>, \u003cem>20\u003c/em>(3), 293.\n\u003c/div>\n\u003cdiv id=\"ref-vevea1995\" class=\"csl-entry\">\nVevea, J. L., &amp; Hedges, L. V. (1995). A general linear model for estimating effect size in the presence of publication bias. \u003cem>Psychometrika\u003c/em>, \u003cem>60\u003c/em>(3), 419–435.\n\u003c/div>\n\u003cdiv id=\"ref-wang2019simple\" class=\"csl-entry\">\nWang, C.-C., &amp; Lee, W.-C. (2019). A simple method to estimate prediction intervals and predictive distributions: Summarizing meta-analyses beyond means and confidence intervals. \u003cem>Research Synthesis Methods\u003c/em>, \u003cem>10\u003c/em>(2), 255–266.\n\u003c/div>\n\u003c/div>\n\u003cdiv class=\"footnotes\">\n\u003chr />\n\u003col start=\"240\">\n\u003cli id=\"fn240\">\u003cp>We’ll primarily be using Cohen’s \u003cspan class=\"math inline\">\\(d\\)\u003c/span>, the standardized difference between means, which we introduced in Chapter \u003ca href=\"5-estimation.html#estimation\">5\u003c/a>. There are many more varieties of effect size available, but we focus here on \u003cspan class=\"math inline\">\\(d\\)\u003c/span> because it’s common and easy to reason about in the context of the statistical tools we introduced in the earlier sections of the book.\u003ca href=\"16-meta.html#fnref240\" class=\"footnote-back\">↩︎\u003c/a>\u003c/p>\u003c/li>\n\u003cli id=\"fn241\">\u003cp>Given the number of hotel bookings worldwide – 1.7 billion in the European Union alone in 2013 \u003cspan class=\"citation\">(\u003ca href=\"#ref-kotzeva2015eurostat\" role=\"doc-biblioref\">Kotzeva et al., 2015\u003c/a>)\u003c/span> – the insight provided by the meta-analysis is not trivial!\u003ca href=\"16-meta.html#fnref241\" class=\"footnote-back\">↩︎\u003c/a>\u003c/p>\u003c/li>\n\u003cli id=\"fn242\">\u003cp>You can ignore for now the column of percentages and the final line, “RE Model”; we will return to these later.\u003ca href=\"16-meta.html#fnref242\" class=\"footnote-back\">↩︎\u003c/a>\u003c/p>\u003c/li>\n\u003cli id=\"fn243\">\u003cp>If you are curious, the standard error of the fixed-effect \u003cspan class=\"math inline\">\\(\\widehat{\\mu}\\)\u003c/span> is \u003cspan class=\"math inline\">\\(\\frac{1}{\\sum_{i=1}^k w_i}\\)\u003c/span>. This standard error can be used to construct a confidence interval or \u003cspan class=\"math inline\">\\(p\\)\u003c/span>-value, as described in Chapter \u003ca href=\"6-inference.html#inference\">6\u003c/a>.\u003ca href=\"16-meta.html#fnref243\" class=\"footnote-back\">↩︎\u003c/a>\u003c/p>\u003c/li>\n\u003cli id=\"fn244\">\u003cp>Technically, other specifications of random-effects meta-analysis are possible. For example, robust variance estimation does not require making assumptions about the distribution of effects across studies \u003cspan class=\"citation\">(\u003ca href=\"#ref-hedges2010robust\" role=\"doc-biblioref\">Hedges et al., 2010\u003c/a>)\u003c/span>. These approaches also have other substantial advantages, like their ability to handle effects that are clustered [e.g., because some papers contribute multiple estimates; \u003cspan class=\"citation\">Hedges et al. (\u003ca href=\"#ref-hedges2010robust\" role=\"doc-biblioref\">2010\u003c/a>)\u003c/span>; \u003cspan class=\"citation\">Pustejovsky &amp; Tipton (\u003ca href=\"#ref-pustejovsky2021meta\" role=\"doc-biblioref\">2021\u003c/a>)\u003c/span>] and their ability to provide better inference in meta-analyses with relatively few studies \u003cspan class=\"citation\">(\u003ca href=\"#ref-tipton2015small\" role=\"doc-biblioref\">Tipton, 2015\u003c/a>)\u003c/span>. For these reasons, the authors of this book prefer to use these robust methods by default when conducting meta-analyses.\u003ca href=\"16-meta.html#fnref244\" class=\"footnote-back\">↩︎\u003c/a>\u003c/p>\u003c/li>\n\u003cli id=\"fn245\">\u003cp>A confidence interval and \u003cspan class=\"math inline\">\\(p\\)\u003c/span>-value for the random-effects estimate \u003cspan class=\"math inline\">\\(\\widehat{\\mu}\\)\u003c/span> can be obtained using standard theory for maximum likelihood estimates with an additional adjustment that helps account for uncertainty in estimating \u003cspan class=\"math inline\">\\(\\tau\\)\u003c/span> \u003cspan class=\"citation\">(\u003ca href=\"#ref-knapp2003improved\" role=\"doc-biblioref\">Knapp &amp; Hartung, 2003\u003c/a>)\u003c/span>.\u003ca href=\"16-meta.html#fnref245\" class=\"footnote-back\">↩︎\u003c/a>\u003c/p>\u003c/li>\n\u003cli id=\"fn246\">\u003cp>The estimate of \u003cspan class=\"math inline\">\\(\\widehat{\\tau}^2\\)\u003c/span> is a bit more complicated, but is essentially a weighted average of studies’ residuals, \u003cspan class=\"math inline\">\\(\\widehat{\\theta_i} - \\widehat{\\mu}\\)\u003c/span>, while subtracting away variation due to statistical error, \u003cspan class=\"math inline\">\\(\\widehat{\\sigma}^2_i\\)\u003c/span> \u003cspan class=\"citation\">(\u003ca href=\"#ref-brockwell2001comparison\" role=\"doc-biblioref\">Brockwell &amp; Gordon, 2001\u003c/a>; \u003ca href=\"#ref-dersimonian1986meta\" role=\"doc-biblioref\">DerSimonian &amp; Laird, 1986\u003c/a>)\u003c/span>.\u003ca href=\"16-meta.html#fnref246\" class=\"footnote-back\">↩︎\u003c/a>\u003c/p>\u003c/li>\n\u003cli id=\"fn247\">\u003cp>One common approach to investigating moderators in meta-analysis is meta-regression, in which moderators (e.g., type of intergroup contact) are included as covariates in a random-effects meta-analysis model \u003cspan class=\"citation\">(\u003ca href=\"#ref-thompson2002should\" role=\"doc-biblioref\">Thompson &amp; Higgins, 2002\u003c/a>)\u003c/span>. As in standard regression, coefficients can then be estimated for each moderator, representing the mean difference in population effect between studies with versus without the moderator.\u003ca href=\"16-meta.html#fnref247\" class=\"footnote-back\">↩︎\u003c/a>\u003c/p>\u003c/li>\n\u003cli id=\"fn248\">\u003cp>\u003cspan class=\"citation\">Paluck et al. (\u003ca href=\"#ref-paluck2019contact\" role=\"doc-biblioref\">2019\u003c/a>)\u003c/span> did not use this tool, but they could have used it to communicate, for example, the extent to which participants might have differentially dropped out of the study.\u003ca href=\"16-meta.html#fnref248\" class=\"footnote-back\">↩︎\u003c/a>\u003c/p>\u003c/li>\n\u003cli id=\"fn249\">\u003cp>Evidence is mixed regarding whether including gray literature actually reduces across-study biases in meta-analysis \u003cspan class=\"citation\">(\u003ca href=\"#ref-sapbe\" role=\"doc-biblioref\">Mathur &amp; VanderWeele, 2021\u003c/a>; \u003ca href=\"#ref-tsuji2020addressing\" role=\"doc-biblioref\">Tsuji et al., 2020\u003c/a>)\u003c/span>, but it is still common practice to try to include this literature.\u003ca href=\"16-meta.html#fnref249\" class=\"footnote-back\">↩︎\u003c/a>\u003c/p>\u003c/li>\n\u003cli id=\"fn250\">\u003cp>Classic funnel plots look more like Figure \u003ca href=\"16-meta.html#fig:meta-classic-funnel\">16.4\u003c/a>. Our version is different in a couple of ways. Most prominently, we don’t have the vertical axis reversed (which we think is confusing), and we don’t have the left boundary highlighted (because we think folks don’t typically select for negative studies).\u003ca href=\"16-meta.html#fnref250\" class=\"footnote-back\">↩︎\u003c/a>\u003c/p>\u003c/li>\n\u003cli id=\"fn251\">\u003cp>High-level overviews of selection models are given in \u003cspan class=\"citation\">McShane et al. (\u003ca href=\"#ref-mcshane2016adjusting\" role=\"doc-biblioref\">2016\u003c/a>)\u003c/span> and \u003cspan class=\"citation\">Maier et al. (\u003ca href=\"#ref-smt\" role=\"doc-biblioref\">in press\u003c/a>)\u003c/span>. For more methodological detail, see \u003cspan class=\"citation\">Hedges (\u003ca href=\"#ref-hedges1984estimation\" role=\"doc-biblioref\">1984\u003c/a>)\u003c/span>, \u003cspan class=\"citation\">Iyengar &amp; Greenhouse (\u003ca href=\"#ref-iyengar1988\" role=\"doc-biblioref\">1988\u003c/a>)\u003c/span>, and \u003cspan class=\"citation\">Vevea &amp; Hedges (\u003ca href=\"#ref-vevea1995\" role=\"doc-biblioref\">1995\u003c/a>)\u003c/span>. For a tutorial on fitting and interpreting selection models, see \u003cspan class=\"citation\">Maier et al. (\u003ca href=\"#ref-smt\" role=\"doc-biblioref\">in press\u003c/a>)\u003c/span>. For sensitivity analyses, see \u003cspan class=\"citation\">Mathur &amp; VanderWeele (\u003ca href=\"#ref-sapb\" role=\"doc-biblioref\">2020c\u003c/a>)\u003c/span>.\u003ca href=\"16-meta.html#fnref251\" class=\"footnote-back\">↩︎\u003c/a>\u003c/p>\u003c/li>\n\u003c/ol>\n\u003c/div>\n\u003cp style=\"text-align: center;\">\n\u003ca href=\"15-writing.html\">\u003cbutton class=\"btn btn-default\">Previous\u003c/button>\u003c/a>\n\u003ca href=\"17-conclusions.html\">\u003cbutton class=\"btn btn-default\">Next\u003c/button>\u003c/a>\n\u003c/p>\n\u003c/div>\n\u003c/div>\n\n\n\n"}}</script></body>

</html>