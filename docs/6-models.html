<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 6 Models | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 6 Models | Experimentology">

<title>Chapter 6 Models | Experimentology</title>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.6.1/grViz.js"></script>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.18/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="toc/toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-intro.html#intro"><span class="toc-section-number">1</span> Experiments and theories</a></li>
<li><a href="2-replication.html#replication"><span class="toc-section-number">2</span> Replication and reproducibility</a></li>
<li><a href="3-ethics.html#ethics"><span class="toc-section-number">3</span> Ethics</a></li>
<li class="part"><span><b>II Statistics</b></span></li>
<li><a href="4-estimation.html#estimation"><span class="toc-section-number">4</span> Estimation</a></li>
<li><a href="5-inference.html#inference"><span class="toc-section-number">5</span> Inference</a></li>
<li><a href="6-models.html#models"><span class="toc-section-number">6</span> Models</a></li>
<li class="part"><span><b>III Design and Planning</b></span></li>
<li><a href="7-measurement.html#measurement"><span class="toc-section-number">7</span> Measurement</a></li>
<li><a href="8-design.html#design"><span class="toc-section-number">8</span> Design of experiments</a></li>
<li><a href="9-sampling.html#sampling"><span class="toc-section-number">9</span> Sampling</a></li>
<li><a href="10-prereg.html#prereg"><span class="toc-section-number">10</span> Preregistration</a></li>
<li class="part"><span><b>IV Execution</b></span></li>
<li><a href="11-selection.html#selection"><span class="toc-section-number">11</span> Experimental strategy</a></li>
<li><a href="12-collection.html#collection"><span class="toc-section-number">12</span> Data collection</a></li>
<li><a href="13-management.html#management"><span class="toc-section-number">13</span> Project management</a></li>
<li class="part"><span><b>V Analysis and Reporting</b></span></li>
<li><a href="14-viz.html#viz"><span class="toc-section-number">14</span> Visualization</a></li>
<li><a href="15-eda.html#eda"><span class="toc-section-number">15</span> Exploratory data analysis</a></li>
<li><a href="16-writing.html#writing"><span class="toc-section-number">16</span> Reproducible writing</a></li>
<li><a href="17-meta.html#meta"><span class="toc-section-number">17</span> Meta-analysis</a></li>
<li><a href="18-conclusions.html#conclusions"><span class="toc-section-number">18</span> Conclusions</a></li>
<li class="part"><span><b>VI Appendices</b></span></li>
<li><a href="19-git.html#git"><span class="toc-section-number">19</span> GitHub Tutorial</a></li>
<li><a href="20-rmarkdown.html#rmarkdown"><span class="toc-section-number">20</span> R Markdown Tutorial</a></li>
<li><a href="21-tidyverse.html#tidyverse"><span class="toc-section-number">21</span> Tidyverse Tutorial</a></li>
<li><a href="22-ggplot.html#ggplot"><span class="toc-section-number">22</span> ggplot Tutorial</a></li>
<li><a href="23-instructors.html#instructors"><span class="toc-section-number">23</span> Instructor‚Äôs guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="models" class="section level1" number="6">
<h1><span class="header-section-number">Chapter 6</span> Models</h1>
<div class="learning-goals">
<p>üçé Learning goals:</p>
<ul>
<li>Reconceptualize statistical ‚Äútests‚Äù as models of data</li>
<li>Build intuitions about how specific ‚Äútests‚Äù (e.g., t-tests) relate to more general frameworks (e.g., regression, mixed effects models)</li>
<li>Identify which models are best suited for which research questions</li>
<li>Describe what it means to ‚Äòcontrol for‚Äô something</li>
<li>Explore what kinds of clustered variance are present in our designs</li>
<li>Select models appropriate for different kinds of dependent variables</li>
</ul>
</div>
<div class="case-study">
<p>üî¨ Case study: Sometimes effects are driven by specific stimuli. The ‚Äúlanguage as fixed effects fallacy‚Äù (Clark, 1973) means that many early psycholinguistic effects didn‚Äôt take into account stimulus variability. This critique extends to a wide variety of other domains, e.g.¬†task fMRI (Westfall, Nichols, and Yarkoni 2016).</p>
</div>
<div id="inference-and-estimation-for-two-group-designs" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Inference and estimation for two-group designs</h2>
<p>Throughout this book we‚Äôve taken the position that the goal of experiments is to estimate a causal effect of interest, ideally as part of some theory of how different constructs relate to one another. All this talk of hypotheses and inferences above is only indirectly related to that goal.</p>
<ul>
<li>Intuition builder: For very large n, or flat prior, Bayes and frequentist coincide.</li>
</ul>
<div id="simple-models-of-between-group-differences" class="section level3" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Simple models of between-group differences</h3>
<p>Introducing simple inference models:</p>
<ul>
<li>The chi-squared test for inferring whether two samples come from the same distribution</li>
<li>The t-test for inferring whether a single group‚Äôs effect differs from 0</li>
<li>The t-test for inferring whether two groups differ from one another</li>
<li>The paired t-test as a first glimpse at how we might account for participant-level random effects (see Chapter 7).</li>
</ul>
<div class="accident-report">
<p>‚ö†Ô∏è Accident report: Once you have the basic t-test under your belt, it might feel natural to compare each group to 0 and conclude that one group is different from 0 and the other one isn‚Äôt. But ‚Äúthe difference between significant and not significant is not necessarily itself statistically significant‚Äù (Nieuwenhuis, Forstmann, and Wagenmakers 2011).</p>
</div>
<p>How to go from theory to hypotheses to statistical model</p>
<p>Re-casting the t-test as a regression model</p>
<div class="interactive">
<p>‚å®Ô∏è Interactive box: Visualizing how different tests are variants of linear models.</p>
</div>
<ul>
<li>Discrete data and logistic regression. Same thing, different linking function. (lead-in to GLM: probit, Poisson, beta, etc.).</li>
</ul>
<p>Multilevel regression models of a difference between two groups, controlling for experimental items and subject</p>
<ul>
<li>Sidebar: what should you control for? Different subcultures in psychology either post-hoc control for or look for moderation by demographic factors. We discuss the consequences of these decisions for both precision and causal inference.</li>
</ul>
<p>Causality revisited: what can and can‚Äôt be concluded from an experiment</p>
<div class="accident-report">
<p>‚ö†Ô∏è Accident report: Mediation going wrong: even when you have a randomized experiment, you can still mess up your causal inference (Montgomery, Nyhan, and Torres 2018).</p>
</div>
<ul>
<li>Dropping subjects who fail a manipulation check can be problematic (Aronow, Baron, and Pinson 2019).</li>
<li>Mediation requires more confounding assumptions than causal inference about total effects, and these assumptions may be violated even in randomized experiments.</li>
</ul>
<p>Estimating one quantity in isolation is often not the best thing to do. In almost any experiment, there will be variation in the estimate that has to do with other known sources. Imagine the Stroop effect, which has a fairly consistent effect on both fast and slow readers <span class="citation">(<a href="#ref-haaf2017" role="doc-biblioref">Haaf &amp; Rouder, 2017</a>)</span>. But estimates of this effect will be more precise if we take into account that some readers are slower or faster, rather than just averaging across all this variation. That‚Äôs why we need models that take into account different sources of variation.</p>
</div>
</div>
<div id="covariates" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Covariates</h2>
<p>Going back to our example, now we have two variables ‚Äì age and order ‚Äì that are no longer confounded with our primary relationship of interest (i.e., Dylan and writing). But they may still be related to our outcome measure. Here‚Äôs what the picture looks like, repeated from above.</p>
<p>Even if they are not confounding our experimental manipulation, age and experimental condition order may still be correlated with our outcome measure, writing skill. How does this work? Well, the average treatment effect of Dylan on writing is still given by the regression Y ~ X. But we also know that there is some variance in Y that is due to X‚Äô and Z.</p>
<p>That‚Äôs because age and order are covariates: they may ‚Äì by virtue of their potential causal links with the outcome variable ‚Äì have some correlation with outcomes, even in a case where the predictor is experimentally manipulated. This should be intuitive for the external (age) covariate, but it‚Äôs true for both: they may account for variance in Y over and above that controlled by the experimental manipulation of X.</p>
<p>What should we do about our covariates?</p>
<p>Option 1. Nothing! We are totally safe in ignoring all of our covariates, regressing Y on X and treating the estimate as an unbiased estimate of the the effect (the ATE). This is why randomization is awesome. We are guaranteed that, in the limit of many different experiments, even though people with different ages will be in the different Dylan conditions, this source of variation will be averaged out.</p>
<p>The first fallacy of covariates is that, because you have a known covariate, you have to adjust for it. Not true. You can just ignore it and your estimate of the ATE is unbiased. This is the norm in cognitive psychology, for example: variation between individuals is treated as noise and averaged out. Of course, there are weaknesses in this strategy ‚Äì you will not learn about the relationship of your treatment to those covariates! ‚Äì but it is sound.</p>
<p>Option 2. If you have a small handful of covariates that you believe are meaningfully related to the outcome, you can plan in advance to adjust for them in your regression. In our Dylan example, this would be a pre-registered plan to add Z as a predictor: Y ~ X + Z. If age (Z) is highly correlated with writing ability (Y), then this will give us a more precise estimate of the ATE, while remaining unbiased.</p>
<p>When should we do this? Well, it turns out that you need a pretty strong correlation to make a big difference. There‚Äôs some nice code to simulate the effects of covariate adjustment on precision in this useful blogpost on covariate adjustment; I lightly adapted it. Here‚Äôs the result:</p>
<p>Root mean squared error (RMSE; lower RMSE means greater precision, in other words) is plotted as a function of the sample size (N). Different colors show the increase in precision when you control for covariates with different levels of correlation with the outcome variable. For low levels of correlation with the covariate, you don‚Äôt get much increase in precision (pink and red lines). Only as the correlation is .6 or above do we see noticeable increases in precision; and it only really makes a big difference with correlations in the range of .8.</p>
<p>Considering these numbers in light of our Dylan study, I would bet that age and writing skill are not correlated with writing skill &gt; .8 (unless we‚Äôre looking at ages from kindergarten to college!). I would guess that in an adult population this correlation would be much, much lower. So maybe it‚Äôs not worth controlling for age in our analyses.</p>
<p>And the same is probably true for order, our other covariate. Although perhaps we do think that our order has a strong correlation with our skill measure. For example, maybe our experiment is long and there are big fatigue effects. In that case, we would want to condition.</p>
<p>So these are are options: if the covariate is known to be very strong, we can condition. Otherwise we should probably not worry about it.</p>
<p>What shouldn‚Äôt we do with our covariates?</p>
<p>Don‚Äôt condition on lots and lots of covariates because you think they are theoretically important. There are lots of things that people do with covariates that they shouldn‚Äôt be doing. My personal hunch is that this is because a lot of researchers think that covariates (especially demographic ones like age, gender, socioeconomic status, race, ethnicity, etc.) are important. That‚Äôs true: these are important variables. But that doesn‚Äôt mean you need to control for them in every regression. This leads us to the second fallacy.</p>
<p>The second fallacy of covariates is that, because you think covariates are in general meaningful, it is not harmful to control for them in your regression model. In fact, if you control for meaningless covariates in a standard regression model, you will on average reduce your ability to see differences in your treatment effect. Just by chance your noise covariates will ‚Äúsoak up‚Äù variation in the response, leaving less to be accounted for by the true treatment effect! Even if you strongly suspect something is a covariate, you should be careful before throwing it into your regression model.</p>
<p>Don‚Äôt condition on covariates because your groups are unbalanced. People often talk about ‚Äúunhappy randomization‚Äù: you randomize adults to the different Dylan groups, for example, but then it turns out the mean age is a bit different between groups. Then you do a t-test or some other statistical test and find out that you actually have a significant age difference. But this makes no sense: because you randomized, you know that the difference in ages occurred by chance, so why are you using a t-test to test if the variation is due to chance? In addition, if your covariate isn‚Äôt highly correlated with the outcome, this difference won‚Äôt matter (see above). Finally, if you adjust for this covariate because of such a statistical test, you can actually end up biasing estimates of the ATE across the literature. Here‚Äôs a really useful blogpost from the Worldbank that has more details on why you shouldn‚Äôt follow this practice.</p>
<p>Don‚Äôt condition on covariates post-hoc. The previous example is a special case of a general practice that you shouldn‚Äôt follow. Don‚Äôt look at your data and then decide to control for covariates! Conditioning on covariates based on your data is an extremely common route for p-hacking; in fact, it‚Äôs so common that it shows up in Simmons, Nelson, &amp; Simonsohn‚Äôs (2011) instant classic False Positive Psychology paper as one of the key ingredients of analytic flexibility. Data-dependent selection of covariates is a quick route to false positive findings that will be less likely to be replicable in independent samples.</p>
<p>Don‚Äôt condition on a post-treatment variable. As we discussed above, there are some reasons to condition on highly-correlated covariates in general. But there‚Äôs an exception to this rule. There are some variables that are never OK to condition on ‚Äì in particular, any variable that is collected after treatment. For example, we might think that another good covariate would be someone‚Äôs enjoyment of Bob Dylan. So, after the writing measurements are done, we do a Dylan Appreciation Questionnaire (DAQ). The problem is, imagine that having a bad experience writing while listening to Dylan might actually change your DAQ score. So then people in the Dylan condition would have lower DAQ on average. If we control for DAQ in our regression (Y ~ X + DAQ), we then distort our estimate of the effects of Dylan. Because DAQ and X (Dylan condition) are correlated, DAQ will end up soaking up some variance that is actually due to condition. This is bad news. Here‚Äôs a nice paper that explains this issue in more detail.</p>
<p>Don‚Äôt condition on a collider. This issue is a little bit off-topic for the current post, since it‚Äôs primarily an issue in observational designs, but here‚Äôs a really good blogpost about it.</p>
<p>Conclusions</p>
<p>Covariates and confounds are some of the most basic concepts underlying experimental design and analysis in psychology, yet they are surprisingly complicated to explain. Often the issues seem clear until it comes time to do the data analysis, at which point different assumptions lead to different default analytic strategies. I‚Äôm especially concerned that these strategies vary by culture, for example with some psychologists always conditioning on confounders, and others never doing so. (We haven‚Äôt even talked about mediation and moderation!). Hopefully this post has been useful in using the vocabulary of causal models to explain some of these issues.</p>
<hr />
<ul>
<li>The definitive resource on causal graphical models is Pearl (2009). It‚Äôs not easy going, but it‚Äôs very important stuff. Even just starting to read it will strengthen your methods/stats muscles.
** Importantly, it‚Äôs a lot like adding random effects to your model ‚Äì you model sources of structure in your data so that you can better estimate the particular effects of interest.
*** The advice not to model covariates that aren‚Äôt very correlated with your outcome is very frequentist, with the idea being that you lose power when you condition on too many things. In contrast, Gelman &amp; Hill (2006) give more Bayesian advice: if you think a variable matters to your outcome, keep it in the model. This advice is consistent with the idea of modeling experimental covariates, even if they don‚Äôt have a big correlation with the outcome. In the Bayesian framework, including this extra information should (maybe only marginally) improve your precision but you aren‚Äôt ‚Äúspending degrees of freedom‚Äù in the same way.</li>
</ul>
<!-- ### Homeless -->
<!-- ‚å®Ô∏è Interactive box: non-parametric simulations where you can shuffle data across groups a bunch of times and see what kind of distribution it produces by chance -->
<!-- ::: {.interactive} -->
<!-- ‚å®Ô∏è Interactive: Nonparameteric resampling under the null -->
<!-- As we've seen above, hypothesis testing hinges on approximating the null distribution of the statistical estimate. In the examples above, it was easy to use statistical theory to work out the null distribution: for example, in Figure \@ref(fig:inference-null-model), we knew that the null distribution must be binomial since we had a binary outcome, and the binomial distribution parameter $p$ must be 0.5, because that is what the null says.  -->
<!-- But sometimes we don't know what the null distribution would look like. Suppose we want to estimate group differences in a highly skewed continuous outcome, like salary, but we had a small sample size (e.g., $n=10$ per group): -->
<!-- ```{r inference-permutation-1, eval=FALSE} -->
<!-- print(d) -->
<!-- set.seed(451) -->
<!-- n.per.group = 10 -->
<!-- d = data.frame( Y = c( rexp( n = n.per.group, rate = 4 ), -->
<!--                        rexp( n = n.per.group, rate = 4 ) + 0.3 ), -->
<!--                 Group = c( rep( "Control", n.per.group ), -->
<!--                            rep ( "Treatment", n.per.group ) ) ) -->
<!-- colors = c("black", "orange") -->
<!-- ggplot( data = d, -->
<!--         aes(x = Y, group = Group ) ) + -->
<!--   #geom_histogram( aes(fill = Group), alpha = 0.4 ) + -->
<!--   geom_dotplot( aes( fill=Group ), alpha=0.4, binwidth = .05 ) + -->
<!--   scale_y_continuous(NULL, breaks = NULL) + -->
<!--   theme_bw() + xlab("Y") + -->
<!--   ylab("")  -->
<!-- ``` -->
<!-- We can't proceed with a t-test in good conscience because, with only $n=20$, we can't necessarily trust that the Central Limit Theorem has "kicked in" sufficiently for the test to work despite the skewness. Stated otherwise, we can't be sure that the null distribution is normal in this case.  -->
<!-- When we can't rely on theory, another way to approximate a null distribution is through nonparameteric resampling. "Resampling" means that we're going to cleverly draw new samples *from our existing sample*, and "nonparametric" means that we will do this in a way that obviates assumptions about the shape of the null distribution (in contrast to parameteric approaches that do rely on such assumptions). -->
<!-- The central idea is that, if the treatment truly had no effect on the outcome, then the observations would be *exchangeable* between the treatment and control groups. That is, there would not be systematic differences between the treatment and control groups. This may or may not be true in our observed sample (after all, that's why we're doing a hypothesis test in the first place), but we could draw new samples from our existing sample in a manner that forces exchangeability. In this case, we could randomly permute the column of outcomes in our dataset while leaving the column of treatment assignments fixed: -->
<!-- ```{r inference-permutation-2, eval=FALSE} -->
<!-- # show construction of a single permuted dataset -->
<!-- # library(modelr) -->
<!-- # library(purrr) -->
<!-- perm.reps = 100 -->
<!-- perms = permute(data = d, n = perm.reps, Y) -->
<!-- # show the first permuted dataset -->
<!-- print( perms$perm[[1]]$data ) -->
<!-- #@WHY ARE THESE ALL THE SAME?? -->
<!-- cbind( perms$perm[[1]]$data$Y, perms$perm[[2]]$data$Y ) -->
<!-- # c.f. package example for debugging -->
<!-- perms2 <- permute(mtcars,  1000, mpg) -->
<!-- cbind( perms2$perm[[1]]$data$mpg, perms2$perm[[2]]$data$mpg ) -->
<!-- # also the same??  -->
<!-- # get group mean differences for each permutation via OLS -->
<!-- ols = map(perms$perm, ~ lm(Y ~ 1, data = .)) -->
<!-- ATEs = lapply( ols, function(.ols) as.numeric( coef(.ols)["(Intercept)"] ) ) -->
<!-- # now plot the ATEs to show the null sampling distribution -->
<!-- ``` -->
<!-- ::: -->

</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-haaf2017" class="csl-entry">
Haaf, J. M., &amp; Rouder, J. N. (2017). Developing constraint in bayesian mixed models. <em>Psychological Methods</em>, <em>22</em>(4), 779.
</div>
</div>
<p style="text-align: center;">
<a href="5-inference.html"><button class="btn btn-default">Previous</button></a>
<a href="7-measurement.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="toc/toc.js"></script>


</body>
</html>
