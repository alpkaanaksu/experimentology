<!DOCTYPE html>
<html>

<head>


<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 12 Preregistration | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 12 Preregistration | Experimentology">

<title>Chapter 12 Preregistration | Experimentology</title>

<script src="libs/header-attrs-2.13/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.21/datatables.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.11.3/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>



<link rel="stylesheet" type="text/css" href="/assets/index.page.c5e7dffa.css"><link rel="modulepreload" as="script" type="text/javascript" href="/assets/src/index.page.client.jsx.df9edbd0.js"><link rel="modulepreload" as="script" type="text/javascript" href="/assets/contents.b9575bfb.js"></head>

<body>




<div class="row">
<div class="col-sm-12">
<header class="_toc_1lnsy_1" id="toc"><a class="_book_title_1lnsy_24" href="/">Experimentology: An Open Science Approach to Experimental Psychology Methods</a><nav><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Preliminaries</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="1-experiments">Experiments</a><a class="_chapter_title_1lnsy_32" href="2-theories">Theories</a><a class="_chapter_title_1lnsy_32" href="3-replication">Replication and reproducibility</a><a class="_chapter_title_1lnsy_32" href="4-ethics">Ethics</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Statistics</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="5-estimation">Estimation</a><a class="_chapter_title_1lnsy_32" href="6-inference">Inference</a><a class="_chapter_title_1lnsy_32" href="7-models">Models</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Design</div><div class="_part_title_rest_1lnsy_32"> and Planning</div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="8-measurement">Measurement</a><a class="_chapter_title_1lnsy_32" href="9-design">Design of experiments</a><a class="_chapter_title_1lnsy_32" href="10-sampling">Sampling</a><a class="_chapter_title_1lnsy_32" href="11-strategy">Experimental strategy</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Execution</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="12-prereg">Preregistration</a><a class="_chapter_title_1lnsy_32" href="13-consent">Recruitment and Consent</a><a class="_chapter_title_1lnsy_32" href="14-collection">Data collection</a><a class="_chapter_title_1lnsy_32" href="15-management">Project management</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Analysis</div><div class="_part_title_rest_1lnsy_32"> and Reporting</div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="16-viz">Visualization</a><a class="_chapter_title_1lnsy_32" href="17-eda">Exploratory data analysis</a><a class="_chapter_title_1lnsy_32" href="18-writing">Writing</a><a class="_chapter_title_1lnsy_32" href="19-meta">Meta-analysis</a><a class="_chapter_title_1lnsy_32" href="20-conclusions">Conclusions</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Appendices</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="A-git">GitHub Tutorial</a><a class="_chapter_title_1lnsy_32" href="B-rmarkdown">R Markdown Tutorial</a><a class="_chapter_title_1lnsy_32" href="C-tidyverse">Tidyverse Tutorial</a><a class="_chapter_title_1lnsy_32" href="D-ggplot">ggplot Tutorial</a><a class="_chapter_title_1lnsy_32" href="E-instructors">Instructor’s guide</a></div></div></nav></header>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="prereg" class="section level1" number="12">
<h1><span class="header-section-number">Chapter 12</span> Preregistration</h1>



<div class="box learning_goals"><div class="Collapsible"><span id="collapsible-trigger-1654141393436" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1654141393436" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="apple-whole" class="svg-inline--fa fa-apple-whole " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M336 128c-32 0-80.02 16.03-112 32.03c-32.01-16-79.1-32.02-111.1-32.03C32 128 .4134 210.5 .0033 288c-.5313 99.97 63.99 224 159.1 224c32 0 48-16 64-16c16 0 32 16 64 16c96 0 160.4-122.8 159.1-224C447.7 211.6 416 128 336 128zM320 32V0h-32C243.8 0 208 35.82 208 80v32h32C284.2 112 320 76.18 320 32z"></path></svg>Learning goals<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M169.4 278.6C175.6 284.9 183.8 288 192 288s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25s-32.75-12.5-45.25 0L192 210.8L54.63 73.38c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25L169.4 278.6zM329.4 265.4L192 402.8L54.63 265.4c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25l160 160C175.6 476.9 183.8 480 192 480s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25S341.9 252.9 329.4 265.4z"></path></svg></span><div id="collapsible-content-1654141393436" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1654141393436"><div class="Collapsible__contentInner">
<ul>
<li>Recognize the dangers of researcher degrees of freedom in “the garden of forking paths”</li>
<li>Understand the differences between exploratory and confirmatory modes of research</li>
<li>Learn how preregistration and other tools can reduce risk of bias and help others to evaluate your work by increasing transparency</li>
</ul>
</div></div></div></div>
<blockquote>
<p>When not planned beforehand, data analysis can approximate a projective technique, such as the Rorschach, because the investigator can project on the data his own expectancies, desires, or biases and can pull out of the data almost any ‘finding’ he may desire.</p>
<footer>
— Theodore X. Barber <span class="citation">(<a href="#ref-barber1976" role="doc-biblioref">1976</a>)</span>
</footer>
</blockquote>
<blockquote>
<p>The first principle is that you must not fool yourself–and you are the easiest person to fool…After you’ve not fooled yourself, it’s easy not to fool other scientists. You just have to be honest in a conventional way after that.</p>
<footer>
— Richard Feynman <span class="citation">(<a href="#ref-feynman1974" role="doc-biblioref">1974</a>)</span>
</footer>
</blockquote>
<p>This may surprise you coming from the authors of a textbook about research methods, but there is no single “correct” way to design and analyze an experiment.<label for="tufte-sn-154" class="margin-toggle sidenote-number">154</label><input type="checkbox" id="tufte-sn-154" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">154</span> Though there are plenty of incorrect ways to design and analyse experiments and we hope we can help you to avoid these!</span> In fact, for most research decisions, there are many justifiable options. For example, will you stop data collection after 20, 200, or 2000 participants? Will you remove outlier values and how will you define them? Will you conduct subgroup analyses to see whether the results are affected by sex, or age, or some other factor? Consider a simplified, hypothetical case where you need to make five analysis decisions and have five justifiable options for each decision — this alone would result in 3125 (5^5) unique ways to analyze your data!</p>
<p>In this chapter, we will find out why undisclosed flexibility in the design, analysis, reporting, and interpretation of experiments (also referred to as “researcher degrees of freedom”), can lead to scientists fooling themselves and fooling each other. We will also learn about how <strong>preregistration</strong> – the process of writing down and registering your design and analysis decisions before you conduct your study – (and other tools) can be used to protect our research from bias and provide the transparency that other scientists need to properly evaluate and interpret our work.</p>
<p>Our bottom line is that the best practice is to document your experiment – including critical design, sampling, and analysis decisions – before collecting data. This documentation can help you think through your choices to ensure that they are maximally aligned with your goals. Further, the documentation can be timestamped using an external registry and shared so as to show which decisions were <em>post hoc</em> and which were made in advance.</p>
<div class="box case_study"><div class="Collapsible"><span id="collapsible-trigger-1654141393437" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1654141393437" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="microscope" class="svg-inline--fa fa-microscope " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M160 320h12v16c0 8.875 7.125 16 16 16h40c8.875 0 16-7.125 16-16V320H256c17.62 0 32-14.38 32-32V64c0-17.62-14.38-32-32-32V16C256 7.125 248.9 0 240 0h-64C167.1 0 160 7.125 160 16V32C142.4 32 128 46.38 128 64v224C128 305.6 142.4 320 160 320zM464 448h-1.25C493.2 414 512 369.2 512 320c0-105.9-86.13-192-192-192v64c70.63 0 128 57.38 128 128s-57.38 128-128 128H48C21.5 448 0 469.5 0 496C0 504.9 7.125 512 16 512h480c8.875 0 16-7.125 16-16C512 469.5 490.5 448 464 448zM104 416h208c4.375 0 8-3.625 8-8v-16c0-4.375-3.625-8-8-8h-208C99.63 384 96 387.6 96 392v16C96 412.4 99.63 416 104 416z"></path></svg>Case study<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M169.4 278.6C175.6 284.9 183.8 288 192 288s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25s-32.75-12.5-45.25 0L192 210.8L54.63 73.38c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25L169.4 278.6zM329.4 265.4L192 402.8L54.63 265.4c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25l160 160C175.6 476.9 183.8 480 192 480s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25S341.9 252.9 329.4 265.4z"></path></svg></span><div id="collapsible-content-1654141393437" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1654141393437"><div class="Collapsible__contentInner"><p class="title">Undisclosed analytic flexibility?</p>
<p></p>
<p>Educational apps for children are a huge market, but relatively few high-quality, randomized trials have been done to see whether or when they produce educational gains. In that context, <span class="citation">Berkowitz et al. (<a href="#ref-berkowitz2015" role="doc-biblioref">2015</a>)</span> reported a high-quality field experiment of educational apps, with participants randomly assigned to either a math or reading app over the course of a full school year. Critically, along with random assignment, the study also included standardized measures of math and reading achievement. These measures allowed the authors to compute effects in grade-level equivalents, a meaningful unit from a policy perspective. The key result reported by the paper is shown in Figure @(fig:prereg-berkowitz). Families who used the math app frequently showed greater gains in math than the control group.</p>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:prereg-berkowitz"></span>
<img src="images/prereg/berkowitz-1.png" alt="Figure 1 of Berkowitz et al. (2015). Estimated years of math achievement gained over the school year across groups." width="\linewidth" />
Figure 12.1: Figure 1 of Berkowitz et al. (2015). Estimated years of math achievement gained over the school year across groups.
</span>
</p>
<p>Although this finding appeared striking, something was a little odd about the main analyses, exemplified by Figure <a href="12-prereg.html#fig:prereg-berkowitz">12.1</a>. There was no way to see an estimate of the primary causal effect of interest, namely the effect of condition on math scores. Instead the data were presented as estimated effects for specific points – a “matched” subgroup in panel A and the entire group in panel B – both broken down by app usage.</p>
<p>Naively, including app usage might seem like it “clarifies” the causal effect of the app – since if you didn’t use the app, it couldn’t have any effect at all! The trouble with this logic is that usage is not causally isolated from the system, and hence it can be confounded with other factors (as discussed in Chapter <a href="1-experiments.html#experiments">1</a>)! Put another way, families that use the app extensively probably differ in other ways from samples that don’t – maybe they’re more concerned with math learning in general. So we can’t compare high-usage families and conclude that the app is useful in general.</p>
<div class="figure"><span style="display: block;" id="fig:prereg-frank-berkowitz"></span>
<p class="caption marginnote shownote">
Figure 12.2: Figure 1 of Frank (2016). Estimated years of math achievement gained over the school year across groups.
</p>
<img src="images/prereg/ITT.png" alt="Figure 1 of Frank (2016). Estimated years of math achievement gained over the school year across groups." width="\linewidth" />
</div>
<p>When the primary causal effect of interest was estimated via a simple approach, the intervention appeared to have a very weak effect [Figure <a href="12-prereg.html#fig:prereg-frank-berkowitz">12.2</a>; <span class="citation">Frank (<a href="#ref-frank2016" role="doc-biblioref">2016</a>)</span>]. Since this analysis was not favorable to the primary intervention – and because it was not reported in the paper – a reader might worry that the authors had chosen an analysis that “looked better” with respect to their hypotheses of interest. The original authors responded that their analyses were entirely based on prior research and argued that the disagreement about how the data should be analyzed was “philosophical” <span class="citation">(<a href="#ref-berkowitz2016" role="doc-biblioref">Berkowitz et al., 2016</a>)</span>.</p>
<p>Disagreements about data analysis are an important feature of the scientific discourse. The problem here is that it is very difficult to know the extent to which the original analysis was influenced by the results. If the analysis plan had been preregistered, this simple step would have reduced the risk of bias and hence increased the value of the estimates resulting from this high-quality study.<label for="tufte-sn-155" class="margin-toggle sidenote-number">155</label><input type="checkbox" id="tufte-sn-155" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">155</span> Preregistration cannot magically improve a poorly specified analysis. Some of the original analyses in the paper were straightforwardly erroneous: they concluded that there was an effect in one group but not in another by testing the effect separately in those groups and neglecting to test the interaction. This is a statistical fallacy, regardless of whether the analyses were preregistered <span class="citation">(<a href="#ref-gelman2006" role="doc-biblioref">Gelman &amp; Stern, 2006</a>)</span>. Nevertheless, preregistration would have provided transparency to enable a more informed scientific debate about the results without concern for whether they influenced by undisclosed analytic flexibility.</span></p>
</div></div></div></div>
<div id="lost-in-a-garden-of-forking-paths" class="section level2" number="12.1">
<h2><span class="header-section-number">12.1</span> Lost in a garden of forking paths</h2>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:forking-paths"></span>
<img src="images/prereg/forking-paths.png" alt="Garden of forking paths (placeholder image I hacked together, replace with an illustration?)" width="\linewidth" />
Figure 12.3: Garden of forking paths (placeholder image I hacked together, replace with an illustration?)
</span>
</p>
<p>One way to visualize researcher degrees of freedom is as a vast decision tree or “garden of forking paths” <span class="citation">(<a href="#ref-gelman2014" role="doc-biblioref">Gelman &amp; Loken, 2014</a> Figure <a href="12-prereg.html#fig:forking-paths">12.3</a>)</span>. Each node represents a decision point and each branch represents a justifiable choice. Each unique pathway through the garden terminates in an individual result.</p>
<p>Because scientific observations typically consist of both noise (random variation unique to this sample) and signal (regularities that will reoccur in other samples), some of these pathways will inevitably lead to results that are misleading (e.g., inflated effect sizes, exaggerated evidence, or false positives).<label for="tufte-sn-156" class="margin-toggle sidenote-number">156</label><input type="checkbox" id="tufte-sn-156" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">156</span> The signal-to-noise ratio is worse in situations (alas, common in psychology) that involve small effect sizes, high variation, and large measurement errors <span class="citation">(<a href="#ref-ioannidis2005" role="doc-biblioref">Ioannidis, 2005</a>)</span>. Researcher degrees of freedom may be constrained to some extent by strong theory <span class="citation">(<a href="#ref-oberauer2019" role="doc-biblioref">Oberauer &amp; Lewandowsky, 2019</a>)</span>, community methodological norms and standards, or replication studies, though these constraints may be more implicit than explicit, and can still leave plenty of room for flexible decision-making.</span> The more potential paths there are in the garden that you might explore, the higher the chance of encountering misleading results.<label for="tufte-sn-157" class="margin-toggle sidenote-number">157</label><input type="checkbox" id="tufte-sn-157" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">157</span> In frequentist terminology, there is an increasing the chance of making a “Type I error”.</span> Statisticians refer to this as a <em>multiplicity</em> problem.</p>
<p>As we talked about in Chapter <a href="6-inference.html#inference">6</a>, multiplicity can be addressed to some extent with statistical countermeasures, like the Bonferroni correction; however, these adjustment methods need to account for every path that you <em>could have</em> taken <span class="citation">(<a href="#ref-degroot2014" role="doc-biblioref">de Groot, 1956/2014</a>; <a href="#ref-gelman2014" role="doc-biblioref">Gelman &amp; Loken, 2014</a>)</span>. When you navigate the garden of forking paths <em>during data analysis</em>, it is easy to forget, or even be unaware of every path that you could have taken, so these methods can no longer be used effectively.</p>
<div id="results-dependent-analysis" class="section level3" number="12.1.1">
<h3><span class="header-section-number">12.1.1</span> Results-dependent analysis</h3>
<p>When a researcher navigates the garden of forking paths during data analysis, their decisions can be biased because they are receiving feedback on how different choices affect the results (<strong>results-dependent</strong> decision making). If a researcher is seeking a particular kind of result (which is likely – see the interaction box below), then they are more likely to follow the branches that steer them in that direction.</p>
<p>You could think of this a bit like playing a game of “hot🔥! or cold☃️!” where hot🔥! indicates that the choice will move the researcher closer to a desirable overall result and cold☃️! indicates that the choice will move them further away. Each time the researcher reaches a decision point, they try one of the branches and get feedback on how that choice affects the results. If the feedback is hot🔥! then they take that branch. If the answer is cold☃️!, they try a different branch. If they reach the end of a complete pathway, and the results are cold☃️!, maybe they even retrace their steps and try some different branches earlier in the pathway. This strategy create a risk of bias<label for="tufte-sn-158" class="margin-toggle sidenote-number">158</label><input type="checkbox" id="tufte-sn-158" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">158</span> We say “risk of bias” rather than just “bias” because in most scientific contexts, we do not have a known ground truth to compare the results to. So in any specific situation, we do not know the extent to which results-dependent analyses have actually biased the results.</span> because the results are being systematically skewed towards the researcher’s preferences<label for="tufte-sn-159" class="margin-toggle sidenote-number">159</label><input type="checkbox" id="tufte-sn-159" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">159</span> Another way to think of this is in terms of ‘regression to the mean’. When a sample statistic is selected because it crosses some threshold (e.g., statistical significance), then it is more likely to provide a biased estimate that decreases upon subsequent measurement.</span> <span class="citation">(<a href="#ref-hardwicke2021b" role="doc-biblioref">Hardwicke &amp; Wagenmakers, 2021</a>)</span>.</p>
<div class="box examples"><div class="Collapsible"><span id="collapsible-trigger-1654141393437" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1654141393437" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="list" class="svg-inline--fa fa-list " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M88 48C101.3 48 112 58.75 112 72V120C112 133.3 101.3 144 88 144H40C26.75 144 16 133.3 16 120V72C16 58.75 26.75 48 40 48H88zM480 64C497.7 64 512 78.33 512 96C512 113.7 497.7 128 480 128H192C174.3 128 160 113.7 160 96C160 78.33 174.3 64 192 64H480zM480 224C497.7 224 512 238.3 512 256C512 273.7 497.7 288 480 288H192C174.3 288 160 273.7 160 256C160 238.3 174.3 224 192 224H480zM480 384C497.7 384 512 398.3 512 416C512 433.7 497.7 448 480 448H192C174.3 448 160 433.7 160 416C160 398.3 174.3 384 192 384H480zM16 232C16 218.7 26.75 208 40 208H88C101.3 208 112 218.7 112 232V280C112 293.3 101.3 304 88 304H40C26.75 304 16 293.3 16 280V232zM88 368C101.3 368 112 378.7 112 392V440C112 453.3 101.3 464 88 464H40C26.75 464 16 453.3 16 440V392C16 378.7 26.75 368 40 368H88z"></path></svg>Examples<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M169.4 278.6C175.6 284.9 183.8 288 192 288s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25s-32.75-12.5-45.25 0L192 210.8L54.63 73.38c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25L169.4 278.6zM329.4 265.4L192 402.8L54.63 265.4c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25l160 160C175.6 476.9 183.8 480 192 480s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25S341.9 252.9 329.4 265.4z"></path></svg></span><div id="collapsible-content-1654141393437" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1654141393437"><div class="Collapsible__contentInner"><p class="title">Only human: Cognitive biases and skewed incentives</p>
<p></p>
<p>There’s a storybook image of the scientist as an objective, rationale, and dispassionate arbiter of truth <span class="citation">(<a href="#ref-veldkamp2017" role="doc-biblioref">Veldkamp et al., 2017</a>)</span>. But in reality, scientists are only human: they have egos, career ambitions, and rent to pay! So even if we do want to live up to the storybook image, its important to acknowledge that our decisions and behaviour are also influenced by a range of cognitive biases and external incentives that can steer us away from that goal. This highlights the need for transparency and intellectual humility <span class="citation">(<a href="#ref-hoekstra2020" role="doc-biblioref">Hoekstra &amp; Vazire, 2020</a>)</span> when reporting the findings of our research. Let’s first look at some relevant cognitive biases that might lead scientists astray:</p>
<p><strong>Confirmation bias</strong>: Preferentially seeking out, recalling, or evaluating information in a manner that reinforces one’s existing beliefs <span class="citation">(<a href="#ref-nickerson1998" role="doc-biblioref">Nickerson, 1998</a>)</span>.</p>
<p><strong>Hindsight bias</strong>: Believing that past events were always more likely to occur relative to our actual belief in their likelihood before they happened (“I knew it all along!”) <span class="citation">(<a href="#ref-slovic1977" role="doc-biblioref">Slovic &amp; Fischhoff, 1977</a>)</span>.</p>
<p><strong>Motivated reasoning</strong>: Rationalizing prior decisions so they are framed in a favorable light, even if they were irrational <span class="citation">(<a href="#ref-kunda1990" role="doc-biblioref">Kunda, 1990</a>)</span>.</p>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:apophenia"></span>
<img src="images/prereg/apophenia.png" alt="Examples of apophenia: Mars Face, Winnie the Pooh Cloud, and Jesus Toast." width="\linewidth" />
Figure 12.4: Examples of apophenia: Mars Face, Winnie the Pooh Cloud, and Jesus Toast.
</span>
</p>
<p><strong>Apophenia</strong>: Detecting seemingly meaningful patterns in noise (Figure <a href="12-prereg.html#fig:apophenia">12.4</a>) <span class="citation">(<a href="#ref-gilovich1985" role="doc-biblioref">Gilovich et al., 1985</a>)</span>.</p>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:chrysalis"></span>
<img src="images/prereg/chrysalis.png" alt="The Chrysalis Effect, when ugly truth becomes a beautiful fiction." width="\linewidth" />
Figure 12.5: The Chrysalis Effect, when ugly truth becomes a beautiful fiction.
</span>
</p>
<p>To make matters worse, the incentive structure of the scientific ecosystem often adds additional motivation to get things wrong. The allocation of funding, awards, and publication prestige is often based on the nature of research results rather than research quality <span class="citation">(<a href="#ref-nosek2012" role="doc-biblioref">B. A. Nosek et al., 2012</a>; <a href="#ref-smaldino2016" role="doc-biblioref">Paul E. Smaldino &amp; McElreath, 2016</a>)</span>. For example, many academic journals, especially those that are widely considered to be the most prestigious, appear to have a preference for novel, positive, and ‘statistically significant’ findings over incremental, negative, or null findings <span class="citation">(<a href="#ref-bakker2012" role="doc-biblioref">Bakker et al., 2012</a>)</span>. There is also pressure to write articles with concise, coherent, and compelling narratives <span class="citation">(<a href="#ref-giner-sorolla2012" role="doc-biblioref">Giner-Sorolla, 2012</a>)</span>. This set of forces incentivize scientists to be “impressive” over being right and encourages questionable research practices. The process of iteratively p-hacking and HARKing one’s way to a “beautiful” scientific paper has been dubbed “The Chrysalis Effect” <span class="citation">(<a href="#ref-oboyle2017" role="doc-biblioref">O’Boyle et al., 2017</a>)</span>, as shown in Figure <a href="12-prereg.html#fig:chrysalis">12.5</a>.</p>
</div></div></div></div>
<p>In the most egregious cases, a researcher may try multiple pathways until they obtain a desirable result<label for="tufte-sn-160" class="margin-toggle sidenote-number">160</label><input type="checkbox" id="tufte-sn-160" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">160</span> “If you torture the data long enough, it will confess” <span class="citation">(<a href="#ref-good1972" role="doc-biblioref">Good, 1972</a>)</span></span> and then <strong>selectively report</strong> that result, neglecting to mention that they have tried several other analysis strategies. You may remember an example of this type when participants apparently became younger when they listened to “When I’m 64” by The Beatles in Chapter <a href="4-ethics.html#ethics">4</a>. Another nice example of how damaging the garden of forking paths can be comes from the “discovery” of brain activity in a dead Atlantic Salmon! Researchers deliberately exploited flexibility in the fMRI analysis pipeline to find brain activity where there was only dead fish <span class="citation">(Figure <a href="12-prereg.html#fig:salmon">12.6</a>, <a href="#ref-bennett2009" role="doc-biblioref">Bennett et al., 2009</a>)</span>.</p>
<p>Deliberately taking advantage of researcher degrees of freedom and selectively reporting results is known by various names, like p-hacking, cherry picking, data dredging, and it is unethical because it involves hiding highly relevant information. But you should also be aware that results-dependent analysis incurs a risk of bias even if a researcher has good intentions, doesn’t explicitly try multiple pathways, and honestly reports everything they did.</p>
<p>If a researcher examines a dataset and takes a set of hot🔥 paths, they may reach the result they desire at the end of the pathway without realizing that – had the results been different – they would have followed other pathways <span class="citation">(<a href="#ref-degroot2014" role="doc-biblioref">de Groot, 1956/2014</a>; <a href="#ref-gelman2014" role="doc-biblioref">Gelman &amp; Loken, 2014</a>)</span>. Even though the researcher didn’t intend to hide anything, there is still undisclosed analytic flexibility – important context that is relevant to interpret the results properly. It’s surprisingly easy to convince yourself after the fact that you made the decisions you did for principled reasons that had nothing to do with the results (see “motivated reasoning” box). Results-dependent analysis increases the chances that you will fool yourself by inadvertently stumbling across misleading results. If that analytic flexibility goes undisclosed, you may fool others too.</p>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:salmon"></span>
<img src="images/prereg/salmon.jpeg" alt="By deliberately exploiting analytic flexibility in the processing pipeline of fMRI data, Bennet et al. (2009) were able to identify 'brain activity' in a dead Atlantic Salmon." width="\linewidth" />
Figure 12.6: By deliberately exploiting analytic flexibility in the processing pipeline of fMRI data, Bennet et al. (2009) were able to identify ‘brain activity’ in a dead Atlantic Salmon.
</span>
</p>
</div>
<div id="hypothesize-after-results-are-known" class="section level3" number="12.1.2">
<h3><span class="header-section-number">12.1.2</span> Hypothesize after results are known?</h3>
<p>In addition to flexibility in analysis, there is additional flexibility in how researchers <em>explain</em> research results. As we discussed in Chapter <a href="2-theories.html#theories">2</a>, theories can accommodate even conflicting results in many different ways – for example, by positing auxilliary hypotheses that explain why a particular datapoint is special. We might call these different routes for accommodating theory with data “explanatory degrees of freedom”.</p>
<p>The practice of selecting or developing your hypothesis after seeing the study results has been called “Hypotheisizing After the Results are Known”, or “HARKing” <span class="citation">(<a href="#ref-kerr1998" role="doc-biblioref">Kerr, 1998</a>)</span>. HARKing is potentially problematic because it expands the garden of forking paths and helps to justify the use of various analytic degrees of freedom (Figure <a href="12-prereg.html#fig:grid">12.7</a>). For example, you may come up with an explanation for why an intervention is effective in men but not in women in order to justify a post-hoc subgroup analysis based on sex (see Case Study). The extent to which HARKing is problematic is contested <span class="citation">(for discussion see <a href="#ref-hardwicke2021b" role="doc-biblioref">Hardwicke &amp; Wagenmakers, 2021</a>)</span>. But at the very least it’s important to be honest about whether hypotheses were developed before or after seeing research results.</p>
<div class="figure"><span style="display: block;" id="fig:grid"></span>
<p class="caption marginnote shownote">
Figure 12.7: A scientist exploring a grid of individual research results. The horizontal axis illustrates a simplified ‘garden of forking paths’: the many justifiable analysis specifications that the scientist can use to transform the data (D) into the evidence (E). The vertical axis illustrates that there may be several relevant theories (T), and hypotheses (H) derived from those theories, which could be constructed or selected and then confronted with the evidence. Thus, an unconstrained scientist can simultaneously exploit their analytic degrees of freedom and explanatory degrees of freedom to fit evidence to hypotheses and fit hypotheses to evidence in order to arrive at a study outcome that is more likely to align more with their preferences, but less likely to align with the truth. Caption is copied verbatim so needs editing. Shared under a CC-BY license, artwork by Viktor Beekman, concept by Tom Hardwicke and Eric-Jan Wagenmakers.
</p>
<img src="images/prereg/grid.jpg" alt="A scientist exploring a grid of individual research results. The horizontal axis illustrates a simplified ‘garden of forking paths’: the many justifiable analysis specifications that the scientist can use to transform the data (D) into the evidence (E). The vertical axis illustrates that there may be several relevant theories (T), and hypotheses (H) derived from those theories, which could be constructed or selected and then confronted with the evidence. Thus, an unconstrained scientist can simultaneously exploit their analytic degrees of freedom and explanatory degrees of freedom to fit evidence to hypotheses and fit hypotheses to evidence in order to arrive at a study outcome that is more likely to align more with their preferences, but less likely to align with the truth. Caption is copied verbatim so needs editing. Shared under a CC-BY license, artwork by Viktor Beekman, concept by Tom Hardwicke and Eric-Jan Wagenmakers." width="\linewidth" />
</div>
<p>But hang on a minute! Isn’t it a good thing to seek out interesting results if they are there in the data? Shouldn’t we “let the data speak”? The answer is yes! Exploratory research is <em>not</em> the same as p-hacking. P-hacking is explicitly dishonest because it involves deliberately withholding information. In contrast, exploratory data analysis is a critical part of the scientific process (see Chapter <a href="17-eda.html#eda">17</a> for further discussion).</p>

<p>The important thing to remember about exploratory research is that you need to (a) be aware of the increased risk of bias and calibrate your confidence in the results accordingly; (2) be honest with other researchers about your analysis strategy so they are also aware of the risk of bias and can calibrate <em>their</em> confidence in the results accordingly. It’s important to understand the distinction between <strong>exploratory</strong> and <strong>confirmatory</strong> research modes.<label for="tufte-sn-161" class="margin-toggle sidenote-number">161</label><input type="checkbox" id="tufte-sn-161" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">161</span> In practice, an individual study may contain both exploratory and confirmatory aspects which is why we describe them as different “modes.”</span> Confirmatory research involves making design and analysis decisions <em>before</em> the results have been observed. In the next section, we will learn about how to do that using preregistration.</p>
</div>
</div>
<div id="reducing-bias-increasing-transparency-and-calibrating-confidence-with-preregistration" class="section level2" number="12.2">
<h2><span class="header-section-number">12.2</span> Reducing bias, increasing transparency, and calibrating confidence with preregistration</h2>
<p>If you make research decisions before you see the study results, you can counter the problem of undisclosed researcher degrees of freedom outlined above. That way, your decisions are made before seeing the study results – a bit like planning your route through the garden of forking paths before you start your journey. <strong>Preregistration</strong> is a great way to do this <span class="citation">(<a href="#ref-hardwicke2021b" role="doc-biblioref">Hardwicke &amp; Wagenmakers, 2021</a>; <a href="#ref-wagenmakers2012" role="doc-biblioref">Wagenmakers et al., 2012</a>)</span>.</p>
<p>Preregistration is the process of declaring your research decisions in an public registry before you analyze (and often before you collect) the data. Preregistration ensures that your decisions are results-independent, which reduces risk of bias arising from the issues described above. Preregistration also transparently conveys to others what you planned, helping them to determine the risk of bias and calibrate their confidence in the results. In other words, preregistration transparently provides the context needed to properly evaluate and interpret research. In theory, preregistration dissuades researchers from engaging in questionable research practices like p-hacking and undisclosed HARKing, because they can be held accountable to their original plan.</p>
<p>Preregistration does not require that you specify all research decisions in advance, only that you are transparent about what was planned, and what was not planned. This division helps to make a distinction between which aspects of the research were exploratory and which were confirmatory (Figure <a href="12-prereg.html#fig:continuum">12.8</a>). All else being equal, we should have more confidence in confirmatory findings, because there is a lower risk of bias. Exploratory analyses have a higher risk of bias, but they are also more <strong>sensitive</strong> to serendipitous (unexpected) discoveries. Exploratory and confirmatory research are both valuable activities – it is just important to differentiate them <span class="citation">(<a href="#ref-tukey1980" role="doc-biblioref">John W. Tukey, 1980</a>)</span>! Preregistration offers the best of both worlds by clearly separating one from the other.</p>
<div class="figure"><span style="display: block;" id="fig:continuum"></span>
<p class="caption marginnote shownote">
Figure 12.8: Preregistration clarifies where aspects of your research fall on a spectrum of exploratory and confirmatory modes of research. A preregistration is just a snapshot of your current thinking. If you have planned very little, your preregistration may not have much detail, but that’s absolutely fine! The important thing is that preregistration transparently conveys what was planned (confirmatory) and what was not (exploratory). Increasing the amount of detail in your preregistration increases your protection against bias.
</p>
<img src="images/prereg/continuum.png" alt="Preregistration clarifies where aspects of your research fall on a spectrum of exploratory and confirmatory modes of research. A preregistration is just a snapshot of your current thinking. If you have planned very little, your preregistration may not have much detail, but that's absolutely fine! The important thing is that preregistration transparently conveys what was planned (confirmatory) and what was not (exploratory). Increasing the amount of detail in your preregistration increases your protection against bias." width="\linewidth" />
</div>
<p>In addition to the benefits described above, preregistration may improve the quality of research by encouraging closer attention to study planning. We’ve found that the process of writing a preregistration really helps facilitate communication between collaborators, and can catch addressable problems before time and resources are wasted on a poorly designed study. Detailed advanced planning can also create opportunities for useful community feedback, particularly in the context of Registered Reports, where dedicated peer reviewers will evaluate your study before its even begun (Box 2).</p>
<div class="box examples"><div class="Collapsible"><span id="collapsible-trigger-1654141393437" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1654141393437" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="list" class="svg-inline--fa fa-list " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M88 48C101.3 48 112 58.75 112 72V120C112 133.3 101.3 144 88 144H40C26.75 144 16 133.3 16 120V72C16 58.75 26.75 48 40 48H88zM480 64C497.7 64 512 78.33 512 96C512 113.7 497.7 128 480 128H192C174.3 128 160 113.7 160 96C160 78.33 174.3 64 192 64H480zM480 224C497.7 224 512 238.3 512 256C512 273.7 497.7 288 480 288H192C174.3 288 160 273.7 160 256C160 238.3 174.3 224 192 224H480zM480 384C497.7 384 512 398.3 512 416C512 433.7 497.7 448 480 448H192C174.3 448 160 433.7 160 416C160 398.3 174.3 384 192 384H480zM16 232C16 218.7 26.75 208 40 208H88C101.3 208 112 218.7 112 232V280C112 293.3 101.3 304 88 304H40C26.75 304 16 293.3 16 280V232zM88 368C101.3 368 112 378.7 112 392V440C112 453.3 101.3 464 88 464H40C26.75 464 16 453.3 16 440V392C16 378.7 26.75 368 40 368H88z"></path></svg>Examples<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M169.4 278.6C175.6 284.9 183.8 288 192 288s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25s-32.75-12.5-45.25 0L192 210.8L54.63 73.38c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25L169.4 278.6zM329.4 265.4L192 402.8L54.63 265.4c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25l160 160C175.6 476.9 183.8 480 192 480s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25S341.9 252.9 329.4 265.4z"></path></svg></span><div id="collapsible-content-1654141393437" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1654141393437"><div class="Collapsible__contentInner"><p class="title">Preregistration and friends: A toolbox to address researcher degrees of freedom</p>
<p></p>
<p>Several useful tools and concepts that can be used to complement or extend preregistration. In general, we would recommend that these are combined with preregistration, rather than used as a replacement. Preregistration provides transparency about the research and planning process, so its function complements other methods for avoiding bias <span class="citation">(<a href="#ref-hardwicke2021b" role="doc-biblioref">Hardwicke &amp; Wagenmakers, 2021</a>)</span>.</p>
<p><strong>Robustness checks</strong>. Robustness checks (also called “sensitivity analyses”) assess how different decision choices in the garden of forking paths affect the eventual pattern of results. This technique is particularly helpful when you have to choose between several justifiable analytic choices, neither of which seem superior to the other, or which have complementary strengths and weaknesses. For example, you might run the analysis three times using three different methods for handling missing data. Robust results should not vary substantially across the three different choices.</p>
<p><strong>Multiverse analyses</strong>. Recently, some researchers have started running large-scale robustness checks. These have been called “multiverse analysis” <span class="citation">(<a href="#ref-steegen2016" role="doc-biblioref">Steegen et al., 2016</a>)</span> or “specification curve analysis” <span class="citation">(<a href="#ref-simonsohn2020" role="doc-biblioref">Simonsohn et al., 2020</a>)</span>. These techniques evaluate the factorial intersection of multiple choices for multiple decisions – like simultaneously evaluating thousands of pathways in the garden of forking paths. Some have argued that these large-scale robustness checks can make preregistration redundant; after all, why prespecify a single path if you can explore them all <span class="citation">(<a href="#ref-oberauer2019" role="doc-biblioref">Oberauer &amp; Lewandowsky, 2019</a>; <a href="#ref-rubin2020" role="doc-biblioref">Rubin, 2020</a>)</span>? But interpreting the results of a multiverse analysis are not straightforward; for example, it seems unlikely that all of the decision choices are equally justifiable <span class="citation">(<a href="#ref-giudice2021" role="doc-biblioref">Giudice &amp; Gangestad, 2021</a>)</span>. Furthermore, if robustness checks are not preregistered, then they introduce researcher degrees of freedom, and create an opportunity for selective reporting, which increases risk of bias.</p>
<p><strong>Held-out sample</strong>. One option to benefit from both exploratory and confirmatory research modes is to split your data into <strong>training</strong> and <strong>test</strong> samples. (The test sample is commonly called the “held out” because it is “held out” from the exploratory process.) You can generate hypotheses in an exploratory mode in the training sample and use that as the basis to preregister confirmatory analyses in the hold-out sample. A notable disadvantage of this strategy is that splitting the data reduces statistical power, but in cases where data are plentiful – including in much of machine learning – this technique is the gold standard.</p>
<p><strong>Masked analysis</strong> (sometimes also called “blind analysis”). Sometimes problems, such as missing data, attrition, or randomization failure can arise during data collection that you did not anticipate in your preregistered plan. How do you diagnose and address these issues without increasing risk of bias through results-dependent analysis? One option is masked analysis, which disguises aspects of the data related to the results (for example, by shuffling condition labels or adding noise) while still allowing some degree of data inspection <span class="citation">(<a href="#ref-dutilh2019" role="doc-biblioref">Dutilh et al., 2019</a>)</span>. After diagnosing a problem, you can adjust your preregistered plan without increasing risk of bias, because you have not engaged in results-dependent decision making.</p>
<p><strong>Standard Operating Procedures</strong>. Community norms, perhaps at the level of your research field or lab, can act as a natural constraint on researcher degrees of freedom. For example, there may be a generally accepted approach for handling outliers in your community. You can make these constraints explicit by writing them down in a Standard Operating Procedures document - a bit like a living meta-preregistration <span class="citation">(<a href="#ref-lin2016" role="doc-biblioref">Lin &amp; Green, 2016</a>)</span>. Each time you preregister an individual study, you can co-register this document alongside it. Make sure you are clear about which document you will follow in the event of a mismatch!</p>
<p><strong>Open lab notebooks</strong>. Maintaining a lab notebook can be a useful way to keep a record of your decisions as a research project unfolds. Preregistration is bit like taking a snapshot of your lab notebook at the start of the project, when all you have written down is your research plan. Making your lab notebook publicly available is a great way to transparently document your research and departures from the preregistered plan.</p>
<p><strong>Registered Reports</strong>. Registered Reports are a type of article format that embeds preregistration directly into the publication pipeline [<span class="citation">Chambers &amp; Tzavella (<a href="#ref-chambers2020" role="doc-biblioref">2020</a>)</span>; Figure <a href="12-prereg.html#fig:reg-reports">12.9</a>]. The idea is that you submit your preregistered protocol to a journal and it is peer reviewed, before you’ve even started your study. If the study is approved, the journal agrees to publish it, regardless of the results. This is a radical departure from traditional publication models where peer reviewers and journals evaluate your study <em>after</em> its been completed and the results are known. Because the study is accepted for publication independently of the results, Registered Reports can offer the benefits of preregistration with additional protection against publication bias. They also provide a great opportunity to feedback on your study design while you can still change it!</p>
</div></div></div></div>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:reg-reports"></span>
<img src="images/prereg/registered-reports.png" alt="Registered Reports (https://www.cos.io/initiatives/registered-reports)" width="\linewidth" />
Figure 12.9: Registered Reports (<a href="https://www.cos.io/initiatives/registered-reports" class="uri">https://www.cos.io/initiatives/registered-reports</a>)
</span>
</p>
</div>
<div id="how-to-preregister" class="section level2" number="12.3">
<h2><span class="header-section-number">12.3</span> How to preregister</h2>
<p>High-stakes studies such as medical trials must be preregistered <span class="citation">(<a href="#ref-dickersin2012" role="doc-biblioref">Dickersin &amp; Rennie, 2012</a>)</span>. In 2005, a large international consortium of medical journals decided that they would not publish unregistered trials. And the discipline of economics has strong norms about study registration (see e.g. <a href>https://www.socialscienceregistry.org</a>). But preregistration is actually pretty new to psychology <span class="citation">(<a href="#ref-nosek2018" role="doc-biblioref">B. A. Nosek et al., 2018</a>)</span>, and there’s still no standard way of doing it – you’re already at the cutting edge!</p>
<p>We recommend using the Open Science Framework (OSF) as your registry. OSF is one of the most popular registries in psychology and you can do lots of other useful things there to make your research transparent, like sharing data, materials, analysis scripts, and preprints. On the OSF it is possible to “register” any file you have uploaded. When you register a file, it creates a timestamped, read-only copy, with a dedicated link. You can add this link to articles reporting your research.</p>
<p>One approach to preregistration is to write a protocol document that specifies the study rationale, aims or hypotheses, methods, and analysis plan, and register that document.<label for="tufte-sn-162" class="margin-toggle sidenote-number">162</label><input type="checkbox" id="tufte-sn-162" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">162</span> You can think of a study protocol a bit like a research paper without a results and discussion section (here’s an example from one of our own studies: <a href="https://osf.io/2cnkq/" class="uri">https://osf.io/2cnkq/</a>).</span> The OSF also has a collection of dedicated preregistration templates that you can use if you prefer. These templates are often tailored to the needs of particular types of research. For example, there are templates for general quantitative psychology research <span class="citation">(“PRP-QUANT” <a href="#ref-bosnjak2021" role="doc-biblioref">Bosnjak et al., 2021</a>)</span>, cognitive modelling <span class="citation">(<a href="#ref-cruwell2021" role="doc-biblioref">Crüwell &amp; Evans, 2021</a>)</span>, and secondary data analysis <span class="citation">(<a href="#ref-akker2019" role="doc-biblioref">Akker et al., 2019</a>)</span>. The OSF interface may change, but currently <a href="https://help.osf.io/hc/en-us/articles/360019738834-Create-a-Preregistration">this guide</a> provides a set of steps to create a preregistration.</p>
<p>Once you’ve preregistered your plan, you just go off and run the study and report the results, right? Well hopefully… but things might not turn out to be that straightforward. It’s quite common to forget to include something in your plan or to have to depart from the plan due to something unexpected. Preregistration can actually be pretty hard in practice <span class="citation">(<a href="#ref-nosek2019" role="doc-biblioref">B. A. Nosek et al., 2019</a>)</span>!</p>
<p>Don’t worry though - remember that the primary goal of preregistration is transparency to enable others to evaluate and interpret our work. If you decide to depart from your original plan and conduct results-dependent analyses, then this decision may increase the risk of bias. But if you communicate this decision transparently to your readers, they can appropriately calibrate their confidence in the results. You may even be able to run both the planned and unplanned analyses as a robustness check (see Box) to evaluate the extent to which this choice impacts the results.</p>
<p>When you report your study, it is important to distinguish between what was planned and what was not. If you ran a lot of results-dependent analyses, then it might be worth having separate exploratory and confirmatory results sections. On the other hand, if you mainly stuck to your original plan, with only minor departures, then you could include a table (perhaps in an appendix) that outlines these changes (for example, see Supplementary Information A of <a href="https://doi.org/10.31222/osf.io/wt5ny">this article</a>).</p>
</div>
<div id="chapter-summary-preregistration" class="section level2" number="12.4">
<h2><span class="header-section-number">12.4</span> Chapter summary: Preregistration</h2>
<p>We’ve advocated here for preregistering your planned analyses. This practice allows us to minimize bias caused by results-dependent analysis (the “garden of forking paths” that we described). Preregistration is a “<a href="https://www.cos.io/blog/preregistration-plan-not-prison">plan, not a prison</a>”: in most cases preregistered, confirmatory analyses coexist with exploratory analyses. Both are an important part of good research – the key is simply to disclose which is which!</p>
<div class="box exercises"><div class="Collapsible"><span id="collapsible-trigger-1654141393437" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1654141393437" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="pen-ruler" class="svg-inline--fa fa-pen-ruler " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M492.7 42.75C517.7 67.74 517.7 108.3 492.7 133.3L436.3 189.7L322.3 75.72L378.7 19.32C403.7-5.678 444.3-5.678 469.3 19.32L492.7 42.75zM44.89 353.2L299.7 98.34L413.7 212.3L158.8 467.1C152.1 473.8 143.8 478.7 134.6 481.4L30.59 511.1C22.21 513.5 13.19 511.1 7.03 504.1C.8669 498.8-1.47 489.8 .9242 481.4L30.65 377.4C33.26 368.2 38.16 359.9 44.89 353.2zM249.4 103.4L103.4 249.4L16 161.9C-2.745 143.2-2.745 112.8 16 94.06L94.06 16C112.8-2.745 143.2-2.745 161.9 16L181.7 35.76C181.4 36.05 181 36.36 180.7 36.69L116.7 100.7C110.4 106.9 110.4 117.1 116.7 123.3C122.9 129.6 133.1 129.6 139.3 123.3L203.3 59.31C203.6 58.99 203.1 58.65 204.2 58.3L249.4 103.4zM453.7 307.8C453.4 308 453 308.4 452.7 308.7L388.7 372.7C382.4 378.9 382.4 389.1 388.7 395.3C394.9 401.6 405.1 401.6 411.3 395.3L475.3 331.3C475.6 330.1 475.1 330.6 476.2 330.3L496 350.1C514.7 368.8 514.7 399.2 496 417.9L417.9 496C399.2 514.7 368.8 514.7 350.1 496L262.6 408.6L408.6 262.6L453.7 307.8z"></path></svg>Exercises<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M169.4 278.6C175.6 284.9 183.8 288 192 288s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25s-32.75-12.5-45.25 0L192 210.8L54.63 73.38c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25L169.4 278.6zM329.4 265.4L192 402.8L54.63 265.4c-12.5-12.5-32.75-12.5-45.25 0s-12.5 32.75 0 45.25l160 160C175.6 476.9 183.8 480 192 480s16.38-3.125 22.62-9.375l160-160c12.5-12.5 12.5-32.75 0-45.25S341.9 252.9 329.4 265.4z"></path></svg></span><div id="collapsible-content-1654141393437" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1654141393437"><div class="Collapsible__contentInner">
<ol style="list-style-type: decimal;">
<li><p>P-hack your way to scientific glory! To get a feel for how results-dependent analyses might work in practice, have a play around with this app: <a href="https://projects.fivethirtyeight.com/p-hacking/" class="uri">https://projects.fivethirtyeight.com/p-hacking/</a></p></li>
<li><p>Preregister your next experiment! The best way to get started with preregistration is to have a go with your next study. Head over to <a href="https://osf.io/registries/osf/new" class="uri">https://osf.io/registries/osf/new</a> and register your study protocol or complete one of the templates.</p></li>
</ol>
</div></div></div></div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-akker2019" class="csl-entry">
Akker, O. van den, Weston, S. J., Campbell, L., Chopik, W. J., Damian, R. I., Davis-Kean, P., Hall, A., Kosie, J., Kruse, E., Olsen, J., Ritchie, S. J., Valentine, K. D., Veer, A. van ’t., &amp; Bakker, M. (2019). <em>Preregistration of secondary data analysis: <span>A</span> template and tutorial</em>. PsyArXiv. <a href="https://psyarxiv.com/hvfmr/">https://psyarxiv.com/hvfmr/</a>
</div>
<div id="ref-bakker2012" class="csl-entry">
Bakker, M., Dijk, A. van, &amp; Wicherts, J. M. (2012). The rules of the game called psychological science. <em>Perspectives on Psychological Science</em>, <em>7</em>(6), 543–554. <a href="https://doi.org/10.1177/1745691612459060">https://doi.org/10.1177/1745691612459060</a>
</div>
<div id="ref-barber1976" class="csl-entry">
Barber, T. X. (1976). <em>Pitfalls in <span>Human</span> <span>Research</span>: <span>Ten</span> <span>Pivotal</span> <span>Points</span></em>. Pergamon Press.
</div>
<div id="ref-bennett2009" class="csl-entry">
Bennett, C., Miller, M., &amp; Wolford, G. (2009). Neural correlates of interspecies perspective taking in the post-mortem <span>Atlantic</span> <span>Salmon</span>: An argument for multiple comparisons correction. <em>NeuroImage</em>, <em>47</em>, S125. <a href="https://doi.org/10.1016/S1053-8119(09)71202-9">https://doi.org/10.1016/S1053-8119(09)71202-9</a>
</div>
<div id="ref-berkowitz2015" class="csl-entry">
Berkowitz, T., Schaeffer, M. W., Maloney, E. A., Peterson, L., Gregor, C., Levine, S. C., &amp; Beilock, S. L. (2015). Math at home adds up to achievement in school. <em>Science</em>, <em>350</em>(6257), 196–198. <a href="https://doi.org/10.1126/science.aac7427">https://doi.org/10.1126/science.aac7427</a>
</div>
<div id="ref-berkowitz2016" class="csl-entry">
Berkowitz, T., Schaeffer, M. W., Rozek, C. S., Maloney, E. A., Levine, S. C., &amp; Beilock, S. L. (2016). Response to comment on <span>“math at home adds up to achievement in school.”</span> <em>Science</em>, <em>351</em>(6278), 1161–1161.
</div>
<div id="ref-bosnjak2021" class="csl-entry">
Bosnjak, M., Fiebach, C., Mellor, D. T., Mueller, S., O’Connor, D., Oswald, F., &amp; Sokol-Chang, R. (2021). <em>A template for preregistration of quantitative research in psychology: Report of the joint psychological societies preregistration task force</em>. PsyArXiv. <a href="https://doi.org/10.31234/osf.io/d7m5r">https://doi.org/10.31234/osf.io/d7m5r</a>
</div>
<div id="ref-chambers2020" class="csl-entry">
Chambers, C., &amp; Tzavella, L. (2020). <em>Registered <span>Reports</span>: Past, present and future</em>. MetaArXiv. <a href="https://doi.org/10.31222/osf.io/43298">https://doi.org/10.31222/osf.io/43298</a>
</div>
<div id="ref-cruwell2021" class="csl-entry">
Crüwell, S., &amp; Evans, N. J. (2021). Preregistration in diverse contexts: A preregistration template for the application of cognitive models. <em>Royal Society Open Science</em>, <em>8</em>(10), 210155. <a href="https://doi.org/10.1098/rsos.210155">https://doi.org/10.1098/rsos.210155</a>
</div>
<div id="ref-degroot2014" class="csl-entry">
de Groot, A. D. (1956/2014). The meaning of <span>“significance”</span> for different types of research (E.-J. Wagenmakers, D. Borsboom, J. Verhagen, R. A. Kievit, M. Bakker, A. O. J. Cramer, D. Matzke, D. Mellenbergh, &amp; H. L. J. van der Maas, Trans.). <em>Acta Psychologica</em>, <em>148</em>, 188–194. <a href="https://doi.org/10.1016/j.actpsy.2014.02.001">https://doi.org/10.1016/j.actpsy.2014.02.001</a>
</div>
<div id="ref-dickersin2012" class="csl-entry">
Dickersin, K., &amp; Rennie, D. (2012). The evolution of trial registries and their use to assess the clinical trial enterprise. <em>JAMA</em>, <em>307</em>(17), 1861–1864. <a href="https://doi.org/10.1001/jama.2012.4230">https://doi.org/10.1001/jama.2012.4230</a>
</div>
<div id="ref-dutilh2019" class="csl-entry">
Dutilh, G., Sarafoglou, A., &amp; Wagenmakers, E.-J. (2019). Flexible yet fair: Blinding analyses in experimental psychology. <em>Synthese</em>. https://doi.org/<a href="https://doi.org/10.1007/s11229-019-02456-7">https://doi.org/10.1007/s11229-019-02456-7</a>
</div>
<div id="ref-feynman1974" class="csl-entry">
Feynman, R. P. (1974). <em>Cargo <span>Cult</span> <span>Science</span></em>. <a href="http://calteches.library.caltech.edu/51/2/CargoCult.pdf">http://calteches.library.caltech.edu/51/2/CargoCult.pdf</a>
</div>
<div id="ref-frank2016" class="csl-entry">
Frank, M. C. (2016). Comment on <span>“math at home adds up to achievement in school.”</span> In <em>Science</em> (No. 6278; Vol. 351, pp. 1161.2–1161).
</div>
<div id="ref-gelman2014" class="csl-entry">
Gelman, A., &amp; Loken, E. (2014). The statistical crisis in science. <em>American Scientist</em>, <em>102</em>(6), 460–465. <a href="https://doi.org/10.1511/2014.111.460">https://doi.org/10.1511/2014.111.460</a>
</div>
<div id="ref-gelman2006" class="csl-entry">
Gelman, A., &amp; Stern, H. (2006). The <span>Difference</span> <span>Between</span> <span>“<span>Significant</span>”</span> and <span>“<span>Not</span> <span>Significant</span>”</span> is not <span>Itself</span> <span>Statistically</span> <span>Significant</span>. <em>The American Statistician</em>, <em>60</em>(4), 328–331. <a href="https://doi.org/10.1198/000313006X152649">https://doi.org/10.1198/000313006X152649</a>
</div>
<div id="ref-gilovich1985" class="csl-entry">
Gilovich, T., Vallone, R., &amp; Tversky, A. (1985). The hot hand in basketball: <span>On</span> the misperception of random sequences. <em>Cognitive Psychology</em>, <em>17</em>(3), 295–314. <a href="https://doi.org/10.1016/0010-0285(85)90010-6">https://doi.org/10.1016/0010-0285(85)90010-6</a>
</div>
<div id="ref-giner-sorolla2012" class="csl-entry">
Giner-Sorolla, R. (2012). Science or art? <span>How</span> aesthetic standards grease the way through the publication bottleneck but undermine science. <em>Perspectives on Psychological Science</em>, <em>7</em>(6), 562–571. <a href="https://doi.org/10.1177/1745691612457576">https://doi.org/10.1177/1745691612457576</a>
</div>
<div id="ref-giudice2021" class="csl-entry">
Giudice, M. D., &amp; Gangestad, S. (2021). A traveler’s guide to the multiverse: Promises, pitfalls, and a framework for the evaluation of analytic decisions. <em>Advances in Methods and Practices in Psychological Science</em>, <em>4</em>(1), 1–15. https://doi.org/<a href="https://doi.org/10.1177/2515245920954925">https://doi.org/10.1177/2515245920954925</a>
</div>
<div id="ref-good1972" class="csl-entry">
Good, I. J. (1972). Statistics and <span>Today</span>’s <span>Problems</span>. <em>The American Statistician</em>, <em>26</em>(3), 11–19. <a href="https://doi.org/10.1080/00031305.1972.10478922">https://doi.org/10.1080/00031305.1972.10478922</a>
</div>
<div id="ref-hardwicke2021b" class="csl-entry">
Hardwicke, T. E., &amp; Wagenmakers, E.-J. (2021). <em>Preregistration: <span>A</span> pragmatic tool to reduce bias and calibrate confidence in scientific research</em>. MetaArXiv. <a href="https://doi.org/10.31222/osf.io/d7bcu">https://doi.org/10.31222/osf.io/d7bcu</a>
</div>
<div id="ref-hoekstra2020" class="csl-entry">
Hoekstra, R., &amp; Vazire, S. (2020). <em>Intellectual humility is central to science</em> [Preprint]. <a href="https://osf.io/edh2s">https://osf.io/edh2s</a>
</div>
<div id="ref-ioannidis2005" class="csl-entry">
Ioannidis, J. P. A. (2005). Why most published research findings are false. <em>PLOS Medicine</em>, <em>2</em>(8), e124. <a href="https://doi.org/10.1371/journal.pmed.0020124">https://doi.org/10.1371/journal.pmed.0020124</a>
</div>
<div id="ref-kerr1998" class="csl-entry">
Kerr, N. L. (1998). <span>HARKing</span>: <span>Hypothesizing</span> <span>After</span> the <span>Results</span> are <span>Known</span>. <em>Personality &amp; Social Psychology Review (Lawrence Erlbaum Associates)</em>, <em>2</em>(3), 196. <a href="https://doi.org/10.1207/s15327957pspr0203_4">https://doi.org/10.1207/s15327957pspr0203_4</a>
</div>
<div id="ref-kunda1990" class="csl-entry">
Kunda, Z. (1990). The case for motivated reasoning. <em>Psychological Bulletin</em>, <em>108</em>(3), 480–498. <a href="https://doi.org/10.1037/0033-2909.108.3.480">https://doi.org/10.1037/0033-2909.108.3.480</a>
</div>
<div id="ref-lin2016" class="csl-entry">
Lin, W., &amp; Green, D. P. (2016). Standard operating procedures: A safety net for pre-analysis plans. <em>PS: Political Science &amp; Politics</em>, <em>49</em>(03), 495–500. <a href="https://doi.org/10.1017/S1049096516000810">https://doi.org/10.1017/S1049096516000810</a>
</div>
<div id="ref-nickerson1998" class="csl-entry">
Nickerson, R. S. (1998). Confirmation bias: A ubiquitous phenomenon in many guises. <em>Review of General Psychology</em>, <em>2</em>(2), 175–220. <a href="https://doi.org/10.1037/1089-2680.2.2.175">https://doi.org/10.1037/1089-2680.2.2.175</a>
</div>
<div id="ref-nosek2019" class="csl-entry">
Nosek, B. A., Beck, E. D., Campbell, L., Flake, J. K., Hardwicke, T. E., Mellor, D. T., Veer, A. E. van ’t, &amp; Vazire, S. (2019). Preregistration is hard, and worthwhile. <em>Trends in Cognitive Sciences</em>, <em>23</em>(10), 815–818. <a href="https://doi.org/10.1016/j.tics.2019.07.009">https://doi.org/10.1016/j.tics.2019.07.009</a>
</div>
<div id="ref-nosek2018" class="csl-entry">
Nosek, B. A., Ebersole, C. R., DeHaven, A. C., &amp; Mellor, D. T. (2018). The preregistration revolution. <em>Proceedings of the National Academy of Sciences</em>, <em>115</em>(11), 2600–2606. <a href="https://doi.org/10.1073/pnas.1708274114">https://doi.org/10.1073/pnas.1708274114</a>
</div>
<div id="ref-nosek2012" class="csl-entry">
Nosek, B. A., Spies, J. R., &amp; Motyl, M. (2012). Scientific <span>Utopia</span>: <span>II</span>. Restructuring incentives and practices to promote truth over publishability. <em>Perspectives on Psychological Science</em>, <em>7</em>(6), 615–631. <a href="https://doi.org/10.1177/1745691612459058">https://doi.org/10.1177/1745691612459058</a>
</div>
<div id="ref-oboyle2017" class="csl-entry">
O’Boyle, E. H., Banks, G. C., &amp; Gonzalez-Mulé, E. (2017). The chrysalis effect: How ugly initial results metamorphosize into beautiful articles. <em>Journal of Management</em>, <em>43</em>(2), 376–399. <a href="https://doi.org/10.1177/0149206314527133">https://doi.org/10.1177/0149206314527133</a>
</div>
<div id="ref-oberauer2019" class="csl-entry">
Oberauer, K., &amp; Lewandowsky, S. (2019). Addressing the theory crisis in psychology. <em>Psychonomic Bulletin &amp; Review</em>, <em>26</em>(5), 1596–1618.
</div>
<div id="ref-rubin2020" class="csl-entry">
Rubin, M. (2020). Does preregistration improve the credibility of research findings? <em>The Quantitative Methods for Psychology</em>, <em>16</em>(4), 15. <a href="https://doi.org/10.20982/tqmp.16.4.p376">https://doi.org/10.20982/tqmp.16.4.p376</a>
</div>
<div id="ref-simonsohn2020" class="csl-entry">
Simonsohn, U., Simmons, J. P., &amp; Nelson, L. D. (2020). Specification curve analysis. <em>Nature Human Behaviour</em>, 1–7. <a href="https://doi.org/10.1038/s41562-020-0912-z">https://doi.org/10.1038/s41562-020-0912-z</a>
</div>
<div id="ref-slovic1977" class="csl-entry">
Slovic, P., &amp; Fischhoff, B. (1977). On the psychology of experimental surprises. <em>Journal of Experimental Psychology: Human Perception and Performance</em>, <em>3</em>(4), 544–551. <a href="https://doi.org/10.1037/0096-1523.3.4.544">https://doi.org/10.1037/0096-1523.3.4.544</a>
</div>
<div id="ref-smaldino2016" class="csl-entry">
Smaldino, Paul E., &amp; McElreath, R. (2016). The natural selection of bad science. <em>Royal Society Open Science</em>, <em>3</em>(9), 160384. <a href="https://doi.org/10.1098/rsos.160384">https://doi.org/10.1098/rsos.160384</a>
</div>
<div id="ref-steegen2016" class="csl-entry">
Steegen, S., Tuerlinckx, F., Gelman, A., &amp; Vanpaemel, W. (2016). Increasing transparency through a multiverse analysis. <em>Perspectives on Psychological Science</em>, <em>11</em>(5), 702–712. <a href="https://doi.org/10.1177/1745691616658637">https://doi.org/10.1177/1745691616658637</a>
</div>
<div id="ref-tukey1980" class="csl-entry">
Tukey, John W. (1980). We need both exploratory and confirmatory. <em>The American Statistician</em>, <em>34</em>(1), 23–25. <a href="https://doi.org/10.2307/2682991">https://doi.org/10.2307/2682991</a>
</div>
<div id="ref-veldkamp2017" class="csl-entry">
Veldkamp, C. L. S., Hartgerink, C. H. J., Assen, M. A. L. M. van, &amp; Wicherts, J. M. (2017). Who believes in the storybook image of the scientist? <em>Accountability in Research</em>, <em>24</em>(3), 127–151. <a href="https://doi.org/10.1080/08989621.2016.1268922">https://doi.org/10.1080/08989621.2016.1268922</a>
</div>
<div id="ref-wagenmakers2012" class="csl-entry">
Wagenmakers, E.-J., Wetzels, R., Borsboom, D., Maas, H. L. J. van der, &amp; Kievit, R. A. (2012). An agenda for purely confirmatory research. <em>Perspectives on Psychological Science</em>, <em>7</em>(6), 632–638. <a href="https://doi.org/10.1177/1745691612463078">https://doi.org/10.1177/1745691612463078</a>
</div>
</div>

</div>
</div>




<script type="module" src="/assets/src/index.page.client.jsx.df9edbd0.js"></script><script id="vite-plugin-ssr_pageContext" type="application/json">{"pageContext":{"_pageId":"/src/index","url":"/12-prereg","body":"\n\n\n\n\u003cdiv class=\"row\">\n\u003cdiv class=\"col-sm-12\">\n\u003cdiv id=\"TOC\">\n\u003cul>\n\u003cli>\u003ca href=\"#part-preliminaries\" id=\"toc-part-preliminaries\">(PART) Preliminaries\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"1-experiments.html#experiments\" id=\"toc-experiments\">\u003cspan class=\"toc-section-number\">1\u003c/span> Experiments\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"2-theories.html#theories\" id=\"toc-theories\">\u003cspan class=\"toc-section-number\">2\u003c/span> Theories\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"3-replication.html#replication\" id=\"toc-replication\">\u003cspan class=\"toc-section-number\">3\u003c/span> Replication and reproducibility\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"4-ethics.html#ethics\" id=\"toc-ethics\">\u003cspan class=\"toc-section-number\">4\u003c/span> Ethics\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"#part-statistics\" id=\"toc-part-statistics\">(PART) Statistics\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"5-estimation.html#estimation\" id=\"toc-estimation\">\u003cspan class=\"toc-section-number\">5\u003c/span> Estimation\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"6-inference.html#inference\" id=\"toc-inference\">\u003cspan class=\"toc-section-number\">6\u003c/span> Inference\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"7-models.html#models\" id=\"toc-models\">\u003cspan class=\"toc-section-number\">7\u003c/span> Models\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"#part-design-and-planning\" id=\"toc-part-design-and-planning\">(PART) Design and Planning\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"8-measurement.html#measurement\" id=\"toc-measurement\">\u003cspan class=\"toc-section-number\">8\u003c/span> Measurement\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"9-design.html#design\" id=\"toc-design\">\u003cspan class=\"toc-section-number\">9\u003c/span> Design of experiments\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"10-sampling.html#sampling\" id=\"toc-sampling\">\u003cspan class=\"toc-section-number\">10\u003c/span> Sampling\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"11-strategy.html#strategy\" id=\"toc-strategy\">\u003cspan class=\"toc-section-number\">11\u003c/span> Experimental strategy\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"#part-execution\" id=\"toc-part-execution\">(PART) Execution\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"12-prereg.html#prereg\" id=\"toc-prereg\">\u003cspan class=\"toc-section-number\">12\u003c/span> Preregistration\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"13-consent.html#consent\" id=\"toc-consent\">\u003cspan class=\"toc-section-number\">13\u003c/span> Recruitment and Consent\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"14-collection.html#collection\" id=\"toc-collection\">\u003cspan class=\"toc-section-number\">14\u003c/span> Data collection\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"15-management.html#management\" id=\"toc-management\">\u003cspan class=\"toc-section-number\">15\u003c/span> Project management\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"#part-analysis-and-reporting\" id=\"toc-part-analysis-and-reporting\">(PART) Analysis and Reporting\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"16-viz.html#viz\" id=\"toc-viz\">\u003cspan class=\"toc-section-number\">16\u003c/span> Visualization\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"17-eda.html#eda\" id=\"toc-eda\">\u003cspan class=\"toc-section-number\">17\u003c/span> Exploratory data analysis\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"18-writing.html#writing\" id=\"toc-writing\">\u003cspan class=\"toc-section-number\">18\u003c/span> Writing\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"19-meta.html#meta\" id=\"toc-meta\">\u003cspan class=\"toc-section-number\">19\u003c/span> Meta-analysis\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"20-conclusions.html#conclusions\" id=\"toc-conclusions\">\u003cspan class=\"toc-section-number\">20\u003c/span> Conclusions\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"#appendix-appendices\" id=\"toc-appendix-appendices\">(APPENDIX) Appendices\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"A-git.html#git\" id=\"toc-git\">\u003cspan class=\"toc-section-number\">21\u003c/span> GitHub Tutorial\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"B-rmarkdown.html#rmarkdown\" id=\"toc-rmarkdown\">\u003cspan class=\"toc-section-number\">22\u003c/span> R Markdown Tutorial\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"C-tidyverse.html#tidyverse\" id=\"toc-tidyverse\">\u003cspan class=\"toc-section-number\">23\u003c/span> Tidyverse Tutorial\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"D-ggplot.html#ggplot\" id=\"toc-ggplot\">\u003cspan class=\"toc-section-number\">24\u003c/span> ggplot Tutorial\u003c/a>\u003c/li>\n\u003cli>\u003ca href=\"E-instructors.html#instructors\" id=\"toc-instructors\">\u003cspan class=\"toc-section-number\">25\u003c/span> Instructor’s guide\u003c/a>\u003c/li>\n\u003c/ul>\n\u003c/div>\n\u003c/div>\n\u003c/div>\n\u003cdiv class=\"row\">\n\u003cdiv class=\"col-sm-12\">\n\u003cdiv id=\"prereg\" class=\"section level1\" number=\"12\">\n\u003ch1>\u003cspan class=\"header-section-number\">Chapter 12\u003c/span> Preregistration\u003c/h1>\n\u003c!-- ```{r prereg-meme} -->\n\u003c!-- knitr::include_graphics(\"images/prereg/meme.jpg\") -->\n\u003c!-- ``` -->\n\u003cdiv class=\"box learning_goals\">\n\u003cul>\n\u003cli>Recognize the dangers of researcher degrees of freedom in “the garden of forking paths”\u003c/li>\n\u003cli>Understand the differences between exploratory and confirmatory modes of research\u003c/li>\n\u003cli>Learn how preregistration and other tools can reduce risk of bias and help others to evaluate your work by increasing transparency\u003c/li>\n\u003c/ul>\n\u003c/div>\n\u003cblockquote>\n\u003cp>When not planned beforehand, data analysis can approximate a projective technique, such as the Rorschach, because the investigator can project on the data his own expectancies, desires, or biases and can pull out of the data almost any ‘finding’ he may desire.\u003c/p>\n\u003cfooter>\n— Theodore X. Barber \u003cspan class=\"citation\">(\u003ca href=\"#ref-barber1976\" role=\"doc-biblioref\">1976\u003c/a>)\u003c/span>\n\u003c/footer>\n\u003c/blockquote>\n\u003cblockquote>\n\u003cp>The first principle is that you must not fool yourself–and you are the easiest person to fool…After you’ve not fooled yourself, it’s easy not to fool other scientists. You just have to be honest in a conventional way after that.\u003c/p>\n\u003cfooter>\n— Richard Feynman \u003cspan class=\"citation\">(\u003ca href=\"#ref-feynman1974\" role=\"doc-biblioref\">1974\u003c/a>)\u003c/span>\n\u003c/footer>\n\u003c/blockquote>\n\u003cp>This may surprise you coming from the authors of a textbook about research methods, but there is no single “correct” way to design and analyze an experiment.\u003clabel for=\"tufte-sn-154\" class=\"margin-toggle sidenote-number\">154\u003c/label>\u003cinput type=\"checkbox\" id=\"tufte-sn-154\" class=\"margin-toggle\">\u003cspan class=\"sidenote\">\u003cspan class=\"sidenote-number\">154\u003c/span> Though there are plenty of incorrect ways to design and analyse experiments and we hope we can help you to avoid these!\u003c/span> In fact, for most research decisions, there are many justifiable options. For example, will you stop data collection after 20, 200, or 2000 participants? Will you remove outlier values and how will you define them? Will you conduct subgroup analyses to see whether the results are affected by sex, or age, or some other factor? Consider a simplified, hypothetical case where you need to make five analysis decisions and have five justifiable options for each decision — this alone would result in 3125 (5^5) unique ways to analyze your data!\u003c/p>\n\u003cp>In this chapter, we will find out why undisclosed flexibility in the design, analysis, reporting, and interpretation of experiments (also referred to as “researcher degrees of freedom”), can lead to scientists fooling themselves and fooling each other. We will also learn about how \u003cstrong>preregistration\u003c/strong> – the process of writing down and registering your design and analysis decisions before you conduct your study – (and other tools) can be used to protect our research from bias and provide the transparency that other scientists need to properly evaluate and interpret our work.\u003c/p>\n\u003cp>Our bottom line is that the best practice is to document your experiment – including critical design, sampling, and analysis decisions – before collecting data. This documentation can help you think through your choices to ensure that they are maximally aligned with your goals. Further, the documentation can be timestamped using an external registry and shared so as to show which decisions were \u003cem>post hoc\u003c/em> and which were made in advance.\u003c/p>\n\u003cdiv class=\"box case_study\">\n\u003cp>(TITLE) Undisclosed analytic flexibility?\u003c/p>\n\u003cp>Educational apps for children are a huge market, but relatively few high-quality, randomized trials have been done to see whether or when they produce educational gains. In that context, \u003cspan class=\"citation\">Berkowitz et al. (\u003ca href=\"#ref-berkowitz2015\" role=\"doc-biblioref\">2015\u003c/a>)\u003c/span> reported a high-quality field experiment of educational apps, with participants randomly assigned to either a math or reading app over the course of a full school year. Critically, along with random assignment, the study also included standardized measures of math and reading achievement. These measures allowed the authors to compute effects in grade-level equivalents, a meaningful unit from a policy perspective. The key result reported by the paper is shown in Figure @(fig:prereg-berkowitz). Families who used the math app frequently showed greater gains in math than the control group.\u003c/p>\n\u003cp>\n\u003cspan class=\"marginnote shownote\">\n\u003c!--\n\u003cdiv class=\"figure\">-->\u003cspan style=\"display:block;\" id=\"fig:prereg-berkowitz\">\u003c/span>\n\u003cimg src=\"images/prereg/berkowitz-1.png\" alt=\"Figure 1 of Berkowitz et al. (2015). Estimated years of math achievement gained over the school year across groups.\" width=\"\\linewidth\"  />\n\u003c!--\n\u003cp class=\"caption marginnote\">-->Figure 12.1: Figure 1 of Berkowitz et al. (2015). Estimated years of math achievement gained over the school year across groups.\u003c!--\u003c/p>-->\n\u003c!--\u003c/div>-->\u003c/span>\n\u003c/p>\n\u003cp>Although this finding appeared striking, something was a little odd about the main analyses, exemplified by Figure \u003ca href=\"12-prereg.html#fig:prereg-berkowitz\">12.1\u003c/a>. There was no way to see an estimate of the primary causal effect of interest, namely the effect of condition on math scores. Instead the data were presented as estimated effects for specific points – a “matched” subgroup in panel A and the entire group in panel B – both broken down by app usage.\u003c/p>\n\u003cp>Naively, including app usage might seem like it “clarifies” the causal effect of the app – since if you didn’t use the app, it couldn’t have any effect at all! The trouble with this logic is that usage is not causally isolated from the system, and hence it can be confounded with other factors (as discussed in Chapter \u003ca href=\"1-experiments.html#experiments\">1\u003c/a>)! Put another way, families that use the app extensively probably differ in other ways from samples that don’t – maybe they’re more concerned with math learning in general. So we can’t compare high-usage families and conclude that the app is useful in general.\u003c/p>\n\u003cdiv class=\"figure\">\u003cspan style=\"display:block;\" id=\"fig:prereg-frank-berkowitz\">\u003c/span>\n\u003cp class=\"caption marginnote shownote\">\nFigure 12.2: Figure 1 of Frank (2016). Estimated years of math achievement gained over the school year across groups.\n\u003c/p>\n\u003cimg src=\"images/prereg/ITT.png\" alt=\"Figure 1 of Frank (2016). Estimated years of math achievement gained over the school year across groups.\" width=\"\\linewidth\"  />\n\u003c/div>\n\u003cp>When the primary causal effect of interest was estimated via a simple approach, the intervention appeared to have a very weak effect [Figure \u003ca href=\"12-prereg.html#fig:prereg-frank-berkowitz\">12.2\u003c/a>; \u003cspan class=\"citation\">Frank (\u003ca href=\"#ref-frank2016\" role=\"doc-biblioref\">2016\u003c/a>)\u003c/span>]. Since this analysis was not favorable to the primary intervention – and because it was not reported in the paper – a reader might worry that the authors had chosen an analysis that “looked better” with respect to their hypotheses of interest. The original authors responded that their analyses were entirely based on prior research and argued that the disagreement about how the data should be analyzed was “philosophical” \u003cspan class=\"citation\">(\u003ca href=\"#ref-berkowitz2016\" role=\"doc-biblioref\">Berkowitz et al., 2016\u003c/a>)\u003c/span>.\u003c/p>\n\u003cp>Disagreements about data analysis are an important feature of the scientific discourse. The problem here is that it is very difficult to know the extent to which the original analysis was influenced by the results. If the analysis plan had been preregistered, this simple step would have reduced the risk of bias and hence increased the value of the estimates resulting from this high-quality study.\u003clabel for=\"tufte-sn-155\" class=\"margin-toggle sidenote-number\">155\u003c/label>\u003cinput type=\"checkbox\" id=\"tufte-sn-155\" class=\"margin-toggle\">\u003cspan class=\"sidenote\">\u003cspan class=\"sidenote-number\">155\u003c/span> Preregistration cannot magically improve a poorly specified analysis. Some of the original analyses in the paper were straightforwardly erroneous: they concluded that there was an effect in one group but not in another by testing the effect separately in those groups and neglecting to test the interaction. This is a statistical fallacy, regardless of whether the analyses were preregistered \u003cspan class=\"citation\">(\u003ca href=\"#ref-gelman2006\" role=\"doc-biblioref\">Gelman &amp; Stern, 2006\u003c/a>)\u003c/span>. Nevertheless, preregistration would have provided transparency to enable a more informed scientific debate about the results without concern for whether they influenced by undisclosed analytic flexibility.\u003c/span>\u003c/p>\n\u003c/div>\n\u003cdiv id=\"lost-in-a-garden-of-forking-paths\" class=\"section level2\" number=\"12.1\">\n\u003ch2>\u003cspan class=\"header-section-number\">12.1\u003c/span> Lost in a garden of forking paths\u003c/h2>\n\u003cp>\n\u003cspan class=\"marginnote shownote\">\n\u003c!--\n\u003cdiv class=\"figure\">-->\u003cspan style=\"display:block;\" id=\"fig:forking-paths\">\u003c/span>\n\u003cimg src=\"images/prereg/forking-paths.png\" alt=\"Garden of forking paths (placeholder image I hacked together, replace with an illustration?)\" width=\"\\linewidth\"  />\n\u003c!--\n\u003cp class=\"caption marginnote\">-->Figure 12.3: Garden of forking paths (placeholder image I hacked together, replace with an illustration?)\u003c!--\u003c/p>-->\n\u003c!--\u003c/div>-->\u003c/span>\n\u003c/p>\n\u003cp>One way to visualize researcher degrees of freedom is as a vast decision tree or “garden of forking paths” \u003cspan class=\"citation\">(\u003ca href=\"#ref-gelman2014\" role=\"doc-biblioref\">Gelman &amp; Loken, 2014\u003c/a> Figure \u003ca href=\"12-prereg.html#fig:forking-paths\">12.3\u003c/a>)\u003c/span>. Each node represents a decision point and each branch represents a justifiable choice. Each unique pathway through the garden terminates in an individual result.\u003c/p>\n\u003cp>Because scientific observations typically consist of both noise (random variation unique to this sample) and signal (regularities that will reoccur in other samples), some of these pathways will inevitably lead to results that are misleading (e.g., inflated effect sizes, exaggerated evidence, or false positives).\u003clabel for=\"tufte-sn-156\" class=\"margin-toggle sidenote-number\">156\u003c/label>\u003cinput type=\"checkbox\" id=\"tufte-sn-156\" class=\"margin-toggle\">\u003cspan class=\"sidenote\">\u003cspan class=\"sidenote-number\">156\u003c/span> The signal-to-noise ratio is worse in situations (alas, common in psychology) that involve small effect sizes, high variation, and large measurement errors \u003cspan class=\"citation\">(\u003ca href=\"#ref-ioannidis2005\" role=\"doc-biblioref\">Ioannidis, 2005\u003c/a>)\u003c/span>. Researcher degrees of freedom may be constrained to some extent by strong theory \u003cspan class=\"citation\">(\u003ca href=\"#ref-oberauer2019\" role=\"doc-biblioref\">Oberauer &amp; Lewandowsky, 2019\u003c/a>)\u003c/span>, community methodological norms and standards, or replication studies, though these constraints may be more implicit than explicit, and can still leave plenty of room for flexible decision-making.\u003c/span> The more potential paths there are in the garden that you might explore, the higher the chance of encountering misleading results.\u003clabel for=\"tufte-sn-157\" class=\"margin-toggle sidenote-number\">157\u003c/label>\u003cinput type=\"checkbox\" id=\"tufte-sn-157\" class=\"margin-toggle\">\u003cspan class=\"sidenote\">\u003cspan class=\"sidenote-number\">157\u003c/span> In frequentist terminology, there is an increasing the chance of making a “Type I error”.\u003c/span> Statisticians refer to this as a \u003cem>multiplicity\u003c/em> problem.\u003c/p>\n\u003cp>As we talked about in Chapter \u003ca href=\"6-inference.html#inference\">6\u003c/a>, multiplicity can be addressed to some extent with statistical countermeasures, like the Bonferroni correction; however, these adjustment methods need to account for every path that you \u003cem>could have\u003c/em> taken \u003cspan class=\"citation\">(\u003ca href=\"#ref-degroot2014\" role=\"doc-biblioref\">de Groot, 1956/2014\u003c/a>; \u003ca href=\"#ref-gelman2014\" role=\"doc-biblioref\">Gelman &amp; Loken, 2014\u003c/a>)\u003c/span>. When you navigate the garden of forking paths \u003cem>during data analysis\u003c/em>, it is easy to forget, or even be unaware of every path that you could have taken, so these methods can no longer be used effectively.\u003c/p>\n\u003cdiv id=\"results-dependent-analysis\" class=\"section level3\" number=\"12.1.1\">\n\u003ch3>\u003cspan class=\"header-section-number\">12.1.1\u003c/span> Results-dependent analysis\u003c/h3>\n\u003cp>When a researcher navigates the garden of forking paths during data analysis, their decisions can be biased because they are receiving feedback on how different choices affect the results (\u003cstrong>results-dependent\u003c/strong> decision making). If a researcher is seeking a particular kind of result (which is likely – see the interaction box below), then they are more likely to follow the branches that steer them in that direction.\u003c/p>\n\u003cp>You could think of this a bit like playing a game of “hot🔥! or cold☃️!” where hot🔥! indicates that the choice will move the researcher closer to a desirable overall result and cold☃️! indicates that the choice will move them further away. Each time the researcher reaches a decision point, they try one of the branches and get feedback on how that choice affects the results. If the feedback is hot🔥! then they take that branch. If the answer is cold☃️!, they try a different branch. If they reach the end of a complete pathway, and the results are cold☃️!, maybe they even retrace their steps and try some different branches earlier in the pathway. This strategy create a risk of bias\u003clabel for=\"tufte-sn-158\" class=\"margin-toggle sidenote-number\">158\u003c/label>\u003cinput type=\"checkbox\" id=\"tufte-sn-158\" class=\"margin-toggle\">\u003cspan class=\"sidenote\">\u003cspan class=\"sidenote-number\">158\u003c/span> We say “risk of bias” rather than just “bias” because in most scientific contexts, we do not have a known ground truth to compare the results to. So in any specific situation, we do not know the extent to which results-dependent analyses have actually biased the results.\u003c/span> because the results are being systematically skewed towards the researcher’s preferences\u003clabel for=\"tufte-sn-159\" class=\"margin-toggle sidenote-number\">159\u003c/label>\u003cinput type=\"checkbox\" id=\"tufte-sn-159\" class=\"margin-toggle\">\u003cspan class=\"sidenote\">\u003cspan class=\"sidenote-number\">159\u003c/span> Another way to think of this is in terms of ‘regression to the mean’. When a sample statistic is selected because it crosses some threshold (e.g., statistical significance), then it is more likely to provide a biased estimate that decreases upon subsequent measurement.\u003c/span> \u003cspan class=\"citation\">(\u003ca href=\"#ref-hardwicke2021b\" role=\"doc-biblioref\">Hardwicke &amp; Wagenmakers, 2021\u003c/a>)\u003c/span>.\u003c/p>\n\u003cdiv class=\"box examples\">\n\u003cp>(TITLE) Only human: Cognitive biases and skewed incentives\u003c/p>\n\u003cp>There’s a storybook image of the scientist as an objective, rationale, and dispassionate arbiter of truth \u003cspan class=\"citation\">(\u003ca href=\"#ref-veldkamp2017\" role=\"doc-biblioref\">Veldkamp et al., 2017\u003c/a>)\u003c/span>. But in reality, scientists are only human: they have egos, career ambitions, and rent to pay! So even if we do want to live up to the storybook image, its important to acknowledge that our decisions and behaviour are also influenced by a range of cognitive biases and external incentives that can steer us away from that goal. This highlights the need for transparency and intellectual humility \u003cspan class=\"citation\">(\u003ca href=\"#ref-hoekstra2020\" role=\"doc-biblioref\">Hoekstra &amp; Vazire, 2020\u003c/a>)\u003c/span> when reporting the findings of our research. Let’s first look at some relevant cognitive biases that might lead scientists astray:\u003c/p>\n\u003cp>\u003cstrong>Confirmation bias\u003c/strong>: Preferentially seeking out, recalling, or evaluating information in a manner that reinforces one’s existing beliefs \u003cspan class=\"citation\">(\u003ca href=\"#ref-nickerson1998\" role=\"doc-biblioref\">Nickerson, 1998\u003c/a>)\u003c/span>.\u003c/p>\n\u003cp>\u003cstrong>Hindsight bias\u003c/strong>: Believing that past events were always more likely to occur relative to our actual belief in their likelihood before they happened (“I knew it all along!”) \u003cspan class=\"citation\">(\u003ca href=\"#ref-slovic1977\" role=\"doc-biblioref\">Slovic &amp; Fischhoff, 1977\u003c/a>)\u003c/span>.\u003c/p>\n\u003cp>\u003cstrong>Motivated reasoning\u003c/strong>: Rationalizing prior decisions so they are framed in a favorable light, even if they were irrational \u003cspan class=\"citation\">(\u003ca href=\"#ref-kunda1990\" role=\"doc-biblioref\">Kunda, 1990\u003c/a>)\u003c/span>.\u003c/p>\n\u003cp>\n\u003cspan class=\"marginnote shownote\">\n\u003c!--\n\u003cdiv class=\"figure\">-->\u003cspan style=\"display:block;\" id=\"fig:apophenia\">\u003c/span>\n\u003cimg src=\"images/prereg/apophenia.png\" alt=\"Examples of apophenia: Mars Face, Winnie the Pooh Cloud, and Jesus Toast.\" width=\"\\linewidth\"  />\n\u003c!--\n\u003cp class=\"caption marginnote\">-->Figure 12.4: Examples of apophenia: Mars Face, Winnie the Pooh Cloud, and Jesus Toast.\u003c!--\u003c/p>-->\n\u003c!--\u003c/div>-->\u003c/span>\n\u003c/p>\n\u003cp>\u003cstrong>Apophenia\u003c/strong>: Detecting seemingly meaningful patterns in noise (Figure \u003ca href=\"12-prereg.html#fig:apophenia\">12.4\u003c/a>) \u003cspan class=\"citation\">(\u003ca href=\"#ref-gilovich1985\" role=\"doc-biblioref\">Gilovich et al., 1985\u003c/a>)\u003c/span>.\u003c/p>\n\u003cp>\n\u003cspan class=\"marginnote shownote\">\n\u003c!--\n\u003cdiv class=\"figure\">-->\u003cspan style=\"display:block;\" id=\"fig:chrysalis\">\u003c/span>\n\u003cimg src=\"images/prereg/chrysalis.png\" alt=\"The Chrysalis Effect, when ugly truth becomes a beautiful fiction.\" width=\"\\linewidth\"  />\n\u003c!--\n\u003cp class=\"caption marginnote\">-->Figure 12.5: The Chrysalis Effect, when ugly truth becomes a beautiful fiction.\u003c!--\u003c/p>-->\n\u003c!--\u003c/div>-->\u003c/span>\n\u003c/p>\n\u003cp>To make matters worse, the incentive structure of the scientific ecosystem often adds additional motivation to get things wrong. The allocation of funding, awards, and publication prestige is often based on the nature of research results rather than research quality \u003cspan class=\"citation\">(\u003ca href=\"#ref-nosek2012\" role=\"doc-biblioref\">B. A. Nosek et al., 2012\u003c/a>; \u003ca href=\"#ref-smaldino2016\" role=\"doc-biblioref\">Paul E. Smaldino &amp; McElreath, 2016\u003c/a>)\u003c/span>. For example, many academic journals, especially those that are widely considered to be the most prestigious, appear to have a preference for novel, positive, and ‘statistically significant’ findings over incremental, negative, or null findings \u003cspan class=\"citation\">(\u003ca href=\"#ref-bakker2012\" role=\"doc-biblioref\">Bakker et al., 2012\u003c/a>)\u003c/span>. There is also pressure to write articles with concise, coherent, and compelling narratives \u003cspan class=\"citation\">(\u003ca href=\"#ref-giner-sorolla2012\" role=\"doc-biblioref\">Giner-Sorolla, 2012\u003c/a>)\u003c/span>. This set of forces incentivize scientists to be “impressive” over being right and encourages questionable research practices. The process of iteratively p-hacking and HARKing one’s way to a “beautiful” scientific paper has been dubbed “The Chrysalis Effect” \u003cspan class=\"citation\">(\u003ca href=\"#ref-oboyle2017\" role=\"doc-biblioref\">O’Boyle et al., 2017\u003c/a>)\u003c/span>, as shown in Figure \u003ca href=\"12-prereg.html#fig:chrysalis\">12.5\u003c/a>.\u003c/p>\n\u003c/div>\n\u003cp>In the most egregious cases, a researcher may try multiple pathways until they obtain a desirable result\u003clabel for=\"tufte-sn-160\" class=\"margin-toggle sidenote-number\">160\u003c/label>\u003cinput type=\"checkbox\" id=\"tufte-sn-160\" class=\"margin-toggle\">\u003cspan class=\"sidenote\">\u003cspan class=\"sidenote-number\">160\u003c/span> “If you torture the data long enough, it will confess” \u003cspan class=\"citation\">(\u003ca href=\"#ref-good1972\" role=\"doc-biblioref\">Good, 1972\u003c/a>)\u003c/span>\u003c/span> and then \u003cstrong>selectively report\u003c/strong> that result, neglecting to mention that they have tried several other analysis strategies. You may remember an example of this type when participants apparently became younger when they listened to “When I’m 64” by The Beatles in Chapter \u003ca href=\"4-ethics.html#ethics\">4\u003c/a>. Another nice example of how damaging the garden of forking paths can be comes from the “discovery” of brain activity in a dead Atlantic Salmon! Researchers deliberately exploited flexibility in the fMRI analysis pipeline to find brain activity where there was only dead fish \u003cspan class=\"citation\">(Figure \u003ca href=\"12-prereg.html#fig:salmon\">12.6\u003c/a>, \u003ca href=\"#ref-bennett2009\" role=\"doc-biblioref\">Bennett et al., 2009\u003c/a>)\u003c/span>.\u003c/p>\n\u003cp>Deliberately taking advantage of researcher degrees of freedom and selectively reporting results is known by various names, like p-hacking, cherry picking, data dredging, and it is unethical because it involves hiding highly relevant information. But you should also be aware that results-dependent analysis incurs a risk of bias even if a researcher has good intentions, doesn’t explicitly try multiple pathways, and honestly reports everything they did.\u003c/p>\n\u003cp>If a researcher examines a dataset and takes a set of hot🔥 paths, they may reach the result they desire at the end of the pathway without realizing that – had the results been different – they would have followed other pathways \u003cspan class=\"citation\">(\u003ca href=\"#ref-degroot2014\" role=\"doc-biblioref\">de Groot, 1956/2014\u003c/a>; \u003ca href=\"#ref-gelman2014\" role=\"doc-biblioref\">Gelman &amp; Loken, 2014\u003c/a>)\u003c/span>. Even though the researcher didn’t intend to hide anything, there is still undisclosed analytic flexibility – important context that is relevant to interpret the results properly. It’s surprisingly easy to convince yourself after the fact that you made the decisions you did for principled reasons that had nothing to do with the results (see “motivated reasoning” box). Results-dependent analysis increases the chances that you will fool yourself by inadvertently stumbling across misleading results. If that analytic flexibility goes undisclosed, you may fool others too.\u003c/p>\n\u003cp>\n\u003cspan class=\"marginnote shownote\">\n\u003c!--\n\u003cdiv class=\"figure\">-->\u003cspan style=\"display:block;\" id=\"fig:salmon\">\u003c/span>\n\u003cimg src=\"images/prereg/salmon.jpeg\" alt=\"By deliberately exploiting analytic flexibility in the processing pipeline of fMRI data, Bennet et al. (2009) were able to identify 'brain activity' in a dead Atlantic Salmon.\" width=\"\\linewidth\"  />\n\u003c!--\n\u003cp class=\"caption marginnote\">-->Figure 12.6: By deliberately exploiting analytic flexibility in the processing pipeline of fMRI data, Bennet et al. (2009) were able to identify ‘brain activity’ in a dead Atlantic Salmon.\u003c!--\u003c/p>-->\n\u003c!--\u003c/div>-->\u003c/span>\n\u003c/p>\n\u003c/div>\n\u003cdiv id=\"hypothesize-after-results-are-known\" class=\"section level3\" number=\"12.1.2\">\n\u003ch3>\u003cspan class=\"header-section-number\">12.1.2\u003c/span> Hypothesize after results are known?\u003c/h3>\n\u003cp>In addition to flexibility in analysis, there is additional flexibility in how researchers \u003cem>explain\u003c/em> research results. As we discussed in Chapter \u003ca href=\"2-theories.html#theories\">2\u003c/a>, theories can accommodate even conflicting results in many different ways – for example, by positing auxilliary hypotheses that explain why a particular datapoint is special. We might call these different routes for accommodating theory with data “explanatory degrees of freedom”.\u003c/p>\n\u003cp>The practice of selecting or developing your hypothesis after seeing the study results has been called “Hypotheisizing After the Results are Known”, or “HARKing” \u003cspan class=\"citation\">(\u003ca href=\"#ref-kerr1998\" role=\"doc-biblioref\">Kerr, 1998\u003c/a>)\u003c/span>. HARKing is potentially problematic because it expands the garden of forking paths and helps to justify the use of various analytic degrees of freedom (Figure \u003ca href=\"12-prereg.html#fig:grid\">12.7\u003c/a>). For example, you may come up with an explanation for why an intervention is effective in men but not in women in order to justify a post-hoc subgroup analysis based on sex (see Case Study). The extent to which HARKing is problematic is contested \u003cspan class=\"citation\">(for discussion see \u003ca href=\"#ref-hardwicke2021b\" role=\"doc-biblioref\">Hardwicke &amp; Wagenmakers, 2021\u003c/a>)\u003c/span>. But at the very least it’s important to be honest about whether hypotheses were developed before or after seeing research results.\u003c/p>\n\u003cdiv class=\"figure\">\u003cspan style=\"display:block;\" id=\"fig:grid\">\u003c/span>\n\u003cp class=\"caption marginnote shownote\">\nFigure 12.7: A scientist exploring a grid of individual research results. The horizontal axis illustrates a simplified ‘garden of forking paths’: the many justifiable analysis specifications that the scientist can use to transform the data (D) into the evidence (E). The vertical axis illustrates that there may be several relevant theories (T), and hypotheses (H) derived from those theories, which could be constructed or selected and then confronted with the evidence. Thus, an unconstrained scientist can simultaneously exploit their analytic degrees of freedom and explanatory degrees of freedom to fit evidence to hypotheses and fit hypotheses to evidence in order to arrive at a study outcome that is more likely to align more with their preferences, but less likely to align with the truth. Caption is copied verbatim so needs editing. Shared under a CC-BY license, artwork by Viktor Beekman, concept by Tom Hardwicke and Eric-Jan Wagenmakers.\n\u003c/p>\n\u003cimg src=\"images/prereg/grid.jpg\" alt=\"A scientist exploring a grid of individual research results. The horizontal axis illustrates a simplified ‘garden of forking paths’: the many justifiable analysis specifications that the scientist can use to transform the data (D) into the evidence (E). The vertical axis illustrates that there may be several relevant theories (T), and hypotheses (H) derived from those theories, which could be constructed or selected and then confronted with the evidence. Thus, an unconstrained scientist can simultaneously exploit their analytic degrees of freedom and explanatory degrees of freedom to fit evidence to hypotheses and fit hypotheses to evidence in order to arrive at a study outcome that is more likely to align more with their preferences, but less likely to align with the truth. Caption is copied verbatim so needs editing. Shared under a CC-BY license, artwork by Viktor Beekman, concept by Tom Hardwicke and Eric-Jan Wagenmakers.\" width=\"\\linewidth\"  />\n\u003c/div>\n\u003cp>But hang on a minute! Isn’t it a good thing to seek out interesting results if they are there in the data? Shouldn’t we “let the data speak”? The answer is yes! Exploratory research is \u003cem>not\u003c/em> the same as p-hacking. P-hacking is explicitly dishonest because it involves deliberately withholding information. In contrast, exploratory data analysis is a critical part of the scientific process (see Chapter \u003ca href=\"17-eda.html#eda\">17\u003c/a> for further discussion).\u003c/p>\n\u003c!-- In fact, we have dedicated a whole chapter to exploratory data analysis (Chapter \\@ref(eda)).  -->\n\u003cp>The important thing to remember about exploratory research is that you need to (a) be aware of the increased risk of bias and calibrate your confidence in the results accordingly; (2) be honest with other researchers about your analysis strategy so they are also aware of the risk of bias and can calibrate \u003cem>their\u003c/em> confidence in the results accordingly. It’s important to understand the distinction between \u003cstrong>exploratory\u003c/strong> and \u003cstrong>confirmatory\u003c/strong> research modes.\u003clabel for=\"tufte-sn-161\" class=\"margin-toggle sidenote-number\">161\u003c/label>\u003cinput type=\"checkbox\" id=\"tufte-sn-161\" class=\"margin-toggle\">\u003cspan class=\"sidenote\">\u003cspan class=\"sidenote-number\">161\u003c/span> In practice, an individual study may contain both exploratory and confirmatory aspects which is why we describe them as different “modes.”\u003c/span> Confirmatory research involves making design and analysis decisions \u003cem>before\u003c/em> the results have been observed. In the next section, we will learn about how to do that using preregistration.\u003c/p>\n\u003c/div>\n\u003c/div>\n\u003cdiv id=\"reducing-bias-increasing-transparency-and-calibrating-confidence-with-preregistration\" class=\"section level2\" number=\"12.2\">\n\u003ch2>\u003cspan class=\"header-section-number\">12.2\u003c/span> Reducing bias, increasing transparency, and calibrating confidence with preregistration\u003c/h2>\n\u003cp>If you make research decisions before you see the study results, you can counter the problem of undisclosed researcher degrees of freedom outlined above. That way, your decisions are made before seeing the study results – a bit like planning your route through the garden of forking paths before you start your journey. \u003cstrong>Preregistration\u003c/strong> is a great way to do this \u003cspan class=\"citation\">(\u003ca href=\"#ref-hardwicke2021b\" role=\"doc-biblioref\">Hardwicke &amp; Wagenmakers, 2021\u003c/a>; \u003ca href=\"#ref-wagenmakers2012\" role=\"doc-biblioref\">Wagenmakers et al., 2012\u003c/a>)\u003c/span>.\u003c/p>\n\u003cp>Preregistration is the process of declaring your research decisions in an public registry before you analyze (and often before you collect) the data. Preregistration ensures that your decisions are results-independent, which reduces risk of bias arising from the issues described above. Preregistration also transparently conveys to others what you planned, helping them to determine the risk of bias and calibrate their confidence in the results. In other words, preregistration transparently provides the context needed to properly evaluate and interpret research. In theory, preregistration dissuades researchers from engaging in questionable research practices like p-hacking and undisclosed HARKing, because they can be held accountable to their original plan.\u003c/p>\n\u003cp>Preregistration does not require that you specify all research decisions in advance, only that you are transparent about what was planned, and what was not planned. This division helps to make a distinction between which aspects of the research were exploratory and which were confirmatory (Figure \u003ca href=\"12-prereg.html#fig:continuum\">12.8\u003c/a>). All else being equal, we should have more confidence in confirmatory findings, because there is a lower risk of bias. Exploratory analyses have a higher risk of bias, but they are also more \u003cstrong>sensitive\u003c/strong> to serendipitous (unexpected) discoveries. Exploratory and confirmatory research are both valuable activities – it is just important to differentiate them \u003cspan class=\"citation\">(\u003ca href=\"#ref-tukey1980\" role=\"doc-biblioref\">John W. Tukey, 1980\u003c/a>)\u003c/span>! Preregistration offers the best of both worlds by clearly separating one from the other.\u003c/p>\n\u003cdiv class=\"figure\">\u003cspan style=\"display:block;\" id=\"fig:continuum\">\u003c/span>\n\u003cp class=\"caption marginnote shownote\">\nFigure 12.8: Preregistration clarifies where aspects of your research fall on a spectrum of exploratory and confirmatory modes of research. A preregistration is just a snapshot of your current thinking. If you have planned very little, your preregistration may not have much detail, but that’s absolutely fine! The important thing is that preregistration transparently conveys what was planned (confirmatory) and what was not (exploratory). Increasing the amount of detail in your preregistration increases your protection against bias.\n\u003c/p>\n\u003cimg src=\"images/prereg/continuum.png\" alt=\"Preregistration clarifies where aspects of your research fall on a spectrum of exploratory and confirmatory modes of research. A preregistration is just a snapshot of your current thinking. If you have planned very little, your preregistration may not have much detail, but that's absolutely fine! The important thing is that preregistration transparently conveys what was planned (confirmatory) and what was not (exploratory). Increasing the amount of detail in your preregistration increases your protection against bias.\" width=\"\\linewidth\"  />\n\u003c/div>\n\u003cp>In addition to the benefits described above, preregistration may improve the quality of research by encouraging closer attention to study planning. We’ve found that the process of writing a preregistration really helps facilitate communication between collaborators, and can catch addressable problems before time and resources are wasted on a poorly designed study. Detailed advanced planning can also create opportunities for useful community feedback, particularly in the context of Registered Reports, where dedicated peer reviewers will evaluate your study before its even begun (Box 2).\u003c/p>\n\u003cdiv class=\"box examples\">\n\u003cp>(TITLE) Preregistration and friends: A toolbox to address researcher degrees of freedom\u003c/p>\n\u003cp>Several useful tools and concepts that can be used to complement or extend preregistration. In general, we would recommend that these are combined with preregistration, rather than used as a replacement. Preregistration provides transparency about the research and planning process, so its function complements other methods for avoiding bias \u003cspan class=\"citation\">(\u003ca href=\"#ref-hardwicke2021b\" role=\"doc-biblioref\">Hardwicke &amp; Wagenmakers, 2021\u003c/a>)\u003c/span>.\u003c/p>\n\u003cp>\u003cstrong>Robustness checks\u003c/strong>. Robustness checks (also called “sensitivity analyses”) assess how different decision choices in the garden of forking paths affect the eventual pattern of results. This technique is particularly helpful when you have to choose between several justifiable analytic choices, neither of which seem superior to the other, or which have complementary strengths and weaknesses. For example, you might run the analysis three times using three different methods for handling missing data. Robust results should not vary substantially across the three different choices.\u003c/p>\n\u003cp>\u003cstrong>Multiverse analyses\u003c/strong>. Recently, some researchers have started running large-scale robustness checks. These have been called “multiverse analysis” \u003cspan class=\"citation\">(\u003ca href=\"#ref-steegen2016\" role=\"doc-biblioref\">Steegen et al., 2016\u003c/a>)\u003c/span> or “specification curve analysis” \u003cspan class=\"citation\">(\u003ca href=\"#ref-simonsohn2020\" role=\"doc-biblioref\">Simonsohn et al., 2020\u003c/a>)\u003c/span>. These techniques evaluate the factorial intersection of multiple choices for multiple decisions – like simultaneously evaluating thousands of pathways in the garden of forking paths. Some have argued that these large-scale robustness checks can make preregistration redundant; after all, why prespecify a single path if you can explore them all \u003cspan class=\"citation\">(\u003ca href=\"#ref-oberauer2019\" role=\"doc-biblioref\">Oberauer &amp; Lewandowsky, 2019\u003c/a>; \u003ca href=\"#ref-rubin2020\" role=\"doc-biblioref\">Rubin, 2020\u003c/a>)\u003c/span>? But interpreting the results of a multiverse analysis are not straightforward; for example, it seems unlikely that all of the decision choices are equally justifiable \u003cspan class=\"citation\">(\u003ca href=\"#ref-giudice2021\" role=\"doc-biblioref\">Giudice &amp; Gangestad, 2021\u003c/a>)\u003c/span>. Furthermore, if robustness checks are not preregistered, then they introduce researcher degrees of freedom, and create an opportunity for selective reporting, which increases risk of bias.\u003c/p>\n\u003cp>\u003cstrong>Held-out sample\u003c/strong>. One option to benefit from both exploratory and confirmatory research modes is to split your data into \u003cstrong>training\u003c/strong> and \u003cstrong>test\u003c/strong> samples. (The test sample is commonly called the “held out” because it is “held out” from the exploratory process.) You can generate hypotheses in an exploratory mode in the training sample and use that as the basis to preregister confirmatory analyses in the hold-out sample. A notable disadvantage of this strategy is that splitting the data reduces statistical power, but in cases where data are plentiful – including in much of machine learning – this technique is the gold standard.\u003c/p>\n\u003cp>\u003cstrong>Masked analysis\u003c/strong> (sometimes also called “blind analysis”). Sometimes problems, such as missing data, attrition, or randomization failure can arise during data collection that you did not anticipate in your preregistered plan. How do you diagnose and address these issues without increasing risk of bias through results-dependent analysis? One option is masked analysis, which disguises aspects of the data related to the results (for example, by shuffling condition labels or adding noise) while still allowing some degree of data inspection \u003cspan class=\"citation\">(\u003ca href=\"#ref-dutilh2019\" role=\"doc-biblioref\">Dutilh et al., 2019\u003c/a>)\u003c/span>. After diagnosing a problem, you can adjust your preregistered plan without increasing risk of bias, because you have not engaged in results-dependent decision making.\u003c/p>\n\u003cp>\u003cstrong>Standard Operating Procedures\u003c/strong>. Community norms, perhaps at the level of your research field or lab, can act as a natural constraint on researcher degrees of freedom. For example, there may be a generally accepted approach for handling outliers in your community. You can make these constraints explicit by writing them down in a Standard Operating Procedures document - a bit like a living meta-preregistration \u003cspan class=\"citation\">(\u003ca href=\"#ref-lin2016\" role=\"doc-biblioref\">Lin &amp; Green, 2016\u003c/a>)\u003c/span>. Each time you preregister an individual study, you can co-register this document alongside it. Make sure you are clear about which document you will follow in the event of a mismatch!\u003c/p>\n\u003cp>\u003cstrong>Open lab notebooks\u003c/strong>. Maintaining a lab notebook can be a useful way to keep a record of your decisions as a research project unfolds. Preregistration is bit like taking a snapshot of your lab notebook at the start of the project, when all you have written down is your research plan. Making your lab notebook publicly available is a great way to transparently document your research and departures from the preregistered plan.\u003c/p>\n\u003cp>\u003cstrong>Registered Reports\u003c/strong>. Registered Reports are a type of article format that embeds preregistration directly into the publication pipeline [\u003cspan class=\"citation\">Chambers &amp; Tzavella (\u003ca href=\"#ref-chambers2020\" role=\"doc-biblioref\">2020\u003c/a>)\u003c/span>; Figure \u003ca href=\"12-prereg.html#fig:reg-reports\">12.9\u003c/a>]. The idea is that you submit your preregistered protocol to a journal and it is peer reviewed, before you’ve even started your study. If the study is approved, the journal agrees to publish it, regardless of the results. This is a radical departure from traditional publication models where peer reviewers and journals evaluate your study \u003cem>after\u003c/em> its been completed and the results are known. Because the study is accepted for publication independently of the results, Registered Reports can offer the benefits of preregistration with additional protection against publication bias. They also provide a great opportunity to feedback on your study design while you can still change it!\u003c/p>\n\u003c/div>\n\u003cp>\n\u003cspan class=\"marginnote shownote\">\n\u003c!--\n\u003cdiv class=\"figure\">-->\u003cspan style=\"display:block;\" id=\"fig:reg-reports\">\u003c/span>\n\u003cimg src=\"images/prereg/registered-reports.png\" alt=\"Registered Reports (https://www.cos.io/initiatives/registered-reports)\" width=\"\\linewidth\"  />\n\u003c!--\n\u003cp class=\"caption marginnote\">-->Figure 12.9: Registered Reports (\u003ca href=\"https://www.cos.io/initiatives/registered-reports\" class=\"uri\">https://www.cos.io/initiatives/registered-reports\u003c/a>)\u003c!--\u003c/p>-->\n\u003c!--\u003c/div>-->\u003c/span>\n\u003c/p>\n\u003c/div>\n\u003cdiv id=\"how-to-preregister\" class=\"section level2\" number=\"12.3\">\n\u003ch2>\u003cspan class=\"header-section-number\">12.3\u003c/span> How to preregister\u003c/h2>\n\u003cp>High-stakes studies such as medical trials must be preregistered \u003cspan class=\"citation\">(\u003ca href=\"#ref-dickersin2012\" role=\"doc-biblioref\">Dickersin &amp; Rennie, 2012\u003c/a>)\u003c/span>. In 2005, a large international consortium of medical journals decided that they would not publish unregistered trials. And the discipline of economics has strong norms about study registration (see e.g. \u003ca href=\"\">https://www.socialscienceregistry.org\u003c/a>). But preregistration is actually pretty new to psychology \u003cspan class=\"citation\">(\u003ca href=\"#ref-nosek2018\" role=\"doc-biblioref\">B. A. Nosek et al., 2018\u003c/a>)\u003c/span>, and there’s still no standard way of doing it – you’re already at the cutting edge!\u003c/p>\n\u003cp>We recommend using the Open Science Framework (OSF) as your registry. OSF is one of the most popular registries in psychology and you can do lots of other useful things there to make your research transparent, like sharing data, materials, analysis scripts, and preprints. On the OSF it is possible to “register” any file you have uploaded. When you register a file, it creates a timestamped, read-only copy, with a dedicated link. You can add this link to articles reporting your research.\u003c/p>\n\u003cp>One approach to preregistration is to write a protocol document that specifies the study rationale, aims or hypotheses, methods, and analysis plan, and register that document.\u003clabel for=\"tufte-sn-162\" class=\"margin-toggle sidenote-number\">162\u003c/label>\u003cinput type=\"checkbox\" id=\"tufte-sn-162\" class=\"margin-toggle\">\u003cspan class=\"sidenote\">\u003cspan class=\"sidenote-number\">162\u003c/span> You can think of a study protocol a bit like a research paper without a results and discussion section (here’s an example from one of our own studies: \u003ca href=\"https://osf.io/2cnkq/\" class=\"uri\">https://osf.io/2cnkq/\u003c/a>).\u003c/span> The OSF also has a collection of dedicated preregistration templates that you can use if you prefer. These templates are often tailored to the needs of particular types of research. For example, there are templates for general quantitative psychology research \u003cspan class=\"citation\">(“PRP-QUANT” \u003ca href=\"#ref-bosnjak2021\" role=\"doc-biblioref\">Bosnjak et al., 2021\u003c/a>)\u003c/span>, cognitive modelling \u003cspan class=\"citation\">(\u003ca href=\"#ref-cruwell2021\" role=\"doc-biblioref\">Crüwell &amp; Evans, 2021\u003c/a>)\u003c/span>, and secondary data analysis \u003cspan class=\"citation\">(\u003ca href=\"#ref-akker2019\" role=\"doc-biblioref\">Akker et al., 2019\u003c/a>)\u003c/span>. The OSF interface may change, but currently \u003ca href=\"https://help.osf.io/hc/en-us/articles/360019738834-Create-a-Preregistration\">this guide\u003c/a> provides a set of steps to create a preregistration.\u003c/p>\n\u003cp>Once you’ve preregistered your plan, you just go off and run the study and report the results, right? Well hopefully… but things might not turn out to be that straightforward. It’s quite common to forget to include something in your plan or to have to depart from the plan due to something unexpected. Preregistration can actually be pretty hard in practice \u003cspan class=\"citation\">(\u003ca href=\"#ref-nosek2019\" role=\"doc-biblioref\">B. A. Nosek et al., 2019\u003c/a>)\u003c/span>!\u003c/p>\n\u003cp>Don’t worry though - remember that the primary goal of preregistration is transparency to enable others to evaluate and interpret our work. If you decide to depart from your original plan and conduct results-dependent analyses, then this decision may increase the risk of bias. But if you communicate this decision transparently to your readers, they can appropriately calibrate their confidence in the results. You may even be able to run both the planned and unplanned analyses as a robustness check (see Box) to evaluate the extent to which this choice impacts the results.\u003c/p>\n\u003cp>When you report your study, it is important to distinguish between what was planned and what was not. If you ran a lot of results-dependent analyses, then it might be worth having separate exploratory and confirmatory results sections. On the other hand, if you mainly stuck to your original plan, with only minor departures, then you could include a table (perhaps in an appendix) that outlines these changes (for example, see Supplementary Information A of \u003ca href=\"https://doi.org/10.31222/osf.io/wt5ny\">this article\u003c/a>).\u003c/p>\n\u003c/div>\n\u003cdiv id=\"chapter-summary-preregistration\" class=\"section level2\" number=\"12.4\">\n\u003ch2>\u003cspan class=\"header-section-number\">12.4\u003c/span> Chapter summary: Preregistration\u003c/h2>\n\u003cp>We’ve advocated here for preregistering your planned analyses. This practice allows us to minimize bias caused by results-dependent analysis (the “garden of forking paths” that we described). Preregistration is a “\u003ca href=\"https://www.cos.io/blog/preregistration-plan-not-prison\">plan, not a prison\u003c/a>”: in most cases preregistered, confirmatory analyses coexist with exploratory analyses. Both are an important part of good research – the key is simply to disclose which is which!\u003c/p>\n\u003cdiv class=\"box exercises\">\n\u003col style=\"list-style-type: decimal\">\n\u003cli>\u003cp>P-hack your way to scientific glory! To get a feel for how results-dependent analyses might work in practice, have a play around with this app: \u003ca href=\"https://projects.fivethirtyeight.com/p-hacking/\" class=\"uri\">https://projects.fivethirtyeight.com/p-hacking/\u003c/a>\u003c/p>\u003c/li>\n\u003cli>\u003cp>Preregister your next experiment! The best way to get started with preregistration is to have a go with your next study. Head over to \u003ca href=\"https://osf.io/registries/osf/new\" class=\"uri\">https://osf.io/registries/osf/new\u003c/a> and register your study protocol or complete one of the templates.\u003c/p>\u003c/li>\n\u003c/ol>\n\u003c/div>\n\n\u003c/div>\n\u003c/div>\n\u003ch3>References\u003c/h3>\n\u003cdiv id=\"refs\" class=\"references csl-bib-body hanging-indent\" line-spacing=\"2\">\n\u003cdiv id=\"ref-akker2019\" class=\"csl-entry\">\nAkker, O. van den, Weston, S. J., Campbell, L., Chopik, W. J., Damian, R. I., Davis-Kean, P., Hall, A., Kosie, J., Kruse, E., Olsen, J., Ritchie, S. J., Valentine, K. D., Veer, A. van ’t., &amp; Bakker, M. (2019). \u003cem>Preregistration of secondary data analysis: \u003cspan>A\u003c/span> template and tutorial\u003c/em>. PsyArXiv. \u003ca href=\"https://psyarxiv.com/hvfmr/\">https://psyarxiv.com/hvfmr/\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-bakker2012\" class=\"csl-entry\">\nBakker, M., Dijk, A. van, &amp; Wicherts, J. M. (2012). The rules of the game called psychological science. \u003cem>Perspectives on Psychological Science\u003c/em>, \u003cem>7\u003c/em>(6), 543–554. \u003ca href=\"https://doi.org/10.1177/1745691612459060\">https://doi.org/10.1177/1745691612459060\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-barber1976\" class=\"csl-entry\">\nBarber, T. X. (1976). \u003cem>Pitfalls in \u003cspan>Human\u003c/span> \u003cspan>Research\u003c/span>: \u003cspan>Ten\u003c/span> \u003cspan>Pivotal\u003c/span> \u003cspan>Points\u003c/span>\u003c/em>. Pergamon Press.\n\u003c/div>\n\u003cdiv id=\"ref-bennett2009\" class=\"csl-entry\">\nBennett, C., Miller, M., &amp; Wolford, G. (2009). Neural correlates of interspecies perspective taking in the post-mortem \u003cspan>Atlantic\u003c/span> \u003cspan>Salmon\u003c/span>: An argument for multiple comparisons correction. \u003cem>NeuroImage\u003c/em>, \u003cem>47\u003c/em>, S125. \u003ca href=\"https://doi.org/10.1016/S1053-8119(09)71202-9\">https://doi.org/10.1016/S1053-8119(09)71202-9\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-berkowitz2015\" class=\"csl-entry\">\nBerkowitz, T., Schaeffer, M. W., Maloney, E. A., Peterson, L., Gregor, C., Levine, S. C., &amp; Beilock, S. L. (2015). Math at home adds up to achievement in school. \u003cem>Science\u003c/em>, \u003cem>350\u003c/em>(6257), 196–198. \u003ca href=\"https://doi.org/10.1126/science.aac7427\">https://doi.org/10.1126/science.aac7427\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-berkowitz2016\" class=\"csl-entry\">\nBerkowitz, T., Schaeffer, M. W., Rozek, C. S., Maloney, E. A., Levine, S. C., &amp; Beilock, S. L. (2016). Response to comment on \u003cspan>“math at home adds up to achievement in school.”\u003c/span> \u003cem>Science\u003c/em>, \u003cem>351\u003c/em>(6278), 1161–1161.\n\u003c/div>\n\u003cdiv id=\"ref-bosnjak2021\" class=\"csl-entry\">\nBosnjak, M., Fiebach, C., Mellor, D. T., Mueller, S., O’Connor, D., Oswald, F., &amp; Sokol-Chang, R. (2021). \u003cem>A template for preregistration of quantitative research in psychology: Report of the joint psychological societies preregistration task force\u003c/em>. PsyArXiv. \u003ca href=\"https://doi.org/10.31234/osf.io/d7m5r\">https://doi.org/10.31234/osf.io/d7m5r\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-chambers2020\" class=\"csl-entry\">\nChambers, C., &amp; Tzavella, L. (2020). \u003cem>Registered \u003cspan>Reports\u003c/span>: Past, present and future\u003c/em>. MetaArXiv. \u003ca href=\"https://doi.org/10.31222/osf.io/43298\">https://doi.org/10.31222/osf.io/43298\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-cruwell2021\" class=\"csl-entry\">\nCrüwell, S., &amp; Evans, N. J. (2021). Preregistration in diverse contexts: A preregistration template for the application of cognitive models. \u003cem>Royal Society Open Science\u003c/em>, \u003cem>8\u003c/em>(10), 210155. \u003ca href=\"https://doi.org/10.1098/rsos.210155\">https://doi.org/10.1098/rsos.210155\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-degroot2014\" class=\"csl-entry\">\nde Groot, A. D. (1956/2014). The meaning of \u003cspan>“significance”\u003c/span> for different types of research (E.-J. Wagenmakers, D. Borsboom, J. Verhagen, R. A. Kievit, M. Bakker, A. O. J. Cramer, D. Matzke, D. Mellenbergh, &amp; H. L. J. van der Maas, Trans.). \u003cem>Acta Psychologica\u003c/em>, \u003cem>148\u003c/em>, 188–194. \u003ca href=\"https://doi.org/10.1016/j.actpsy.2014.02.001\">https://doi.org/10.1016/j.actpsy.2014.02.001\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-dickersin2012\" class=\"csl-entry\">\nDickersin, K., &amp; Rennie, D. (2012). The evolution of trial registries and their use to assess the clinical trial enterprise. \u003cem>JAMA\u003c/em>, \u003cem>307\u003c/em>(17), 1861–1864. \u003ca href=\"https://doi.org/10.1001/jama.2012.4230\">https://doi.org/10.1001/jama.2012.4230\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-dutilh2019\" class=\"csl-entry\">\nDutilh, G., Sarafoglou, A., &amp; Wagenmakers, E.-J. (2019). Flexible yet fair: Blinding analyses in experimental psychology. \u003cem>Synthese\u003c/em>. https://doi.org/\u003ca href=\"https://doi.org/10.1007/s11229-019-02456-7\">https://doi.org/10.1007/s11229-019-02456-7\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-feynman1974\" class=\"csl-entry\">\nFeynman, R. P. (1974). \u003cem>Cargo \u003cspan>Cult\u003c/span> \u003cspan>Science\u003c/span>\u003c/em>. \u003ca href=\"http://calteches.library.caltech.edu/51/2/CargoCult.pdf\">http://calteches.library.caltech.edu/51/2/CargoCult.pdf\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-frank2016\" class=\"csl-entry\">\nFrank, M. C. (2016). Comment on \u003cspan>“math at home adds up to achievement in school.”\u003c/span> In \u003cem>Science\u003c/em> (No. 6278; Vol. 351, pp. 1161.2–1161).\n\u003c/div>\n\u003cdiv id=\"ref-gelman2014\" class=\"csl-entry\">\nGelman, A., &amp; Loken, E. (2014). The statistical crisis in science. \u003cem>American Scientist\u003c/em>, \u003cem>102\u003c/em>(6), 460–465. \u003ca href=\"https://doi.org/10.1511/2014.111.460\">https://doi.org/10.1511/2014.111.460\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-gelman2006\" class=\"csl-entry\">\nGelman, A., &amp; Stern, H. (2006). The \u003cspan>Difference\u003c/span> \u003cspan>Between\u003c/span> \u003cspan>“\u003cspan>Significant\u003c/span>”\u003c/span> and \u003cspan>“\u003cspan>Not\u003c/span> \u003cspan>Significant\u003c/span>”\u003c/span> is not \u003cspan>Itself\u003c/span> \u003cspan>Statistically\u003c/span> \u003cspan>Significant\u003c/span>. \u003cem>The American Statistician\u003c/em>, \u003cem>60\u003c/em>(4), 328–331. \u003ca href=\"https://doi.org/10.1198/000313006X152649\">https://doi.org/10.1198/000313006X152649\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-gilovich1985\" class=\"csl-entry\">\nGilovich, T., Vallone, R., &amp; Tversky, A. (1985). The hot hand in basketball: \u003cspan>On\u003c/span> the misperception of random sequences. \u003cem>Cognitive Psychology\u003c/em>, \u003cem>17\u003c/em>(3), 295–314. \u003ca href=\"https://doi.org/10.1016/0010-0285(85)90010-6\">https://doi.org/10.1016/0010-0285(85)90010-6\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-giner-sorolla2012\" class=\"csl-entry\">\nGiner-Sorolla, R. (2012). Science or art? \u003cspan>How\u003c/span> aesthetic standards grease the way through the publication bottleneck but undermine science. \u003cem>Perspectives on Psychological Science\u003c/em>, \u003cem>7\u003c/em>(6), 562–571. \u003ca href=\"https://doi.org/10.1177/1745691612457576\">https://doi.org/10.1177/1745691612457576\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-giudice2021\" class=\"csl-entry\">\nGiudice, M. D., &amp; Gangestad, S. (2021). A traveler’s guide to the multiverse: Promises, pitfalls, and a framework for the evaluation of analytic decisions. \u003cem>Advances in Methods and Practices in Psychological Science\u003c/em>, \u003cem>4\u003c/em>(1), 1–15. https://doi.org/\u003ca href=\"https://doi.org/10.1177/2515245920954925\">https://doi.org/10.1177/2515245920954925\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-good1972\" class=\"csl-entry\">\nGood, I. J. (1972). Statistics and \u003cspan>Today\u003c/span>’s \u003cspan>Problems\u003c/span>. \u003cem>The American Statistician\u003c/em>, \u003cem>26\u003c/em>(3), 11–19. \u003ca href=\"https://doi.org/10.1080/00031305.1972.10478922\">https://doi.org/10.1080/00031305.1972.10478922\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-hardwicke2021b\" class=\"csl-entry\">\nHardwicke, T. E., &amp; Wagenmakers, E.-J. (2021). \u003cem>Preregistration: \u003cspan>A\u003c/span> pragmatic tool to reduce bias and calibrate confidence in scientific research\u003c/em>. MetaArXiv. \u003ca href=\"https://doi.org/10.31222/osf.io/d7bcu\">https://doi.org/10.31222/osf.io/d7bcu\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-hoekstra2020\" class=\"csl-entry\">\nHoekstra, R., &amp; Vazire, S. (2020). \u003cem>Intellectual humility is central to science\u003c/em> [Preprint]. \u003ca href=\"https://osf.io/edh2s\">https://osf.io/edh2s\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-ioannidis2005\" class=\"csl-entry\">\nIoannidis, J. P. A. (2005). Why most published research findings are false. \u003cem>PLOS Medicine\u003c/em>, \u003cem>2\u003c/em>(8), e124. \u003ca href=\"https://doi.org/10.1371/journal.pmed.0020124\">https://doi.org/10.1371/journal.pmed.0020124\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-kerr1998\" class=\"csl-entry\">\nKerr, N. L. (1998). \u003cspan>HARKing\u003c/span>: \u003cspan>Hypothesizing\u003c/span> \u003cspan>After\u003c/span> the \u003cspan>Results\u003c/span> are \u003cspan>Known\u003c/span>. \u003cem>Personality &amp; Social Psychology Review (Lawrence Erlbaum Associates)\u003c/em>, \u003cem>2\u003c/em>(3), 196. \u003ca href=\"https://doi.org/10.1207/s15327957pspr0203_4\">https://doi.org/10.1207/s15327957pspr0203_4\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-kunda1990\" class=\"csl-entry\">\nKunda, Z. (1990). The case for motivated reasoning. \u003cem>Psychological Bulletin\u003c/em>, \u003cem>108\u003c/em>(3), 480–498. \u003ca href=\"https://doi.org/10.1037/0033-2909.108.3.480\">https://doi.org/10.1037/0033-2909.108.3.480\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-lin2016\" class=\"csl-entry\">\nLin, W., &amp; Green, D. P. (2016). Standard operating procedures: A safety net for pre-analysis plans. \u003cem>PS: Political Science &amp; Politics\u003c/em>, \u003cem>49\u003c/em>(03), 495–500. \u003ca href=\"https://doi.org/10.1017/S1049096516000810\">https://doi.org/10.1017/S1049096516000810\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-nickerson1998\" class=\"csl-entry\">\nNickerson, R. S. (1998). Confirmation bias: A ubiquitous phenomenon in many guises. \u003cem>Review of General Psychology\u003c/em>, \u003cem>2\u003c/em>(2), 175–220. \u003ca href=\"https://doi.org/10.1037/1089-2680.2.2.175\">https://doi.org/10.1037/1089-2680.2.2.175\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-nosek2019\" class=\"csl-entry\">\nNosek, B. A., Beck, E. D., Campbell, L., Flake, J. K., Hardwicke, T. E., Mellor, D. T., Veer, A. E. van ’t, &amp; Vazire, S. (2019). Preregistration is hard, and worthwhile. \u003cem>Trends in Cognitive Sciences\u003c/em>, \u003cem>23\u003c/em>(10), 815–818. \u003ca href=\"https://doi.org/10.1016/j.tics.2019.07.009\">https://doi.org/10.1016/j.tics.2019.07.009\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-nosek2018\" class=\"csl-entry\">\nNosek, B. A., Ebersole, C. R., DeHaven, A. C., &amp; Mellor, D. T. (2018). The preregistration revolution. \u003cem>Proceedings of the National Academy of Sciences\u003c/em>, \u003cem>115\u003c/em>(11), 2600–2606. \u003ca href=\"https://doi.org/10.1073/pnas.1708274114\">https://doi.org/10.1073/pnas.1708274114\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-nosek2012\" class=\"csl-entry\">\nNosek, B. A., Spies, J. R., &amp; Motyl, M. (2012). Scientific \u003cspan>Utopia\u003c/span>: \u003cspan>II\u003c/span>. Restructuring incentives and practices to promote truth over publishability. \u003cem>Perspectives on Psychological Science\u003c/em>, \u003cem>7\u003c/em>(6), 615–631. \u003ca href=\"https://doi.org/10.1177/1745691612459058\">https://doi.org/10.1177/1745691612459058\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-oboyle2017\" class=\"csl-entry\">\nO’Boyle, E. H., Banks, G. C., &amp; Gonzalez-Mulé, E. (2017). The chrysalis effect: How ugly initial results metamorphosize into beautiful articles. \u003cem>Journal of Management\u003c/em>, \u003cem>43\u003c/em>(2), 376–399. \u003ca href=\"https://doi.org/10.1177/0149206314527133\">https://doi.org/10.1177/0149206314527133\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-oberauer2019\" class=\"csl-entry\">\nOberauer, K., &amp; Lewandowsky, S. (2019). Addressing the theory crisis in psychology. \u003cem>Psychonomic Bulletin &amp; Review\u003c/em>, \u003cem>26\u003c/em>(5), 1596–1618.\n\u003c/div>\n\u003cdiv id=\"ref-rubin2020\" class=\"csl-entry\">\nRubin, M. (2020). Does preregistration improve the credibility of research findings? \u003cem>The Quantitative Methods for Psychology\u003c/em>, \u003cem>16\u003c/em>(4), 15. \u003ca href=\"https://doi.org/10.20982/tqmp.16.4.p376\">https://doi.org/10.20982/tqmp.16.4.p376\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-simonsohn2020\" class=\"csl-entry\">\nSimonsohn, U., Simmons, J. P., &amp; Nelson, L. D. (2020). Specification curve analysis. \u003cem>Nature Human Behaviour\u003c/em>, 1–7. \u003ca href=\"https://doi.org/10.1038/s41562-020-0912-z\">https://doi.org/10.1038/s41562-020-0912-z\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-slovic1977\" class=\"csl-entry\">\nSlovic, P., &amp; Fischhoff, B. (1977). On the psychology of experimental surprises. \u003cem>Journal of Experimental Psychology: Human Perception and Performance\u003c/em>, \u003cem>3\u003c/em>(4), 544–551. \u003ca href=\"https://doi.org/10.1037/0096-1523.3.4.544\">https://doi.org/10.1037/0096-1523.3.4.544\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-smaldino2016\" class=\"csl-entry\">\nSmaldino, Paul E., &amp; McElreath, R. (2016). The natural selection of bad science. \u003cem>Royal Society Open Science\u003c/em>, \u003cem>3\u003c/em>(9), 160384. \u003ca href=\"https://doi.org/10.1098/rsos.160384\">https://doi.org/10.1098/rsos.160384\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-steegen2016\" class=\"csl-entry\">\nSteegen, S., Tuerlinckx, F., Gelman, A., &amp; Vanpaemel, W. (2016). Increasing transparency through a multiverse analysis. \u003cem>Perspectives on Psychological Science\u003c/em>, \u003cem>11\u003c/em>(5), 702–712. \u003ca href=\"https://doi.org/10.1177/1745691616658637\">https://doi.org/10.1177/1745691616658637\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-tukey1980\" class=\"csl-entry\">\nTukey, John W. (1980). We need both exploratory and confirmatory. \u003cem>The American Statistician\u003c/em>, \u003cem>34\u003c/em>(1), 23–25. \u003ca href=\"https://doi.org/10.2307/2682991\">https://doi.org/10.2307/2682991\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-veldkamp2017\" class=\"csl-entry\">\nVeldkamp, C. L. S., Hartgerink, C. H. J., Assen, M. A. L. M. van, &amp; Wicherts, J. M. (2017). Who believes in the storybook image of the scientist? \u003cem>Accountability in Research\u003c/em>, \u003cem>24\u003c/em>(3), 127–151. \u003ca href=\"https://doi.org/10.1080/08989621.2016.1268922\">https://doi.org/10.1080/08989621.2016.1268922\u003c/a>\n\u003c/div>\n\u003cdiv id=\"ref-wagenmakers2012\" class=\"csl-entry\">\nWagenmakers, E.-J., Wetzels, R., Borsboom, D., Maas, H. L. J. van der, &amp; Kievit, R. A. (2012). An agenda for purely confirmatory research. \u003cem>Perspectives on Psychological Science\u003c/em>, \u003cem>7\u003c/em>(6), 632–638. \u003ca href=\"https://doi.org/10.1177/1745691612463078\">https://doi.org/10.1177/1745691612463078\u003c/a>\n\u003c/div>\n\u003c/div>\n\u003cp style=\"text-align: center;\">\n\u003ca href=\"11-strategy.html\">\u003cbutton class=\"btn btn-default\">Previous\u003c/button>\u003c/a>\n\u003ca href=\"13-consent.html\">\u003cbutton class=\"btn btn-default\">Next\u003c/button>\u003c/a>\n\u003c/p>\n\u003c/div>\n\u003c/div>\n\n\n\n"}}</script></body>

</html>