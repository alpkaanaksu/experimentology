<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 10 Preregistration | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />



<meta name="description" content="Chapter 10 Preregistration | Experimentology">

<title>Chapter 10 Preregistration | Experimentology</title>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />




<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="toc/toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-intro.html#intro"><span class="toc-section-number">1</span> Experiments and theories</a></li>
<li><a href="2-replication.html#replication"><span class="toc-section-number">2</span> Replication and reproducibility</a></li>
<li><a href="3-ethics.html#ethics"><span class="toc-section-number">3</span> Ethics</a></li>
<li class="part"><span><b>II Statistics</b></span></li>
<li><a href="4-estimation.html#estimation"><span class="toc-section-number">4</span> Estimation</a></li>
<li><a href="5-inference.html#inference"><span class="toc-section-number">5</span> Inference</a></li>
<li><a href="6-models.html#models"><span class="toc-section-number">6</span> Models</a></li>
<li class="part"><span><b>III Design and Planning</b></span></li>
<li><a href="7-measurement.html#measurement"><span class="toc-section-number">7</span> Measurement</a></li>
<li><a href="8-design.html#design"><span class="toc-section-number">8</span> Design of experiments</a></li>
<li><a href="9-sampling.html#sampling"><span class="toc-section-number">9</span> Sampling</a></li>
<li><a href="10-prereg.html#prereg"><span class="toc-section-number">10</span> Preregistration</a></li>
<li class="part"><span><b>IV Execution</b></span></li>
<li><a href="11-selection.html#selection"><span class="toc-section-number">11</span> Experimental strategy</a></li>
<li><a href="12-collection.html#collection"><span class="toc-section-number">12</span> Data collection</a></li>
<li><a href="13-management.html#management"><span class="toc-section-number">13</span> Project management</a></li>
<li class="part"><span><b>V Analysis and Reporting</b></span></li>
<li><a href="14-viz.html#viz"><span class="toc-section-number">14</span> Visualization</a></li>
<li><a href="15-eda.html#eda"><span class="toc-section-number">15</span> Exploratory data analysis</a></li>
<li><a href="16-writing.html#writing"><span class="toc-section-number">16</span> Reproducible writing</a></li>
<li><a href="17-meta.html#meta"><span class="toc-section-number">17</span> Meta-analysis</a></li>
<li><a href="18-conclusions.html#conclusions"><span class="toc-section-number">18</span> Conclusions</a></li>
<li class="part"><span><b>VI Appendices</b></span></li>
<li><a href="19-git.html#git"><span class="toc-section-number">19</span> GitHub Tutorial</a></li>
<li><a href="20-rmarkdown.html#rmarkdown"><span class="toc-section-number">20</span> R Markdown Tutorial</a></li>
<li><a href="21-tidyverse.html#tidyverse"><span class="toc-section-number">21</span> Tidyverse Tutorial</a></li>
<li><a href="22-ggplot.html#ggplot"><span class="toc-section-number">22</span> ggplot Tutorial</a></li>
<li><a href="23-instructors.html#instructors"><span class="toc-section-number">23</span> Instructor‚Äôs guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="prereg" class="section level1" number="10">
<h1><span class="header-section-number">Chapter 10</span> Preregistration</h1>
<div class="learning-goals">
<p>üçé Learning goals:</p>
<ul>
<li>Recognize the dangers of researcher degrees of freedom in ‚Äúthe garden of forking paths‚Äù</li>
<li>Understand the differences between exploratory and confirmatory modes of research</li>
<li>Learn how preregistration and other tools can reduce risk of bias and help others to evaluate your work by increasing transparency</li>
</ul>
</div>
<blockquote>
<p>When not planned beforehand, data analysis can approximate a projective technique, such as the Rorschach, because the investigator can project on the data his own expectancies, desires, or biases and can pull out of the data almost any ‚Äòfinding‚Äô he may desire.</p>
<footer>
‚Äî Theodore X. Barber <span class="citation">(<a href="#ref-barber1976" role="doc-biblioref">1976</a>)</span>
</footer>
</blockquote>
<blockquote>
<p>The first principle is that you must not fool yourself‚Äìand you are the easiest person to fool‚Ä¶After you‚Äôve not fooled yourself, it‚Äôs easy not to fool other scientists. You just have to be honest in a conventional way after that.</p>
<footer>
‚Äî Richard Feynman <span class="citation">(<a href="#ref-feynman1974" role="doc-biblioref">1974</a>)</span>
</footer>
</blockquote>
<p>This may surprise you coming from the authors of a textbook about research methods, but there is no single ‚Äúcorrect‚Äù way to design and analyze an experiment<label for="tufte-sn-1" class="margin-toggle sidenote-number">1</label><input type="checkbox" id="tufte-sn-1" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">1</span> Though there are plenty of incorrect ways to design and analyse experiments and we hope we can help you to avoid these!</span>. In fact, for most research decisions, there are many justifiable options. For example, will you stop data collection after 20, 200, or 2000 participants? Will you remove outlier values and how will you define them? Will you conduct subgroup analyses to see whether the results are affected by sex, or age, or some other factor? Consider a simplified, hypothetical case where you need to make five analysis decisions and have five justifiable options for each decision ‚Äî this alone would result in 3125 (5^5) unique ways to analyze your data! In this chapter, we will find out why undisclosed flexibility in the design, analysis, reporting, and interpretation of experiments (also referred to as ‚Äúresearcher degrees of freedom‚Äù), can lead to scientists fooling themselves and fooling each other. We will also learn about how preregistration (and other tools) can be used to protect our research from bias and provide the transparency that other scientists need to properly evaluate and interpret our work.</p>
<div id="lost-in-a-garden-of-forking-paths" class="section level2" number="10.1">
<h2><span class="header-section-number">10.1</span> Lost in a garden of forking paths</h2>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:forking-paths"></span>
<img src="images/prereg/forking-paths.png" alt="Garden of forking paths (placeholder image I hacked together, replace with an illustration?)" width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 10.1: Garden of forking paths (placeholder image I hacked together, replace with an illustration?)<!--</p>-->
<!--</div>--></span>
</p>
<p>One way to visualize researcher degrees of freedom is as a vast decision tree or ‚Äúgarden of forking paths‚Äù <span class="citation">(<a href="#ref-gelman2014" role="doc-biblioref">Gelman &amp; Loken, 2014</a> Figure <a href="10-prereg.html#fig:forking-paths">10.1</a>)</span>. Each node represents a decision point and each branch represents a justifiable choice. Each unique pathway through the garden terminates in an individual result. Because scientific observations typically consist of both noise (random variation unique to this sample) and signal (regularities that will reoccur in other samples), some of these pathways will inevitably lead to results that are misleading (e.g., inflated effect sizes, exaggerated evidence, or false positives)<label for="tufte-sn-2" class="margin-toggle sidenote-number">2</label><input type="checkbox" id="tufte-sn-2" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">2</span> The signal-to-noise ratio is worse in situations (alas, common in psychology) that involve small effect sizes, high variation, and large measurement errors <span class="citation">(<a href="#ref-ioannidis2005" role="doc-biblioref">Ioannidis, 2005</a>)</span>. Researcher degrees of freedom may be constrained to some extent by strong theory <span class="citation">(<a href="#ref-oberauer2019" role="doc-biblioref">Oberauer &amp; Lewandowsky, 2019</a>)</span>, community methodological norms and standards, or replication studies, though these constraints may be more implicit than explicit, and can still leave plenty of room for flexible decision-making.</span>. The more potential paths there are in the garden that you might explore, the higher the chance of encountering misleading results<label for="tufte-sn-3" class="margin-toggle sidenote-number">3</label><input type="checkbox" id="tufte-sn-3" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">3</span> In frequentist terminology, there is an increasing the chance of making a ‚ÄòType I error.‚Äô</span>. Statisticians refer to this as a <em>multiplicity</em> problem.</p>
<p>As noted in Chapter <a href="5-inference.html#inference">5</a>, multiplicity can be addressed to some extent with statistical countermeasures, like the Bonferroni correction; however, these adjustment methods need to account for every path that you <em>could have</em> taken <span class="citation">(<a href="#ref-gelman2014" role="doc-biblioref">Gelman &amp; Loken, 2014</a>; <a href="#ref-degroot2014" role="doc-biblioref">Groot, 2014</a>)</span>. When you navigate the garden of forking paths <em>during data analysis</em>, it is easy to forget, or even be unaware of every path that you could have taken, so these methods can longer be used effectively. Additionally, when a researcher navigates the garden of forking paths during data analysis, their decisions can be biased because they are receiving feedback on how different choices affect the results (<em>results-dependent</em> decision making). If a researcher is seeking a particular kind of result (which is likely - see Box 1), then they are more likely to follow the branches that steer them in that direction. You could think of this a bit like playing a game of ‚Äúhotüî•! or cold‚òÉÔ∏è!‚Äù where hotüî•! indicates that the choice will move the researcher closer to a desirable overall result and cold‚òÉÔ∏è! indicates that the choice will move them further away. Each time the researcher reaches a decision point, they try one of the branches and get feedback on how that choice affects the results. If the feedback is hotüî•! then they take that branch. If the answer is cold‚òÉÔ∏è!, they try a different branch. If they reach the end of a complete pathway, and the results are cold‚òÉÔ∏è!, maybe they even retrace their steps and try some different branches earlier in the pathway. This strategy create a risk of bias<label for="tufte-sn-4" class="margin-toggle sidenote-number">4</label><input type="checkbox" id="tufte-sn-4" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">4</span> We say ‚Äúrisk of bias‚Äù rather than just ‚Äúbias‚Äù because in most scientific contexts, we do not have a known ground truth to compare the results to. So in any specific situation, we do not know the extent to which results-dependent analyses have actually biased the results.</span> because the results are being systematically skewed towards the researcher‚Äôs preferences<label for="tufte-sn-5" class="margin-toggle sidenote-number">5</label><input type="checkbox" id="tufte-sn-5" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">5</span> Another way to think of this is in terms of ‚Äòregression to the mean.‚Äô When a sample statistic is selected because it crosses some threshold (e.g., statistical significance), then it is more likely to provide a biased estimate that decreases upon subsequent measurement.</span> <span class="citation">(<a href="#ref-hardwicke2021b" role="doc-biblioref">Tom E. Hardwicke &amp; Wagenmakers, 2021</a>)</span>.</p>
<div class="interactive">
<p>Box 1. Only human: Cognitive biases and skewed incentives</p>
<p>There‚Äôs a storybook image of the scientist as an objective, rationale, and dispassionate arbiter of truth <span class="citation">(<a href="#ref-veldkamp2017" role="doc-biblioref">Veldkamp et al., 2017</a>)</span>. But in reality, scientists are only human: they have egos, career ambitions, and rent to pay! So even if we do want to live up to the storybook image, its important to acknowledge that our decisions and behaviour are also influenced by a range of cognitive biases and external incentives that can steer us away from that goal. Unfortunately, the allocation of funding, awards, and publication prestige is often based on the nature of research results rather than research quality <span class="citation">(<a href="#ref-nosek2012" role="doc-biblioref">Nosek et al., 2012</a>; <a href="#ref-smaldino2016" role="doc-biblioref">Smaldino &amp; McElreath, 2016</a>)</span>. For example, many academic journals, especially those that are widely considered to be the most prestigious, appear to have a preference for novel, positive, and ‚Äòstatistically significant‚Äô findings over incremental, negative, or null findings <span class="citation">(<a href="#ref-bakker2012" role="doc-biblioref">Bakker et al., 2012</a>)</span>. There is also pressure to write articles with concise, coherent, and compelling narratives <span class="citation">(<a href="#ref-giner-sorolla2012" role="doc-biblioref">Giner-Sorolla, 2012</a>)</span>. This incentivizes scientists to be ‚Äòimpressive‚Äô over being right and encourages questionable research practices. The process of iteratively p-hacking and HARKing one‚Äôs way to ‚Äòbeautiful‚Äô scientific paper has been dubbed ‚ÄúThe Chrysalis Effect‚Äù <span class="citation">(<a href="#ref-oboyle2017" role="doc-biblioref">O‚ÄôBoyle et al., 2017</a> Figure <a href="10-prereg.html#fig:chrysalis">10.2</a>)</span>.</p>
<div class="figure"><span id="fig:chrysalis"></span>
<p class="caption marginnote shownote">
Figure 10.2: The Chrysalis Effect, when ugly truth becomes a beautiful fiction. Placeholder image - replace with illustration?
</p>
<img src="images/prereg/chrysalis.png" alt="The Chrysalis Effect, when ugly truth becomes a beautiful fiction. Placeholder image - replace with illustration?" width="\linewidth"  />
</div>
</div>
<p>In the most egregious cases, a researcher may try multiple pathways until they obtain a desirable result<label for="tufte-sn-6" class="margin-toggle sidenote-number">6</label><input type="checkbox" id="tufte-sn-6" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">6</span> ‚ÄúIf you torture the data long enough, it will confess‚Äù <span class="citation">(<a href="#ref-good1972" role="doc-biblioref">Good, 1972</a>)</span></span> and then <em>selectively report</em> that result, neglecting to mention that they have tried several other analysis strategies. You may remember an example of this when participants apparently became younger when they listened to ‚ÄúWhen I‚Äôm 64‚Äù by The Beatles in Chapter <a href="3-ethics.html#ethics">3</a>. Another nice example is when a group of enterprising researchers were able to ‚Äòdiscover‚Äô brain activity in a dead Atlantic Salmon by deliberately exploiting flexibility in the fMRI analysis pipeline <span class="citation">(<a href="#ref-bennett2009" role="doc-biblioref">Bennett et al., 2009</a> Figure <a href="10-prereg.html#fig:salmon">10.3</a>)</span>. Deliberately taking advantage of researcher degrees of freedom and selectively reporting results is known by various names, like p-hacking, cherry picking, data dredging, and it is unethical because it involves hiding highly relevant information. But you should also be aware that results-dependent analysis incurs a risk of bias even if a researcher has good intentions, doesn‚Äôt explicitly try multiple pathways, and honestly reports everything they did. For example, if each branch they took was hotüî•!, they may reach the result they desire at the end of the pathway without realizing that, had the results been different, they would have followed other pathways <span class="citation">(<a href="#ref-gelman2014" role="doc-biblioref">Gelman &amp; Loken, 2014</a>; <a href="#ref-degroot2014" role="doc-biblioref">Groot, 2014</a>)</span>. In other words, even though the researcher doesn‚Äôt intend to deliberately hide anything, there is still undisclosed analytic flexibility - important context that is relevant to properly interpret the results. Its surprisingly easy to convince yourself after the fact that you made the decisions you did for principled reasons that had nothing to do with the results (see ‚Äòmotivated reasoning,‚Äô Box 1). In sum, engaging in results-dependent analysis increases the chances that you will fool yourself by inadvertently stumbling across misleading results - and if that analytic flexibility goes undisclosed, you may fool others too.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:salmon"></span>
<img src="images/prereg/salmon.jpeg" alt="By deliberately exploiting analytic flexibility in the processing pipeline of fMRI data, Bennet et al. (2009) were able to identify 'brain activity' in a dead Atlantic Salmon." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 10.3: By deliberately exploiting analytic flexibility in the processing pipeline of fMRI data, Bennet et al.¬†(2009) were able to identify ‚Äòbrain activity‚Äô in a dead Atlantic Salmon.<!--</p>-->
<!--</div>--></span>
</p>
<p>An important factor that we‚Äôve not yet mentioned in detail, is that there is additional flexibility in how researchers <em>explain</em> research results. Any single result can be consistent with multiple different theories (this is known as the Duhem-Quine problem <span class="citation"><a href="#ref-duhem1954" role="doc-biblioref">Duhem</a> (<a href="#ref-duhem1954" role="doc-biblioref">1954</a>)</span>). We might call these ‚Äúexplanatory degrees of freedom.‚Äù The practice of selecting or developing your hypothesis after seeing the study results has been called ‚ÄúHypotheisizing After the Results are Known,‚Äù or ‚ÄúHARKing‚Äù <span class="citation">(<a href="#ref-kerr1998" role="doc-biblioref">Kerr, 1998</a>)</span>. HARKing is potentially problematic because it expands the garden of forking paths and helps to justify the use of various analytic degrees of freedom (Figure <a href="10-prereg.html#fig:grid">10.4</a>). For example, you may come up with an explanation for why an intervention is effective in men but not in women in order to justify a post-hoc subgroup analysis based on sex (see Case Study). The extent to which HARKing is problematic is contested <span class="citation">(for discussion see <a href="#ref-hardwicke2021b" role="doc-biblioref">Tom E. Hardwicke &amp; Wagenmakers, 2021</a>)</span>, but we would argue that at the very least its important to be honest about whether hypotheses were developed before or after seeing research results.</p>
<div class="figure"><span id="fig:grid"></span>
<p class="caption marginnote shownote">
Figure 10.4: A scientist exploring a grid of individual research results. The horizontal axis illustrates a simplified ‚Äògarden of forking paths‚Äô: the many justifiable analysis specifications that the scientist can use to transform the data (D) into the evidence (E). The vertical axis illustrates that there may be several relevant theories (T), and hypotheses (H) derived from those theories, which could be constructed or selected and then confronted with the evidence. Thus, an unconstrained scientist can simultaneously exploit their analytic degrees of freedom and explanatory degrees of freedom to fit evidence to hypotheses and fit hypotheses to evidence in order to arrive at a study outcome that is more likely to align more with their preferences, but less likely to align with the truth. Caption is copied verbatim so needs editing. Shared under a CC-BY license, artwork by Viktor Beekman, concept by Tom Hardwicke and Eric-Jan Wagenmakers.
</p>
<img src="images/prereg/grid.jpg" alt="A scientist exploring a grid of individual research results. The horizontal axis illustrates a simplified ‚Äògarden of forking paths‚Äô: the many justifiable analysis specifications that the scientist can use to transform the data (D) into the evidence (E). The vertical axis illustrates that there may be several relevant theories (T), and hypotheses (H) derived from those theories, which could be constructed or selected and then confronted with the evidence. Thus, an unconstrained scientist can simultaneously exploit their analytic degrees of freedom and explanatory degrees of freedom to fit evidence to hypotheses and fit hypotheses to evidence in order to arrive at a study outcome that is more likely to align more with their preferences, but less likely to align with the truth. Caption is copied verbatim so needs editing. Shared under a CC-BY license, artwork by Viktor Beekman, concept by Tom Hardwicke and Eric-Jan Wagenmakers." width="\linewidth"  />
</div>
<p>But hang on a minute! Isn‚Äôt it a good thing to seek out interesting results if they are there in the data? Shouldn‚Äôt we ‚Äúlet the data speak?‚Äù The answer is yes<label for="tufte-sn-7" class="margin-toggle sidenote-number">7</label><input type="checkbox" id="tufte-sn-7" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">7</span> Note that exploratory research is not the same as p-hacking, which is explicitly dishonest because it involves deliberately withholding information.</span>! In fact, we have dedicated a whole chapter to exploratory data analysis (Chapter <a href="15-eda.html#eda">15</a>). The important thing to remember about exploratory research is that you need to (a) be aware of the increased risk of bias and calibrate your confidence in the results accordingly; (2) be honest with other researchers about your analysis strategy so they are also aware of the risk of bias and can calibrate <em>their</em> confidence in the results accordingly. Its important to understand the distinction between exploratory and confirmatory research modes<label for="tufte-sn-8" class="margin-toggle sidenote-number">8</label><input type="checkbox" id="tufte-sn-8" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">8</span> In practice, an individual study may contain both exploratory and confirmatory aspects which is why we describe them as different ‚Äòmodes.‚Äô</span>. Confirmatory research involves making design and analysis decisions, <em>before</em> the results have been observed. In the next section, we will learn about how to do that using preregistration.</p>
</div>
<div id="reducing-bias-increasing-transparency-and-calibrating-confidence-with-preregistration" class="section level2" number="10.2">
<h2><span class="header-section-number">10.2</span> Reducing bias, increasing transparency, and calibrating confidence with preregistration</h2>
<p>Perhaps the easiest way to counter the problem of undisclosed researcher degrees of freedom outlined above, is to make research decisions before seeing the study results - a bit like planning your route through the garden of forking paths before you start your journey. A great way to do this is preregistration <span class="citation">(<a href="#ref-hardwicke2021b" role="doc-biblioref">Tom E. Hardwicke &amp; Wagenmakers, 2021</a>; <a href="#ref-wagenmakers2012" role="doc-biblioref">Wagenmakers et al., 2012</a>)</span>. Preregistration involves declaring your research decisions in an online public registry before the data are inspected. Preregistration ensures that your decisions are results-independent, which reduces risk of bias arising from the issues described above. Preregistration also transparently conveys to others what you planned, helping them to determine the risk of bias and calibrate their confidence in the results. In other words, preregistration transparently provides the context needed to properly evaluate and interpret research. Preregistration does not require that you specify all research decisions in advance, only that you are transparent about what was planned, and what was not planned. This helps to make a distinction between which aspects of the research were exploratory and which were confirmatory (Figure <a href="10-prereg.html#fig:continuum">10.5</a>). All else being equal, we should have more confidence in confirmatory findings, because there is a lower risk of bias. Exploratory analyses have a higher risk of bias, but they are also more sensitive to serendipitous (unexpected) discoveries. So exploratory and confirmatory research are both valuable activities, it is just important to differentiate them. Preregistration offers the best of both worlds by clearing separating one from the other.</p>
<div class="figure"><span id="fig:continuum"></span>
<p class="caption marginnote shownote">
Figure 10.5: Preregistration clarifies where aspects of your research fall on a spectrum of exploratory and confirmatory modes of research. A preregistration is just a snapshot of your current thinking. If you have planned very little, your preregistration may not have much detail, but that‚Äôs absolutely fine! The important thing is that preregistration transparently conveys what was planned (confirmatory) and what was not (exploratory). Increasing the amount of detail in your preregistration increases your protection against bias. Placeholder image, use better illustration?
</p>
<img src="images/prereg/continuum.png" alt="Preregistration clarifies where aspects of your research fall on a spectrum of exploratory and confirmatory modes of research. A preregistration is just a snapshot of your current thinking. If you have planned very little, your preregistration may not have much detail, but that's absolutely fine! The important thing is that preregistration transparently conveys what was planned (confirmatory) and what was not (exploratory). Increasing the amount of detail in your preregistration increases your protection against bias. Placeholder image, use better illustration?" width="\linewidth"  />
</div>
<p>In addition to the benefits described above, preregistration may improve the quality of research by encouraging closer attention to study planning. We‚Äôve found that it really helps facilitate communication between collaborators, and can catch addressable problems before time and resources are wasted on a poorly designed study. Detailed advanced planning can also create opportunities for useful community feedback, particularly in the context of Registered Reports, where dedicated peer reviewers will evaluate your study before its even begun (Box 2).</p>
<div class="interactive">
<p>Box 2. Preregistration and friends: A toolbox of countermeasures to address researcher degrees of freedom</p>
<p>NB - CURRENT TEXT VERY CLOSE TO HARDWICKE &amp; WAGENMAKERS (2021) NEEDS EDITING</p>
<p>There are several useful tools that take can be used to address the problem of researcher degrees of freedom. In general, we would recommend that these tools are combined with preregistration, rather than used as a replacement <span class="citation">(<a href="#ref-hardwicke2021b" role="doc-biblioref">Tom E. Hardwicke &amp; Wagenmakers, 2021</a>)</span>.</p>
<p><strong>Robustness checks</strong>. Whilst preregistration aims to constrain researcher degrees of freedom, robustness checks directly exploit them in order to evaluate their impact. Traditional sensitivity analyses may evaluate a few justifiable options for a single research decision92; however, recent approaches, variously known as ‚Äúmultiverse analysis‚Äù93, ‚Äúvibration of effects‚Äù94, ‚Äúspecification curve‚Äù95, or ‚Äúmultimodel analysis‚Äù96, systematically assess the factorial intersection of multiple choices for multiple decisions, potentially resulting in tens of thousands of unique analysis specifications97‚Äì100. This is akin to simultaneously examining multiple cells in the array depicted in Figure 1, rather than a single prespecified cell.</p>
<p>Some have argued that systematic robustness checks render preregistration redundant71,101. However, the subjective choice of which specifications to examine or report90,102 means that robustness checks can introduce researcher degrees of freedom, creating an opportunity for selective reporting, and thereby increasing the risk of bias. Researchers can have the best of both worlds (Section 4.2) by preregistering their robustness checks.</p>
<p><strong>Blind analysis</strong>. Issues arising during data collection such as attrition, missing data, randomisation failures, or unexpected data distributions may invalidate planned analyses. Blind analysis disguises information related to outcomes (e.g., by adding noise or shuffling variables) allowing the data to be inspected whilst ensuring decision-making remains outcome-independent103,104. Blind analysis is used in physics to address concerns about bias introduced by outcome-dependent analyses105‚Äì107. Blind analysis requires some technical expertise and can introduce bias if poorly implemented. Additionally, blind analysis does not prevent selective reporting, so should ideally be used in conjunction with preregistration.</p>
<p><strong>Hold-out sample</strong>. Splitting a dataset can enable exploratory analyses in a ‚Äòtraining‚Äô sample followed by confirmatory analyses in a ‚Äòtest‚Äô or ‚Äòhold-out‚Äô sample108. This approach requires a large sample size as splitting the data reduces statistical power. Preregistering the analyses intended for the hold-out sample ensures they are truly confirmatory.</p>
<p><strong>Preregistration of analysis scripts based on simulated data</strong>. It requires some imagination to anticipate the details to include in an analysis plan69,109 and it can be difficult to communicate analysis specifications in prose110,111. This can be addressed by preregistering analysis scripts prepared using simulated data66.</p>
<p><strong>Standard Operating Procedures</strong>. Maintaining a living document of default research decisions that is co-registered with each study could enhance preregistration efficiency112.</p>
<p><strong>Open lab notebooks</strong>. Open lab notebooks could improve transparency throughout a research project113 and help track departures from the preregistration. Preregistration is similar to sharing the pages of your lab notebook which outline the study plan.</p>
<p><strong>Registered Reports</strong>. Registered Reports are a journal article format that offers in-principle acceptance for publication before studies begin based on peer review of a prespecified study protocol87 (Figure <a href="10-prereg.html#fig:reg-reports">10.6</a>). This radical departure from traditional publication practices promises the benefits of preregistration with enhanced protection against publication bias. Registered Reports may be most suitable for more confirmatory studies114. Related tools that could also mitigate publication bias involve combining standard preregistration with post-study results-blind peer review115 and encouraging results reporting in study registries (a mandate for some clinical trials, often ignored in practice116); however, the time investment and public accountability involved in Registered Reports may offer greater motivation for authors to report results even if they do not reflect their preferred outcome.</p>
</div>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:reg-reports"></span>
<img src="images/prereg/registered-reports.png" alt="Registered Reports (https://www.cos.io/initiatives/registered-reports)" width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 10.6: Registered Reports (<a href="https://www.cos.io/initiatives/registered-reports" class="uri">https://www.cos.io/initiatives/registered-reports</a>)<!--</p>-->
<!--</div>--></span>
</p>
<div class="case-study">
<p>üî¨ Case study: Undisclosed analytic flexibility in the wild</p>
<p>NB this box was previously called ‚ÄúA tale of two RCTs‚Äù but I‚Äôm not sure why - there‚Äôs only one RCT right?</p>
<p>A few years ago, one of us (Mike) was reading a paper <span class="citation">(<a href="#ref-berkowitz2015" role="doc-biblioref">Berkowitz et al., 2015</a>)</span> and something seemed odd. The paper reported a randomized field experiment evaluating an educational app intended to increase children‚Äôs math skills. According to the reported analysis, the app was a success. But the analysis had not been preregistered, and Mike found some of the analysis choices to be unusual. For example, the analysis had probed whether there was an effect of the app in particular subgroups, but neglected to evaluate whether there was an overall effect. Mike was concerned that the results were affected by undisclosed analytic flexibility and published a commentary consisting of alternative analyses that suggested the app was not effective <span class="citation">(<a href="#ref-frank2016" role="doc-biblioref">Frank, 2016</a>)</span>. The original authors responded that their analyses were entirely based on prior research and argued that the disagreement about how the data should be analyzed was ‚Äúphilosophical‚Äù <span class="citation">(<a href="#ref-berkowitz2016" role="doc-biblioref">Berkowitz et al., 2016</a>)</span>. The problem here is that it is very difficult to know the extent to which the original analysis was influenced by the results. If the analysis plan had been preregistered, this would reduce the risk of bias and allow Mike and other readers to have more confidence in the reported results[^10}.</p>
</div>
</div>
<div id="how-to-preregister" class="section level2" number="10.3">
<h2><span class="header-section-number">10.3</span> How to preregister</h2>
<p>Preregistration is actually pretty new to psychology, and there‚Äôs still no standard way of doing it - you‚Äôre already at the cutting edge<label for="tufte-sn-9" class="margin-toggle sidenote-number">9</label><input type="checkbox" id="tufte-sn-9" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">9</span> There‚Äôs a much longer history of registering research in medicine <span class="citation">(<a href="#ref-dickersin2012" role="doc-biblioref">Dickersin &amp; Rennie, 2012</a>)</span> and preregistration has only recently started to gain traction in other disciplines like psychology <span class="citation">(<a href="#ref-nosek2018" role="doc-biblioref">Nosek et al., 2018</a>)</span>. Its still very rare though; one study estimated that about 3% of psychology studies are preregistered <span class="citation">(<a href="#ref-hardwicke2021c" role="doc-biblioref">Tom E. Hardwicke et al., 2021</a>)</span>.</span>! We recommend using the Open Science Framework (OSF) as this is one of the most popular registries in psychology and you can do lots of other useful things there to make your research transparent, like sharing data, materials, analysis scripts, and preprints. On the OSF it is possible to ‚Äúregister‚Äù any file you have uploaded. When you register a file, it creates a timestamped, read-only copy, with a dedicated link. You can add this link to articles reporting your research. One approach to preregistration is to write a protocol document that specifies the study rationale, aims or hypotheses, methods, and analysis plan, and register that (here‚Äôs an example from one of our own studies: <a href="https://osf.io/2cnkq/" class="uri">https://osf.io/2cnkq/</a>). The OSF also has a collection of dedicated preregistration templates that you can use if you prefer. These templates are often tailored to the needs of particular types of research. For example, there are templates for general quantitative psychology research <span class="citation">(<span>‚ÄúPRP-QUANT‚Äù</span> <a href="#ref-bosnjak2021" role="doc-biblioref">Bosnjak et al., 2021</a>)</span>, cognitive modelling <span class="citation">(<a href="#ref-cruwell2021" role="doc-biblioref">Cr√ºwell &amp; Evans, 2021</a>)</span>, and secondary data analysis <span class="citation">(<a href="#ref-akker2019" role="doc-biblioref">Akker et al., 2019</a>)</span>. The OSF interface may change, but currently if you go to <a href="https://osf.io/registries/osf/new" class="uri">https://osf.io/registries/osf/new</a> you can follow the steps to create a new registration.</p>
<p>Once you‚Äôve preregistered your plan, you just go off and run the study and report the results, right? Well hopefully‚Ä¶but things might not turn out to be that straightforward. Its quite common to forgot to include something in your plan or to have to depart from the plan due to something unexpected. Preregistration can actually be pretty hard in practice <span class="citation">(<a href="#ref-nosek2019" role="doc-biblioref">Nosek et al., 2019</a>)</span>! Don‚Äôt worry though - remember that the primary goal of preregistration is transparency to enable others to evaluate and interpret our work. If you decide to depart from your original plan and conduct results-dependent analyses, then this may increase the risk of bias. But its important to know that so we can appropriately calibrate our confidence in the results. You may even be able to run both the planned and unplanned analyses as a robustness check (Box 2) to evaluate the extent to which this choice impacts the results.</p>
<p>When you report your study, it is important to distinguish between what was planned and what was not. If you ran a lot of results-dependent analyses, then it might be worth having separate exploratory and confirmatory results sections. If you mainly stuck to your original plan, with only minor departures, then you could include a table (perhaps in an appendix) that outlines these changes (for example, see Supplementary Information A of this article: <a href="https://doi.org/10.31222/osf.io/wt5ny" class="uri">https://doi.org/10.31222/osf.io/wt5ny</a>)</p>
<!-- TODO: Barriers to adoption of preregistration. -->
<!-- TODO: Limitations of the preregistration approach, especially with respect to iterative theory building (e.g., Navarro critique). -->
<div class="exercise">
<p><span id="exr:unlabeled-div-1" class="exercise"><strong>Exercise 10.1  </strong></span>P-hack your way to scientific glory! To get a feel for how results-dependent analyses might work in practice, have a play around with this app: <a href="https://projects.fivethirtyeight.com/p-hacking/" class="uri">https://projects.fivethirtyeight.com/p-hacking/</a></p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-2" class="exercise"><strong>Exercise 10.2  </strong></span>Preregister your next experiment! The best way to get started with preregistration is to have a go with your next study. Head over to <a href="https://osf.io/registries/osf/new" class="uri">https://osf.io/registries/osf/new</a> and register your study protocol or complete one of the templates.</p>
</div>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-akker2019" class="csl-entry">
Akker, O. van den, Weston, S. J., Campbell, L., Chopik, W. J., Damian, R. I., Davis-Kean, P., Hall, A., Kosie, J., Kruse, E., Olsen, J., Ritchie, S. J., Valentine, K. D., Veer, A. van ‚Äôt., &amp; Bakker, M. (2019). <em>Preregistration of secondary data analysis: <span>A</span> template and tutorial</em>. PsyArXiv. <a href="https://psyarxiv.com/hvfmr/">https://psyarxiv.com/hvfmr/</a>
</div>
<div id="ref-bakker2012" class="csl-entry">
Bakker, M., Dijk, A. van, &amp; Wicherts, J. M. (2012). The rules of the game called psychological science. <em>Perspectives on Psychological Science</em>, <em>7</em>(6), 543‚Äì554. <a href="https://doi.org/10.1177/1745691612459060">https://doi.org/10.1177/1745691612459060</a>
</div>
<div id="ref-barber1976" class="csl-entry">
Barber, T. X. (1976). <em>Pitfalls in <span>Human</span> <span>Research</span>: <span>Ten</span> <span>Pivotal</span> <span>Points</span></em>. Pergamon Press.
</div>
<div id="ref-bennett2009" class="csl-entry">
Bennett, C., Miller, M., &amp; Wolford, G. (2009). Neural correlates of interspecies perspective taking in the post-mortem <span>Atlantic</span> <span>Salmon</span>: An argument for multiple comparisons correction. <em>NeuroImage</em>, <em>47</em>, S125. <a href="https://doi.org/10.1016/S1053-8119(09)71202-9">https://doi.org/10.1016/S1053-8119(09)71202-9</a>
</div>
<div id="ref-berkowitz2015" class="csl-entry">
Berkowitz, T., Schaeffer, M. W., Maloney, E. A., Peterson, L., Gregor, C., Levine, S. C., &amp; Beilock, S. L. (2015). Math at home adds up to achievement in school. <em>Science</em>, <em>350</em>(6257), 196‚Äì198. <a href="https://doi.org/10.1126/science.aac7427">https://doi.org/10.1126/science.aac7427</a>
</div>
<div id="ref-berkowitz2016" class="csl-entry">
Berkowitz, T., Schaeffer, M. W., Rozek, C. S., Maloney, E. A., Levine, S. C., &amp; Beilock, S. L. (2016). Response to <span>Comment</span> on <span>‚Äú<span>Math</span> at home adds up to achievement in school.‚Äù</span> <em>Science</em>, <em>351</em>(6278), 1161‚Äì1161. <a href="https://doi.org/10.1126/science.aad8555">https://doi.org/10.1126/science.aad8555</a>
</div>
<div id="ref-bosnjak2021" class="csl-entry">
Bosnjak, M., Fiebach, C., Mellor, D. T., Mueller, S., O‚ÄôConnor, D., Oswald, F., &amp; Sokol-Chang, R. (2021). <em>A template for preregistration of quantitative research in psychology: Report of the joint psychological societies preregistration task force</em>. PsyArXiv. <a href="https://doi.org/10.31234/osf.io/d7m5r">https://doi.org/10.31234/osf.io/d7m5r</a>
</div>
<div id="ref-cruwell2021" class="csl-entry">
Cr√ºwell, S., &amp; Evans, N. J. (2021). Preregistration in diverse contexts: A preregistration template for the application of cognitive models. <em>Royal Society Open Science</em>, <em>8</em>(10), 210155. <a href="https://doi.org/10.1098/rsos.210155">https://doi.org/10.1098/rsos.210155</a>
</div>
<div id="ref-dickersin2012" class="csl-entry">
Dickersin, K., &amp; Rennie, D. (2012). The evolution of trial registries and their use to assess the clinical trial enterprise. <em>JAMA</em>, <em>307</em>(17), 1861‚Äì1864. <a href="https://doi.org/10.1001/jama.2012.4230">https://doi.org/10.1001/jama.2012.4230</a>
</div>
<div id="ref-duhem1954" class="csl-entry">
Duhem, P. (1954). <em>The <span>Aim</span> and <span>Structure</span> of <span>Physical</span> <span>Theory</span></em> (P. W. Wiener, Trans.; 2nd ed.). Princeton University Press.
</div>
<div id="ref-feynman1974" class="csl-entry">
Feynman, R. P. (1974). <em>Cargo <span>Cult</span> <span>Science</span></em>. <a href="http://calteches.library.caltech.edu/51/2/CargoCult.pdf">http://calteches.library.caltech.edu/51/2/CargoCult.pdf</a>
</div>
<div id="ref-frank2016" class="csl-entry">
Frank, M. C. (2016). Comment on <span>‚Äúmath at home adds up to achievement in school.‚Äù</span> In <em>Science</em> (No. 6278; Vol. 351, pp. 1161.2‚Äì1161).
</div>
<div id="ref-gelman2014" class="csl-entry">
Gelman, A., &amp; Loken, E. (2014). The statistical crisis in science. <em>American Scientist</em>, <em>102</em>(6), 460‚Äì465. <a href="https://doi.org/10.1511/2014.111.460">https://doi.org/10.1511/2014.111.460</a>
</div>
<div id="ref-giner-sorolla2012" class="csl-entry">
Giner-Sorolla, R. (2012). Science or art? <span>How</span> aesthetic standards grease the way through the publication bottleneck but undermine science. <em>Perspectives on Psychological Science</em>, <em>7</em>(6), 562‚Äì571. <a href="https://doi.org/10.1177/1745691612457576">https://doi.org/10.1177/1745691612457576</a>
</div>
<div id="ref-good1972" class="csl-entry">
Good, I. J. (1972). Statistics and <span>Today</span>‚Äôs <span>Problems</span>. <em>The American Statistician</em>, <em>26</em>(3), 11‚Äì19. <a href="https://doi.org/10.1080/00031305.1972.10478922">https://doi.org/10.1080/00031305.1972.10478922</a>
</div>
<div id="ref-degroot2014" class="csl-entry">
Groot, A. D. de. (2014). The meaning of <span>‚Äúsignificance‚Äù</span> for different types of research (E.-J. Wagenmakers, D. Borsboom, J. Verhagen, R. A. Kievit, M. Bakker, A. O. J. Cramer, D. Matzke, D. Mellenbergh, &amp; H. L. J. van der Maas, Trans.). <em>Acta Psychologica</em>, <em>148</em>, 188‚Äì194. <a href="https://doi.org/10.1016/j.actpsy.2014.02.001">https://doi.org/10.1016/j.actpsy.2014.02.001</a>
</div>
<div id="ref-hardwicke2021c" class="csl-entry">
Hardwicke, Tom E., Thibault, R. T., Kosie, J., Wallach, J. D., Kidwell, M. C., &amp; Ioannidis, J. (2021). Estimating the prevalence of transparency and reproducibility-related research practices in psychology (2014-2017). <em>Perspectives on Psychological Science</em>. https://doi.org/<a href="https://doi.org/10.1177/1745691620979806">https://doi.org/10.1177/1745691620979806</a>
</div>
<div id="ref-hardwicke2021b" class="csl-entry">
Hardwicke, Tom E., &amp; Wagenmakers, E.-J. (2021). <em>Preregistration: <span>A</span> pragmatic tool to reduce bias and calibrate confidence in scientific research</em>. MetaArXiv. <a href="https://doi.org/10.31222/osf.io/d7bcu">https://doi.org/10.31222/osf.io/d7bcu</a>
</div>
<div id="ref-ioannidis2005" class="csl-entry">
Ioannidis, J. P. A. (2005). Why most published research findings are false. <em>PLOS Medicine</em>, <em>2</em>(8), e124. <a href="https://doi.org/10.1371/journal.pmed.0020124">https://doi.org/10.1371/journal.pmed.0020124</a>
</div>
<div id="ref-kerr1998" class="csl-entry">
Kerr, N. L. (1998). <span>HARKing</span>: <span>Hypothesizing</span> <span>After</span> the <span>Results</span> are <span>Known</span>. <em>Personality &amp; Social Psychology Review (Lawrence Erlbaum Associates)</em>, <em>2</em>(3), 196. <a href="https://doi.org/10.1207/s15327957pspr0203_4">https://doi.org/10.1207/s15327957pspr0203_4</a>
</div>
<div id="ref-nosek2019" class="csl-entry">
Nosek, B. A., Beck, E. D., Campbell, L., Flake, J. K., Hardwicke, T. E., Mellor, D. T., Veer, A. E. van ‚Äôt, &amp; Vazire, S. (2019). Preregistration is hard, and worthwhile. <em>Trends in Cognitive Sciences</em>, <em>23</em>(10), 815‚Äì818. <a href="https://doi.org/10.1016/j.tics.2019.07.009">https://doi.org/10.1016/j.tics.2019.07.009</a>
</div>
<div id="ref-nosek2018" class="csl-entry">
Nosek, B. A., Ebersole, C. R., DeHaven, A. C., &amp; Mellor, D. T. (2018). The preregistration revolution. <em>Proceedings of the National Academy of Sciences</em>, <em>115</em>(11), 2600‚Äì2606. <a href="https://doi.org/10.1073/pnas.1708274114">https://doi.org/10.1073/pnas.1708274114</a>
</div>
<div id="ref-nosek2012" class="csl-entry">
Nosek, B. A., Spies, J. R., &amp; Motyl, M. (2012). Scientific <span>Utopia</span>: <span>II</span>. Restructuring incentives and practices to promote truth over publishability. <em>Perspectives on Psychological Science</em>, <em>7</em>(6), 615‚Äì631. <a href="https://doi.org/10.1177/1745691612459058">https://doi.org/10.1177/1745691612459058</a>
</div>
<div id="ref-oboyle2017" class="csl-entry">
O‚ÄôBoyle, E. H., Banks, G. C., &amp; Gonzalez-Mul√©, E. (2017). The chrysalis effect: How ugly initial results metamorphosize into beautiful articles. <em>Journal of Management</em>, <em>43</em>(2), 376‚Äì399. <a href="https://doi.org/10.1177/0149206314527133">https://doi.org/10.1177/0149206314527133</a>
</div>
<div id="ref-oberauer2019" class="csl-entry">
Oberauer, K., &amp; Lewandowsky, S. (2019). Addressing the theory crisis in psychology. <em>Psychonomic Bulletin &amp; Review</em>, <em>26</em>(5), 1596‚Äì1618.
</div>
<div id="ref-smaldino2016" class="csl-entry">
Smaldino, P. E., &amp; McElreath, R. (2016). The natural selection of bad science. <em>Royal Society Open Science</em>, <em>3</em>(9), 160384. <a href="https://doi.org/10.1098/rsos.160384">https://doi.org/10.1098/rsos.160384</a>
</div>
<div id="ref-veldkamp2017" class="csl-entry">
Veldkamp, C. L. S., Hartgerink, C. H. J., Assen, M. A. L. M. van, &amp; Wicherts, J. M. (2017). Who believes in the storybook image of the scientist? <em>Accountability in Research</em>, <em>24</em>(3), 127‚Äì151. <a href="https://doi.org/10.1080/08989621.2016.1268922">https://doi.org/10.1080/08989621.2016.1268922</a>
</div>
<div id="ref-wagenmakers2012" class="csl-entry">
Wagenmakers, E.-J., Wetzels, R., Borsboom, D., Maas, H. L. J. van der, &amp; Kievit, R. A. (2012). An agenda for purely confirmatory research. <em>Perspectives on Psychological Science</em>, <em>7</em>(6), 632‚Äì638. <a href="https://doi.org/10.1177/1745691612463078">https://doi.org/10.1177/1745691612463078</a>
</div>
</div>
<p style="text-align: center;">
<a href="9-sampling.html"><button class="btn btn-default">Previous</button></a>
<a href="11-selection.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="toc/toc.js"></script>


</body>
</html>
