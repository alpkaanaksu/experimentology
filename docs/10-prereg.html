<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 10 Preregistration | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />



<meta name="description" content="Chapter 10 Preregistration | Experimentology">

<title>Chapter 10 Preregistration | Experimentology</title>

<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />




<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="toc/toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-intro.html#intro"><span class="toc-section-number">1</span> Experiments and theories</a></li>
<li><a href="2-replication.html#replication"><span class="toc-section-number">2</span> Replication and reproducibility</a></li>
<li><a href="3-ethics.html#ethics"><span class="toc-section-number">3</span> Ethics</a></li>
<li class="part"><span><b>II Statistics</b></span></li>
<li><a href="4-estimation.html#estimation"><span class="toc-section-number">4</span> Estimation</a></li>
<li><a href="5-inference.html#inference"><span class="toc-section-number">5</span> Inference</a></li>
<li><a href="6-models.html#models"><span class="toc-section-number">6</span> Models</a></li>
<li class="part"><span><b>III Design and Planning</b></span></li>
<li><a href="7-measurement.html#measurement"><span class="toc-section-number">7</span> Measurement</a></li>
<li><a href="8-design.html#design"><span class="toc-section-number">8</span> Design of experiments</a></li>
<li><a href="9-sampling.html#sampling"><span class="toc-section-number">9</span> Sampling</a></li>
<li><a href="10-prereg.html#prereg"><span class="toc-section-number">10</span> Preregistration</a></li>
<li class="part"><span><b>IV Execution</b></span></li>
<li><a href="11-selection.html#selection"><span class="toc-section-number">11</span> Experimental strategy</a></li>
<li><a href="12-collection.html#collection"><span class="toc-section-number">12</span> Data collection</a></li>
<li><a href="13-management.html#management"><span class="toc-section-number">13</span> Project management</a></li>
<li class="part"><span><b>V Analysis and Reporting</b></span></li>
<li><a href="14-viz.html#viz"><span class="toc-section-number">14</span> Visualization</a></li>
<li><a href="15-eda.html#eda"><span class="toc-section-number">15</span> Exploratory data analysis</a></li>
<li><a href="16-writing.html#writing"><span class="toc-section-number">16</span> Reproducible writing</a></li>
<li><a href="17-meta.html#meta"><span class="toc-section-number">17</span> Meta-analysis</a></li>
<li><a href="18-conclusions.html#conclusions"><span class="toc-section-number">18</span> Conclusions</a></li>
<li class="part"><span><b>VI Appendices</b></span></li>
<li><a href="19-git.html#git"><span class="toc-section-number">19</span> GitHub Tutorial</a></li>
<li><a href="20-rmarkdown.html#rmarkdown"><span class="toc-section-number">20</span> R Markdown Tutorial</a></li>
<li><a href="21-tidyverse.html#tidyverse"><span class="toc-section-number">21</span> Tidyverse Tutorial</a></li>
<li><a href="22-ggplot.html#ggplot"><span class="toc-section-number">22</span> ggplot Tutorial</a></li>
<li><a href="23-instructors.html#instructors"><span class="toc-section-number">23</span> Instructor‚Äôs guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="prereg" class="section level1" number="10">
<h1><span class="header-section-number">Chapter 10</span> Preregistration</h1>
<div class="learning-goals">
<p>üçé Learning goals:</p>
<ul>
<li>Recognize the dangers of researcher degrees of freedom in ‚Äúthe garden of forking paths‚Äù</li>
<li>Understand the differences between exploratory and confirmatory modes of research</li>
<li>Learn how preregistration and other tools can counter bias and help others to evaluate your work by increasing transparency</li>
</ul>
</div>
<blockquote>
<p>The first principle is that you must not fool yourself‚Äìand you are the easiest person to fool‚Ä¶After you‚Äôve not fooled yourself, it‚Äôs easy not to fool other scientists. You just have to be honest in a conventional way after that.</p>
<footer>
‚Äî Richard Feynman <span class="citation">(<a href="#ref-feynman1974" role="doc-biblioref">1974</a>)</span>
</footer>
</blockquote>
<p>This may surprise you coming from the authors of a textbook about research methods, but there is no single ‚Äúcorrect‚Äù way to design and analyze an experiment<label for="tufte-sn-1" class="margin-toggle sidenote-number">1</label><input type="checkbox" id="tufte-sn-1" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">1</span> Though there are plenty of incorrect ways to design and analyse experiments and we hope we can help you to avoid these!</span>. In fact, for most research decisions, there are many justifiable options. For example, will you stop data collection after 20, 200, or 2000 participants? Will you remove outlier values and how will you define them? Will you conduct subgroup analyses to see whether the results are affected by sex, or age, or some other factor? Consider a simplified, hypothetical case where you need to make five analysis decisions and have five justifiable options for each decision ‚Äî this alone would result in 3125 (5^5) unique ways to analyze your data! In this chapter, we will find out why flexibility in the design, analysis, reporting, and interpretation of experiments (also referred to as ‚Äúresearcher degrees of freedom‚Äù), can lead to scientists fooling themselves and fooling each other. We will also learn about how preregistration (and other tools) can be used to protect our research from bias and provide the transparency that other scientists need to effectively evaluate our work.</p>
<div id="lost-in-a-garden-of-forking-paths" class="section level2" number="10.1">
<h2><span class="header-section-number">10.1</span> Lost in a garden of forking paths</h2>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span style="display:block;" id="fig:forking-paths"></span>
<img src="images/prereg/forking-paths.png" alt="Garden of forking paths (placeholder image I hacked together, replace with an illustration?)" width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 10.1: Garden of forking paths (placeholder image I hacked together, replace with an illustration?)<!--</p>-->
<!--</div>--></span>
</p>
<p>One way to visualize researcher degrees of freedom is as a vast decision tree or ‚Äúgarden of forking paths‚Äù (Figure <a href="10-prereg.html#fig:forking-paths">10.1</a>; <span class="citation"><a href="#ref-gelman2014" role="doc-biblioref">Gelman &amp; Loken</a> (<a href="#ref-gelman2014" role="doc-biblioref">2014</a>)</span>). Each node represents a decision point and each branch represents a justifiable choice. Each unique pathway through the garden terminates in an individual result. Because scientific observations typically consist of both noise (random variation unique to this sample) and signal (regularities that will reoccur in other samples), some of these pathways will inevitably lead to results that are misleading (e.g., inflated effect sizes, exaggerated evidence, or false positives)<label for="tufte-sn-2" class="margin-toggle sidenote-number">2</label><input type="checkbox" id="tufte-sn-2" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">2</span> The signal-to-noise ratio is worse in situations (alas, common in psychology) that involve small effect sizes, high variation, and large measurement errors <span class="citation">(<a href="#ref-ioannidis2005" role="doc-biblioref">Ioannidis, 2005</a>)</span>. Researcher degrees of freedom may be constrained to some extent by strong theory <span class="citation">(<a href="#ref-oberauer2019" role="doc-biblioref">Oberauer &amp; Lewandowsky, 2019</a>)</span>, community norms, or replication studies, though these constraints may be more implicit than explicit, and still leave plenty of room for flexible decision-making.</span>. The more potential paths there are in the garden that you might explore, the higher the chance of encountering misleading results<label for="tufte-sn-3" class="margin-toggle sidenote-number">3</label><input type="checkbox" id="tufte-sn-3" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">3</span> In frequentist terminology, you are increasing the chance of making a ‚ÄòType I error‚Äô or ‚Äòfalse positive‚Äô finding.</span>. Statisticians refer to this as a <em>multiplicity</em> problem.</p>
<p>As noted in Chapter <a href="5-inference.html#inference">5</a>, multiplicity can be addressed to some extent with statistical countermeasures, like the Bonferroni correction; however, these adjustment methods need to account for every path that you <em>could have</em> taken <span class="citation">(<a href="#ref-gelman2014" role="doc-biblioref">Gelman &amp; Loken, 2014</a>; <a href="#ref-degroot2014" role="doc-biblioref">Groot, 2014</a>)</span>. When you navigate the garden of forking paths <em>during data analysis</em>, it is easy to forget, or even be unaware of every path that you could have taken, so these methods can longer be used effectively. Additionally, when a researcher navigates the garden of forking paths during data analysis, their decisions can be biased because they are receiving feedback on how different choices affect the results (<em>results-dependent</em> decision making). If a researcher is seeking a particular kind of result (which is likely - see Box 1), then they are more likely to follow the branches that take them in that direction. You could think of this a bit like playing a game of hot!üî• or cold!‚òÉÔ∏è. Each time the researcher reaches a decision point, they try one of the branches and get some feedback on how it affects the results. If the feedback is hot!üî• (i.e., the result is in line with their preferences) then they take that branch. If the answer is cold!‚òÉÔ∏è, they try a different branch. If they reach the end of a complete pathway, and the results are cold!‚òÉÔ∏è, maybe they even retrace their steps and try some different branches earlier in the pathway. This strategy create a risk of bias<label for="tufte-sn-4" class="margin-toggle sidenote-number">4</label><input type="checkbox" id="tufte-sn-4" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">4</span> We say ‚Äúrisk of bias‚Äù rather than just ‚Äúbias‚Äù because in most scientific contexts, we do not have a known ground truth to compare the results to. So in any specific situation, we do not know the extent to which results-dependent analyses have actually biased the results.</span> because the results of the study are being systematically skewed towards the researcher‚Äôs preferences<label for="tufte-sn-5" class="margin-toggle sidenote-number">5</label><input type="checkbox" id="tufte-sn-5" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">5</span> Another way to think of this is in terms of ‚Äòregression to the mean.‚Äô When a sample statistic is selected because it crosses some threshold (e.g., statistical significance), then it is more likely to provide a biased estimate that decreases upon subsequent measurement - in other words, it regresses to the mean.</span>.</p>
<p>In the most egregious cases, a researcher may try multiple pathways until they obtain a desirable result<label for="tufte-sn-6" class="margin-toggle sidenote-number">6</label><input type="checkbox" id="tufte-sn-6" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">6</span> ‚ÄúIf you torture the data long enough, it will confess‚Äù <span class="citation">(<a href="#ref-good1972" role="doc-biblioref">Good, 1972</a>)</span></span> and then <em>selectively report</em> that result, neglecting to mention that they have tried several analysis strategies. You may remember an example of this when participants apparently became younger when they listened to ‚ÄúWhen I‚Äôm 64‚Äù by The Beatles in Chapter <a href="3-ethics.html#ethics">3</a>. Another nice example is when a group of enterprising researchers ‚Äòdiscovered‚Äô brain activity in a dead Atlantic Salmon (Figure <a href="10-prereg.html#fig:salmon">10.2</a>; <span class="citation"><a href="#ref-bennett2009" role="doc-biblioref">Bennett et al.</a> (<a href="#ref-bennett2009" role="doc-biblioref">2009</a>)</span>). Deliberately taking advantage of researcher degrees of freedom and selectively reporting results is known by various names, like p-hacking, cherry picking, data dredging, and it is unethical because it involves hiding highly relevant information. But you should also be aware that results-dependent analysis incurs a risk of bias even if the researcher doesn‚Äôt explicitly try multiple pathways and honestly reports everything they did. For example, if each branch they took was hot!üî•, they may reach the end of the pathway with the result they desire without realizing that, had the results been different, they would have followed other pathways <span class="citation">(<a href="#ref-gelman2014" role="doc-biblioref">Gelman &amp; Loken, 2014</a>; <a href="#ref-degroot2014" role="doc-biblioref">Groot, 2014</a>)</span>. Its surprisingly easy to convince yourself after the fact that you made the decisions you did for principled reasons that had nothing to do with the results (see ‚Äòmotivated reasoning,‚Äô Box 1). In sum, engaging in results-dependent analysis increases the chances that you will fool yourself by inadvertently stumbling across misleading results - and if you are not honest about what you did, you can fool others too.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span style="display:block;" id="fig:salmon"></span>
<img src="images/prereg/salmon.jpeg" alt="By deliberately exploiting analytic flexibility in the processing pipeline of fMRI data, Bennet et al. (2009) were able to identify 'brain activity' in a dead Atlantic Salmon during cognitive perspective taking task." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 10.2: By deliberately exploiting analytic flexibility in the processing pipeline of fMRI data, Bennet et al.¬†(2009) were able to identify ‚Äòbrain activity‚Äô in a dead Atlantic Salmon during cognitive perspective taking task.<!--</p>-->
<!--</div>--></span>
</p>
</div>
<div id="exploratory-and-confirmatory-research" class="section level2" number="10.2">
<h2><span class="header-section-number">10.2</span> Exploratory and confirmatory research</h2>
<p>But hang on a minute! Isn‚Äôt it a good thing to seek out interesting results if they are there in the data? Shouldn‚Äôt we ‚Äúlet the data speak?‚Äù The answer is yes! Examining the data from multiple angles is an important activity, sometimes referred to as ‚Äúexploratory research.‚Äù This is a great way to generate new hypotheses and identify unexpected patterns. But, with exploratory research you need to (a) be aware of the increased risk of bias and calibrate your confidence in the results accordingly; (2) be honest with other researchers about your analysis strategy so they are also aware of the risk of bias and can calibrate their confidence in the results accordingly. Note that ‚Äúexploratory research‚Äù is not the same as p-hacking, which is dishonest because it involves withholding information.</p>
<p>Exploratory research can be contrasted with confirmatory research, where we</p>
<p>Consider also that there are multiple ways to <em>explain</em> your results - any single result can be explained by multiple different theories (known as the Duhem-Quine problem) We might call these ‚Äúexplanatory degrees of freedom.‚Äù</p>
<p>Grid image.
Means (selective reporting) motive (pressure to publish etc) opportunity (RDFs)</p>
<p>HARking - essentially expands the GFP.</p>
<div class="interactive">
<p>‚å®Ô∏è Interactivity box: The perils of p-hacking ‚ÄúHack your way to scientific glory‚Äù (p-hacking exercise).</p>
</div>
</div>
<div id="only-human-cognitive-biases-and-pressure-to-polish" class="section level2" number="10.3">
<h2><span class="header-section-number">10.3</span> Only human: Cognitive biases and pressure to polish</h2>
<p>The chrysalis effect: when ugly truth becomes a beautiful fiction</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span style="display:block;" id="fig:chrysalis"></span>
<img src="images/prereg/chrysalis.png" alt="The chrysalis effect" width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 10.3: The chrysalis effect<!--</p>-->
<!--</div>--></span>
</p>
<p>There‚Äôs a storybook image of the scientist as an objective, rationale, and dispassionate arbiter of truth <span class="citation">(<a href="#ref-veldkamp2017" role="doc-biblioref">Veldkamp et al., 2017</a>)</span>. But in reality, scientists are only human: they have egos, career ambitions, and rent to pay.
cognitive bias and extrinsic motivations</p>
<p>Unfortunately, the scientific ecosystem in which we do our research is tainted with bad incentives.</p>
<p>The allocation of funding, awards, and publication prestige is based on research findings being impressive over being right7‚Äì9. Typically, this manifests as a preference for novel, positive, and ‚Äòstatistically significant‚Äô findings over incremental, negative, or null findings7,23‚Äì25. There is additional pressure to produce articles with concise, coherent, and compelling narratives, encouraging selective reporting of research methods and results23,24,26 in order to hide the messy realities of scientific inquiry beneath a veneer of artificial perfection27.</p>
</div>
<div id="reducing-bias-and-increasing-transparency-with-preregistration-and-friends" class="section level2" number="10.4">
<h2><span class="header-section-number">10.4</span> Reducing bias and increasing transparency with preregistration and friends</h2>
<blockquote>
<p>When not planned beforehand, data analysis can approximate a projective technique, such as the Rorschach, because the investigator can project on the data his own expectancies, desires, or biases and can pull out of the data almost any ‚Äòfinding‚Äô he may desire.</p>
</blockquote>
<blockquote>
<footer>
‚Äî Theodore X. Barber <span class="citation">(<a href="#ref-barber1976" role="doc-biblioref">1976</a>)</span>
</footer>
</blockquote>
<p>Other related tools e.g., Registered reports, blind/masked analysis, cross validation, sensitivity analysis (Hardwicke and Wagenmakers 2021)</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span style="display:block;" id="fig:prereg-evidence"></span>
<img src="images/prereg/prereg-evidence.png" alt="Preregistration may be effective (Kaplan)" width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 10.4: Preregistration may be effective (Kaplan)<!--</p>-->
<!--</div>--></span>
</p>
<div class="case-study">
<p>üî¨ Case study: tale of two RCTs (Frank 2016)</p>
</div>
</div>
<div id="exploratory-and-confirmatory-research-pregistration-offers-the-best-of-both-worlds" class="section level2" number="10.5">
<h2><span class="header-section-number">10.5</span> Exploratory and confirmatory research: Pregistration offers the best of both worlds</h2>
</div>
<div id="how-to-preregister" class="section level2" number="10.6">
<h2><span class="header-section-number">10.6</span> How to preregister</h2>
<ul>
<li>Intro to Open Science Framework as one tool for timestamping your hypotheses, different kinds of preregistration templates.</li>
</ul>
<p>Reporting preregistered work ‚Äì distinguishing exploratory and confirmatory analyses, acknowledging departures from original plan</p>
<div class="accident-report">
<p>‚ö†Ô∏è Accident report: Amending your preregistration is better than not doing it at all.</p>
</div>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span style="display:block;" id="fig:reg-reports"></span>
<img src="images/prereg/registered-reports.png" alt="Registered Reports (https://www.cos.io/initiatives/registered-reports)" width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 10.5: Registered Reports (<a href="https://www.cos.io/initiatives/registered-reports" class="uri">https://www.cos.io/initiatives/registered-reports</a>)<!--</p>-->
<!--</div>--></span>
</p>
</div>
<div id="other-things" class="section level2" number="10.7">
<h2><span class="header-section-number">10.7</span> other things</h2>
<p>Preregistration of secondary data analysis (and caveats about causal inference). Not our main topic, but important.</p>
<p>Barriers to adoption of preregistration.</p>
<p>Limitations of the preregistration approach, especially with respect to iterative theory building (e.g., Navarro critique).</p>
<p>From reviewer: ‚ÄúA mention of what preregistration is not meant for. When teaching preregistration to new graduate students or undergraduates, I find that students often get hung up on trying to ‚Äúguess‚Äù the outcome of their experiment correctly, even when they have little theoretical or empirical background for their predictions. Preregistration isn‚Äôt about testing our precognition skills, though, so I find it helpful to address this issue early on.‚Äù</p>
<p>Why preregister? (Hardwicke and Wagenmakers 2021)</p>
<ul>
<li>Reduce bias ‚Äì addresses the problem of researcher degrees of freedom by reducing exposure to potentially biasing information and reduces selective reporting?</li>
<li>Calibrate confidence ‚Äì helps to gauge risk of bias thereby enabling proper interpretation of confirmatory vs exploratory results.</li>
<li>Increase transparency ‚Äì it‚Äôs not a straightjacket that prevents you from doing anything else, it‚Äôs a snapshot of your current thinking</li>
<li>Front-load careful thinking ‚Äì forces you to sit down and be careful about your thinking, often catch conceptual issues and save time in the long-run</li>
</ul>
<div class="exercise">
<p><span id="exr:unlabeled-div-1" class="exercise"><strong>Exercise 10.1  </strong></span>P-hack your way to scientific glory!</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-2" class="exercise"><strong>Exercise 10.2  </strong></span>Preregister your next experiment!</p>
</div>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-barber1976" class="csl-entry">
Barber, T. X. (1976). <em>Pitfalls in <span>Human</span> <span>Research</span>: <span>Ten</span> <span>Pivotal</span> <span>Points</span></em>. Pergamon Press.
</div>
<div id="ref-bennett2009" class="csl-entry">
Bennett, C., Miller, M., &amp; Wolford, G. (2009). Neural correlates of interspecies perspective taking in the post-mortem <span>Atlantic</span> <span>Salmon</span>: An argument for multiple comparisons correction. <em>NeuroImage</em>, <em>47</em>, S125. <a href="https://doi.org/10.1016/S1053-8119(09)71202-9">https://doi.org/10.1016/S1053-8119(09)71202-9</a>
</div>
<div id="ref-feynman1974" class="csl-entry">
Feynman, R. P. (1974). <em>Cargo <span>Cult</span> <span>Science</span></em>. <a href="http://calteches.library.caltech.edu/51/2/CargoCult.pdf">http://calteches.library.caltech.edu/51/2/CargoCult.pdf</a>
</div>
<div id="ref-gelman2014" class="csl-entry">
Gelman, A., &amp; Loken, E. (2014). The statistical crisis in science. <em>American Scientist</em>, <em>102</em>(6), 460‚Äì465. <a href="https://doi.org/10.1511/2014.111.460">https://doi.org/10.1511/2014.111.460</a>
</div>
<div id="ref-good1972" class="csl-entry">
Good, I. J. (1972). Statistics and <span>Today</span>‚Äôs <span>Problems</span>. <em>The American Statistician</em>, <em>26</em>(3), 11‚Äì19. <a href="https://doi.org/10.1080/00031305.1972.10478922">https://doi.org/10.1080/00031305.1972.10478922</a>
</div>
<div id="ref-degroot2014" class="csl-entry">
Groot, A. D. de. (2014). The meaning of <span>‚Äúsignificance‚Äù</span> for different types of research (E.-J. Wagenmakers, D. Borsboom, J. Verhagen, R. A. Kievit, M. Bakker, A. O. J. Cramer, D. Matzke, D. Mellenbergh, &amp; H. L. J. van der Maas, Trans.). <em>Acta Psychologica</em>, <em>148</em>, 188‚Äì194. <a href="https://doi.org/10.1016/j.actpsy.2014.02.001">https://doi.org/10.1016/j.actpsy.2014.02.001</a>
</div>
<div id="ref-ioannidis2005" class="csl-entry">
Ioannidis, J. P. A. (2005). Why most published research findings are false. <em>PLOS Medicine</em>, <em>2</em>(8), e124. <a href="https://doi.org/10.1371/journal.pmed.0020124">https://doi.org/10.1371/journal.pmed.0020124</a>
</div>
<div id="ref-oberauer2019" class="csl-entry">
Oberauer, K., &amp; Lewandowsky, S. (2019). Addressing the theory crisis in psychology. <em>Psychonomic Bulletin &amp; Review</em>, <em>26</em>(5), 1596‚Äì1618.
</div>
<div id="ref-veldkamp2017" class="csl-entry">
Veldkamp, C. L. S., Hartgerink, C. H. J., Assen, M. A. L. M. van, &amp; Wicherts, J. M. (2017). Who believes in the storybook image of the scientist? <em>Accountability in Research</em>, <em>24</em>(3), 127‚Äì151. <a href="https://doi.org/10.1080/08989621.2016.1268922">https://doi.org/10.1080/08989621.2016.1268922</a>
</div>
</div>
<p style="text-align: center;">
<a href="9-sampling.html"><button class="btn btn-default">Previous</button></a>
<a href="11-selection.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="toc/toc.js"></script>


</body>
</html>
