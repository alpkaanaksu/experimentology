--- 
title: "Experimentology"
subtitle: "An Open Science Approach to Experimental Psychology Methods"
author: "Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams"

site: bookdown::bookdown_site
documentclass: book
classoption: twoside,symmetric

bibliography: experimentology.bib
csl: apa.csl
link-citations: yes
---

# {.unlisted .unnumbered}

Placeholder


## Introduction {-}
## What this book is and isn't about {-}
## How to use this book {-}
## Themes {-}
## The software toolkit of the behavioral researcher (and of this book) {-}

<!--chapter:end:index.Rmd-->


# (PART) Preliminaries {-}
# Experiments {#experiments}

Placeholder


## What is an experiment, and why would you do one?
## Causal inference
## Randomization
## Who are we measuring? 
## Experiments: Chapter summary

<!--chapter:end:001-experiments.Rmd-->


# Theories {#theories}

Placeholder


## Psychological theories
## Theory testing: Popper, Kuhn, and Lakatos, oh my!
## Models and theories 
## Theory testing in psychology
## Theories, frameworks, and paradigms
## Theories: Chapter summary

<!--chapter:end:002-theories.Rmd-->


# Replication and reproducibility  {#replication}

Placeholder


## Reproducibility
## Replication
### Conceptual frameworks for replication
### The meta-science of replication
## Causes of replication failure
### Context, moderators, and expertise
### P-hacking and publication bias
## Replication, reproducibility, theory building, and open science
### Reciprocity between replication and theory
### Necessary but not sufficient
### Open science
## Chapter summary: Replication and reproducibility

<!--chapter:end:003-replication.Rmd-->


# Ethics {#ethics}

Placeholder


## Ethical frameworks
### Consequentialist theories
### Deontological and virtue-based approaches
### Deontological principles for research
## Ethical responsibilities to research participants
### Institutional review boards
## Ethical responsibilities in analysis and reporting of research 
## Ethical responsibilities to the broader scientific community
## Chapter summary: Ethics 

<!--chapter:end:004-ethics.Rmd-->


# (PART) Statistics {-}
# Estimation {#estimation}

Placeholder


## Estimating a quantity
### Maximum likelihood estimation
### Estimating variation in ratings
### Bayesian estimation
## Estimating and comparing effects
### Estimating the treatment effect 
### Measures of effect size
### Pros and cons of standardizing effect sizes
## Summary: Estimation

<!--chapter:end:005-estimation.Rmd-->


# Inference {#inference}

Placeholder


## Sampling variation
### Standard errors
### The central limit theorem
## From variation to inference
## Making inferences
### Bayes Factors
### *p*-values
### The Neyman-Pearson approach
## Inference and its discontents
### Problems with the interpretation of *p*
### Philosophical (and empirical) views of probability
## Estimating precision
### Confidence intervals
### Confidence in confidence intervals? 
## Chapter summary: Inference

<!--chapter:end:006-inference.Rmd-->


# Models {#models} 

Placeholder


## Regression models
### Regression for estimating a simple treatment effect
### Adding predictors
### When does linear regression work?
## Generalized linear models
## Accommodating clustering in our models
### Linear mixed models
### An alternative approach: Generalized estimating equations
## How do you use models to analyze data? 
### When does it makes sense to include covariates in a model?
## Chapter summary: Models

<!--chapter:end:007-models.Rmd-->


# (PART) Design and Planning {-}
# Measurement {#measurement}

Placeholder


## Reliability
### Measurement scales
### Reliability is relative
### Practical advice for computing reliability
## Validity
## How to select a good measure?
### What to measure? 
### Survey measures
## The temptation to measure lots of things
## Chapter summary

<!--chapter:end:008-measurement.Rmd-->


# Design of experiments {#design}

Placeholder


## Experimental designs
### A two-factor experiment
### Generalized factorial designs
### Between- vs. within-participant designs
### Repeated measurements and experimental items
### Discrete and continuous experimental manipulations 
## Choosing your manipulation
### Internal validity threats: Confounding 
### Internal validity threats: Placebo, demand, and expectancy
### External validity of manipulations
## Summary: Experimental design

<!--chapter:end:009-design.Rmd-->


# Sampling {#sampling}

Placeholder


## Sampling theory
### Generalization and the goals of sampling
### Beyond participants: sampling items 
## Sample planning 
### Classic power analysis
### Other approaches to sample size planning
## Issues in sampling 
### Sampling biases
### Heterogeneity and stratification
### Limitations statements
## Summary: Sampling

<!--chapter:end:010-sampling.Rmd-->


# Experimental strategy {#strategy}

Placeholder



<!--chapter:end:011-selection.Rmd-->


# (PART) Execution {-}
# Preregistration {#prereg}

Placeholder


## Lost in a garden of forking paths
### Results-dependent analysis
### Hypothesize after results are known?
## Reducing bias, increasing transparency, and calibrating confidence with preregistration
## How to preregister
## Chapter summary: Preregistration

<!--chapter:end:012-prereg.Rmd-->


# Recruitment and Consent {#consent}

Placeholder


### Informed consent
### Debriefing participants
### Special considerations for vulnerable populations

<!--chapter:end:013-consent.Rmd-->


# Data collection {#collection}

Placeholder


## The participant's perspective
### Data collection in person
### Data collection online
## Ensuring high quality data
### Run pilot studies
### Keep consistent data collection records
### Measure participant compliance
## Chapter summary: Data collection

<!--chapter:end:014-collection.Rmd-->


# Project management {#management}

Placeholder


## Project Management
### Organizing your project
### Versioning
### File names
## Data Management
### Save your raw data
### Document your data collection process
### Organize your data for later analysis (spreadsheet version)
### Organize your data for later analysis (software version)
### Document the format of your data
## Sharing Research Products
### What you can and can't share
### Where and how to share ("If you like it then you've gotta put a license on it")
## Chapter summary

<!--chapter:end:015-manangement.Rmd-->


# (PART) Analysis and Reporting {-}
# Visualization {#viz}

Placeholder


## Basic principles of (confirmatory) visualization
### Principle 1: Show the design
### Principle 2: Facilitate comparison
### Principle 3: Show the data
### Principle 4: Maximize information, minimize ink
## Exploratory visualization
### Distributional information
### Data diagnostics
## Conclusion

<!--chapter:end:016-viz.Rmd-->


# Exploratory data analysis {#eda}

Placeholder


## Sensitivity Analysis
## Data exploration
## Exploratory modeling and theory-building

<!--chapter:end:017-eda.Rmd-->


# Writing {#writing}

Placeholder


## Writing clearly
## Writing reproducibly
## Writing responsibly
### Authorship
### Disclosures
### Generalizability Statements
## Post-publication

<!--chapter:end:018-writing.Rmd-->

```{r include=FALSE, cache=FALSE}
library(tidyverse)
library(glue)
library(here)
library(knitr)
library(shiny)
library(metafor)
library(ggrepel) # viz
library(GGally) # viz
library(BayesFactor) # inference
library(papaja)


opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  comment = "#>",
  collapse = TRUE,
  cache = TRUE,
  echo = FALSE,
  cache.lazy = FALSE,
  dev = "png",
  dpi = 300,
  out.width = "\\linewidth"
)
kable <- function(...) knitr::kable(..., booktabs = TRUE, linesep = "")

set.seed(42)

.font <- "Source Sans Pro"
if (!(.font %in% sysfonts::font_families()))
  sysfonts::font_add_google(.font, .font)
showtext::showtext_auto()

theme_set(theme_bw(base_size = 14, base_family = .font))
theme_update(panel.grid = ggplot2::element_blank(),
             strip.background = ggplot2::element_blank(),
             legend.key = ggplot2::element_blank(),
             panel.border = ggplot2::element_blank(),
             axis.line = ggplot2::element_line(),
             strip.text = ggplot2::element_text(face = "bold"))
.grey <- "grey70"
.refline <- "dotted"
.coef_line <- element_line(colour = .grey, size = 0.1)

.pal <- ggthemes::ptol_pal
.scale_colour_discrete <- ggthemes::scale_colour_ptol
.scale_color_discrete <- .scale_colour_discrete
.scale_fill_discrete <- ggthemes::scale_fill_ptol

.scale_colour_continuous <- viridis::scale_colour_viridis
.scale_color_continuous <- .scale_colour_continuous
.scale_fill_continuous <- viridis::scale_fill_viridis

.scale_colour_numerous <- scale_colour_discrete
.scale_color_numerous <- .scale_colour_numerous
.scale_fill_numerous <- scale_fill_discrete

# from https://bookdown.org/yihui/rmarkdown-cookbook/font-color.html
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
            x)
  } else x
}
```
# Meta-analysis {#meta}

```{r meta-helper-child, child="helper/meta_prelims.Rmd"}
```

:::{.learning-goals}
üçé Learning goals:

- Discuss the benefits of synthesizing evidence across studies
- Conduct a simple fixed- or random-effects meta analysis
- Reason about the role of within- and across-study biases in meta-analysis
:::

Throughout this book, we have focused on the importance of using experiments to estimate effects. Our focus has been on design decisions that maximize the precision of our estimates and minimize their bias. But even when we do our best to get a precise, unbiased effect size estimate, we must also acknowledge the role of participant, stimulus, and methodological variability in limiting both the precision and the generalizability of our estimates. 

Both before and after conducting an experiment, a review of the literature is critical for understanding sources of variation in a particular effect of interest. The term "review" might sound like you throw a search term into Google Scholar, download articles that look topical or interesting, spend a few days reading those articles, and then write a summary. While this ad-hoc technique is one of many ways of performing a review of the literature [@grant2009typology], it doesn't provide any kind of quantitative expectation about a particular effect. Further, it doesn't tell you anything about potential biases in the literature -- for example, a bias for the publication of positive effects. 

To address these questions, a more systematic, quantitative review of the literature is required. This chapter focuses on a specific type of quantitative review called **meta-analysis**. Meta-analysis is a method for combining effect sizes across different measurements. (If you need a refresher on effect size, flip back to Chapter \@ref(estimation), where we introduce the concept).^[We'll exclusively be using Cohen's $d$, the standardized difference between means, which we introduced in Chapter \@ref(estimation). There are many more varieties of effect size available, but we focus here on $d$ because it's common and easy to reason about in the context of the statistical tools we introduced in the earlier sections of the book.]

By combining information from multiple studies, meta-analysis often provides more precise estimates of an effect size than any single study. In addition, meta-analysis also allows the researcher to look at the extent to which an effect varies across studies. If an effect does vary across studies, meta-analysis also can be used to test whether certain study characteristics systematically produce different results (e.g., whether an effect is larger in certain populations). 

:::{.case-study}
üî¨ Case study: Towel reuse by hotel guests

Imagine you are staying in a hotel and you have just taken a shower. Do you throw the towels on the floor or hang them back up again? In a widely-cited study on the power of social norms, @goldstein2008room manipulated whether a hotel sign encouraging towel reuse focused on environmental impact (e.g., "help save the environment") or social norms (e.g., "most guests re-use their towels"). Across two studies, they found that guests were significantly more likely to reuse their towels after receiving the social norm message (Study 1 log OR = 0.38, 95% CI [0.00, 0.77], $p = .05$; Study 2 log OR = 0.30, 95% CI [0.04, 0.57], $p = .03$). 

Five subsequent studies, however, did not find significant evidence of that social norm messaging increased towel resuse (log Odds Ratios were between -1.46 and 0.29, and no hypothesis-consistent $p$-values was less than .05). This caused many researchers to wonder if there is any effect at all.  To examine this question, @scheibehenne2016 statistically combined all the evidence via meta-analysis. When they did, they found that the social norm messages did have a significant overall effect on hotel towel reuse (log OR = 0.23, 95% CI [0.07, 0.38], $p < .005$). This study demonstrates an important strength of meta-analysis: by pooling evidence from multiple studies, meta-analysis can generate more powerful insights than any one study alone. [MM: If we include effect sizes above, here we could allude to them again: "notice how the point estimates are large, even though the p-values are >0.05".].
:::

<!-- All ES's come from Wagenmakers et al. SI -->

Meta-analysis often teaches us something about a body of evidence that we do not intuitively grasp when we casually read through a bunch of articles. Merely reading the individual studies  would give the impression that social norm messages do not increase hotel towel re-use. But meta-analysis indicated that there is an effect *overall* .^[Given the number of hotel bookings worldwide -- 1.7 billion in the European Union alone in 2013 [@kotzeva2015eurostat] -- the insight provided by the meta-analysis is not trivial!]

## The basics of evidence synthesis

The hotel towel-use example is a good starting point, but there is much more we can do with meta-analysis. To illustrate, we'll turn to another example: a meta-analysis of studies investigating the "contact hypothesis" on intergroup relations. 

According to the contact hypothesis, prejudice towards members of minority groups can be reduced through intergroup contact interventions: interventions where members of majority and minority groups come together in the pursuit of a common goal [@allport1954nature].To test this, @paluck2019contact meta-analyzed randomized studies that tested the effects of intergroup contact interventions on long-term prejudice-related outcomes. Using a systematic literature search, they tried to identify all published papers that tested these effects and then extracted effect size estimates from each paper. Because not every paper reports standardized effect sizes -- or even means and standard deviations for every group -- this process can often involve scraping information from plots, tables, and statistical tests to try to reconstruct effect sizes.

As we've seen throughout this book, visualizing data before and after analysis helps benchmark and check our intuitions about the formal statistical results. In a meta-analysis, a common way to do so is the **forest plot**, which depicts individual studies' estimates and confidence intervals.^[You can ignore for now the column of percentages and the final line, "RE Model"; we will return to these later.] In the forest plot in Figure \@ref(fig:meta-forest), the larger squares correspond to more precise studies; notice how much narrower their confidence intervals are than the confidence intervals of less precise studies. 


```{r meta-forest, fig.cap="Forest plot for Paluck et al. meta-analysis. Studies are ordered from smallest to largest standard error.", fig.height=8}
metafor::forest(re.m,
                showweights = TRUE,
                header = TRUE,
                xlab = "Standardized mean difference")
```

<!-- Cohen's $d$ -- which, if you recall from Chapter \@ref(estimation), represents the standardized mean difference -- was used as the effect size index. As we show in the remainder of the chapter, the meta-analytic tools @paluck2019contact used provide several useful insights about this proposed prejudice-reduction intervention. -->

### How not to synthesize evidence

Many people's first instinct to synthesize evidence is to count how many studies support versus did not support the hypothesis under investigation. This technique usually amounts to counting the number of studies with "significant" $p$-values, since (for better or for worse) "significance" is largely what drives the take-home conclusions researchers report [@mcshane2017statistical; @nelson1986interpretation]. In meta-analysis, we call this practice of counting the number of significant $p$-values **vote-counting** [@borenstein2021introduction]. 

For example, in the @paluck2019contact meta-analysis, all studies had positive effect sizes, but only approximately `r sum(DF$sig)` of `r nrow(DF)` were significant. So, based on this vote-count, we would have the impression that most studies do not support the contact hypothesis.

Many literature reviews use this vote-counting approach -- albeit often not explicitly. Despite its intuitive appeal, vote-counting can be very misleading; it characterizes evidence solely in terms of dichotomized $p$-values, while entirely ignoring effect sizes. In Chapter \@ref(replication), we saw how this fetishism of statistical significance can mislead us when we consider individual studies. These problems also apply when considering multiple studies. [MM: I am not sure I agree with this last sentence. NC: how about this new one?]

For example, small studies may consistently produce non-significant effects due to their limited power. But when many such studies are combined in a meta-analysis, the meta-analysis may provide strong evidence of a positive average effect. Inversely, many studies might have statistically significant effects, but if their effect sizes are small, then a meta-analysis might might indicate that the average effect size is too small to be practically meaningful. In these cases, vote-counting based on statistical significance can lead us badly astray [@borenstein2021introduction]. To avoid these pitfalls, meta-analysis combines the effect sizes estimates from each study (not just their $p$-values), weighting them in a principled way.

### Fixed effects meta-analysis

If vote-counting is a bad idea, how should we combine results across studies? Another intuitive approach might be to average effect sizes from each study. For example, in Paluck et al.'s meta-analysis, the mean of the studies' effect size estimates is `r round( mean(DF$d), digits )`. This averaging approach is a step in the right direction, but it has an important limitation: averaging effect size estimates gives equal weight to each study. A small study [e.g.,  N=`r DF[27, ]$n_total`, @clunies1989changing] contributes as much to the mean effect size as a large study [e.g., N=`r DF[1, ]$n_total`, @boisjoly2006empathy]. Larger studies provide more precise estimates of effect sizes than small studies, so weighting all studies equally is not ideal. Instead, larger studies should carry more weight in the analysis. 

To address this issue, **fixed-effects meta-analysis** uses a **weighted average** approach. Larger, more precise studies are given more weight in the calculation of the overall effect size. Specifically, each study is weighted by the inverse of its variance (i.e., the inverse of its squared standard error). This makes sense because larger, more precise studies have smaller variances, and thus get more weight in the analysis. 

In general terms, the fixed-effect pooled estimate is:

$$\widehat{\mu} = \frac{ \sum_{i=1}^k w_i \widehat{\theta}_i}{\sum_{i=1}^k w_i}$$ where $k$ is the number of studies, $\widehat{\theta}_i$ is the point estimate of the $i^{th}$ study, and $w_i = 1/\widehat{\sigma}^2_i$ is study $i$'s weight in the analysis (i.e., the inverse of its variance).[^9]

<!-- In Paluck et al.'s meta-analysis, we would calculate the fixed-effect estimate, $\widehat{\mu}$, as: -->

<!-- <!-- hard-coded from DF$d[1:2] and DF$se_d[1:2] -->
<!-- $$\widehat{\mu} = \frac{ \frac{\widehat{\theta}_{study1}}{\widehat{\sigma}^2_{study1}} + \frac{\widehat{\theta}_{study2}}{\widehat{\sigma}^2_{study2}} + \cdots}{ \frac{1}{\widehat{\sigma}^2_{study1}} + \frac{1}{\widehat{\sigma}^2_{study2}} + \cdots } = -->
<!-- \frac{ \frac{0.03}{0.08^2} + \frac{0.30}{0.08^2} + \cdots }{ \frac{1}{0.08^2} + \frac{1}{0.08^2} + \cdots }$$ -->

We can use the fixed-effects formula to estimate that the overall effect size in Paluck et al.'s meta-analysis studies is a standardized mean difference of $\widehat{\mu}$ = `r round(as.numeric(fe.m$b), 2)`; 95% confidence interval [`r paste0(round(fe.m$ci.lb, 2), ", ", round(fe.m$ci.ub, 2))`]; $p < .001$. Because Cohen's $d$ is our effect size index, this estimate means that intergroup contact decreased prejudice by `r round(as.numeric(fe.m$b), 2)` standard deviations.


[^9]: If you are curious, the standard error of the fixed-effect $\widehat{\mu}$ is $\frac{1}{\sum_{i=1}^k w_i}$. This standard error can be used to construct a confidence interval or $p$-value, as described in Chapter \@ref(inference).


### Limitations of fixed-effects meta-analysis

One of the limitations of fixed-effect meta-analysis is that it assumes that the true effect size is, well, *fixed*! In other words, fixed-effect meta-analysis assumes that there is a single effect size that all studies are estimating. This is a bold assumption. For example, imagine that intergroup contact decreases prejudice when the group succeeds at its goal, but *increases* prejudice when the group fails at its goal. If we meta-analyzed two studies, one in which intergroup contact substantially increased prejudice, and one in which intergroup contact substantially decreased prejudice, it might would appear that the true effect of intergroup contact is close to zero. 

In Paluck et al.'s meta-analysis, studies differed in several ways that could lead to different true effects. For example, some studies recruited adult participants while others recruited children. If we assume that intergroup contact works different for adults versus children, then it is misleading to talk about a single (i.e., fixed) intergroup contact effect. Instead, we would say that the effects of intergroup contact vary across studies, an idea called **heterogeneity**. 

Does this presence of heterogeneity remind you of anything from when we analyzed repeated-measures data in Chapter \@ref(models) on models? Yes! Recall that, with repeated-measures data, we had to deal with the possibility of heterogeneity across participants -- and of the ways we did so was by introducing participant-level random intercepts to our regression model. It turns out that we can do a similar thing in meta-analysis, dealing with heterogeneity across studies.

### Random-effects meta-analysis

While fixed-effect meta-analysis essentially assumes that all studies in the meta-analysis have the same population effect size, $\mu$, random-effects meta-analysis instead postulates that studies' population effects come from a normal distribution with mean $\mu$ and standard deviation $\tau$.[^10] The larger the standard deviation, $\tau$, the more heterogeneous the effects are across studies. A random-effects model then estimates both $\mu$ and $\tau$, for example by maximum likelihood [@dersimonian1986meta; @brockwell2001comparison].[^11]

[^10]: Technically, other specifications of random-effects meta-analysis are possible. For example, robust variance estimation does not require making assumptions about the distribution of effects across studies [@hedges2010robust]. These approaches also have other substantial advantages, like their ability to handle effects that are clustered [e.g., because some papers contribute multiple estimates; @hedges2010robust; @pustejovsky2021meta] and their ability to provide better inference in meta-analyses with relatively few studies [@tipton2015small]. For these reasons, we tend to use these robust methods by default when conducting meta-analyses.

[^11]: A confidence interval and $p$-value for the random-effects estimate $\widehat{\mu}$ can be obtained using standard theory for maximum likelihood estimates with an additional adjustment that helps account for uncertainty in estimating $\tau$ [@knapp2003improved].


Like fixed-effect meta-analysis, the random-effects estimate of $\widehat{\mu}$ is still a weighted average of studies' effect size estimates: $$\widehat{\mu} = \frac{ \sum_{i=1}^k w_i \widehat{\theta}_i}{\sum_{i=1}^k w_i}$$

However, in random-effects meta-analysis, the inverse-variance weights now incorporate heterogeneity: $w_i = 1/\left(\widehat{\tau}^2 + \widehat{\sigma}^2_i \right)$. Where before we had one term in our weights, now we have two. That is because these weights represent the inverse of studies' *marginal* variances, taking into account both statistical error due to their finite sample sizes ($\widehat{\sigma}^2_i$) and also genuine effect heterogeneity ($\widehat{\tau}^2$).[^12] 

[^12]: The estimate of $\widehat{\tau}^2$ is a bit more complicated, but is essentially a weighted average of studies' residuals, $\widehat{\theta_i} - \widehat{\mu}$, while subtracting away variation due to statistical error, $\widehat{\sigma}^2_i$ [@dersimonian1986meta; @brockwell2001comparison].


Conducting a random-effects meta-analysis of Paluck et al.'s dataset yields $\widehat{\mu}$ = `r round(as.numeric(re.m$b), 2)`; 95% confidence interval [`r paste0(round(re.m$ci.lb, 2), ", ", round(re.m$ci.ub, 2))`]; $p < .001$. That is, they estimated that, *on average across studies*, intergroup contact was associated with a decrease in prejudice of `r round(as.numeric(re.m$b), 2)` standard deviations. This meta-analytic estimate is shown as the bottom line of Figure \@ref(fig:meta-forest).

However, these effects appeared to differ across studies. Paluck et al. estimated that the standard deviation of effects across studies was $\widehat{\tau}$ = `r tauStats[1]` ; 95% confidence interval [`r tauStats[2]`, `r tauStats[3]`]. This estimate indicates a substantial amount of heterogeneity!  To conveniently visualize these results, we can plot the estimated density of the population effects, which is just a normal distribution with mean $\widehat{\mu}$ and standard deviation $\widehat{\tau}$ (Figure \@ref(fig:meta-densities)).

This meta-analysis highlights an important point:that the overall effect size  estimate $\widehat{\mu}$ represents only the *mean* population effect across studies. It tells us nothing about how much the effects *vary* across studies. Thus, we recommend always reporting the heterogeneity estimate $\widehat{\tau}$, preferably along with other related metrics [@riley2011interpretation; @wang2019simple; @mathur_mam; @npphat]. Reporting the heterogeneity helps readers know how consistent or inconsistent the effects are across studies, which may point to the need to investigate *moderators* of the effect (i.e., factors that are associated with larger or smaller effects, such as whether participants were adults or children).[^13] 

[^13]: One common approach to investigating moderators in meta-analysis is meta-regression, in which moderators (e.g., type of intergroup contact) are included as covariates in a random-effects meta-analysis model [@thompson2002should]. As in standard regression, coefficients can then be estimated for each moderator, representing the mean difference in population effect between studies with versus without the moderator.

```{r meta-densities, fig.margin = TRUE, fig.cap= "Estimated distribution of population effects from random-effects meta-analysis of Paluck et. al's dataset (heavy red curve) and estimated density of studies' point estimates (thin black curve)."}
# red line: fitted density of *population* effects from meta-analysis
# gray line: just a dumb density estimate from the *point estimates*
library(ggplot2)

ggplot(data = data.frame(x = c(-2, 2)),
       aes(x)) +
  
  # reference lines
  geom_vline(xintercept = 0,
             lwd = 1,
             color = "gray") +
  
  geom_vline(xintercept = re.m$b,
             lty = 2,
             lwd = 1,
             color = "red"
  ) +
  
  # estimated density of estimates (nonparametric)
  geom_density( data = DF,
                aes(x = d),
                adjust = 1.2 ) +
  
  # estimated density from meta-analysis (parametric)
  stat_function( fun = dnorm,
                 n = 101,
                 args = list( mean = re.m$b,
                              sd = sqrt(re.m$tau2) ),
                 lwd = 1.2,
                 color = "red") +

  ylab("") +
  scale_x_continuous( breaks = seq(-2, 2, 0.5)) +
  xlab("Standardized mean difference") +
  theme_minimal() +
  scale_y_continuous(breaks = NULL) +
  theme(text = element_text(size=16),
        axis.text.x = element_text(size=16))
```


## Bias in meta-analysis

Meta-analysis is an invaluable tool for synthesizing evidence across studies. However, the accuracy of meta-analysis can be compromised by two categories of bias: **within-study biases** and **across-study biases**. Either type can lead to meta-analysis estimates that are too large, too small, or in the wrong direction. We will now discuss examples of each type of bias as well as ways to address these biases when conducting a meta-analysis. This includes mitigating the biases at the outset through sound meta-analysis design and also assessing the robustness of the ultimate conclusions to possible remaining bias.

### Within-study biases

Within-study biases -- such as demand characteristics, confounds, and order effects -- not only impact the validity of individual studies, but also any attempt to synthesize those studies. In other words: garbage in, garbage out. For example, @paluck2019contact noted that early studies on intergroup contact almost exclusively used non-randomized designs. Imagine a hypothetical study where researchers studied a completely ineffective intergroup contact intervention, and non-randomly assigned low-prejudice people to the intergroup contact condition and high-prejudice people to the control condition. In a scenario like this, the researcher will, of course, find that the prejudice was lower in the intergroup contact condition. However, this is not a true effect of the contact intervention, but rather a spurious effect of non-random assignment (i.e., confounding). Now imagine meta-analyzing many studies with similarly poor designs. The meta-analyst would find impressive evidence of an intergroup contact effect, but this is simply driven by systematic non-random assignment. 

To mitigate this problem, meta-analysts often exclude studies that may be affected by within-study bias. For example, @paluck2019contact excluded non-randomized studies to avoid concerns about confounding. Preferably, inclusion and exclusion criteria for meta-analyses should be preregistered to ensure that these decisions are applied in a systematic and uniform way, rather than being applied after looking at the results. 

Sometimes, though, certain sources of bias cannot be eliminated through exclusion criteria, for example because it is not possible or ethical to randomize the independent variable. After data have been collected, meta-analysts should also assess studies' risks of bias qualitatively using established rating tools [@sterne2016robins]. Doing so allows the meta-analyst to communicate how much within-study bias there may be.^[@paluck2019contact did not use this tool, but they could have used it to communicate, for example, the extent to which participants might have differentially dropped out of the study.] Meta-analysts can also conduct sensitivity analyses to assess how much results might be affected by different within-study biases or by excluding certain types of studies [@art]. For example, if nonrandom assignment is a concern, a meta-analysts may run the analyses including only randomized studies, versus including all studies, in order to determine if including nonrandomized studies changes the meta-analysis results. These two options parallel our discussion of experimental preregistration in Chapter \@ref(prereg): To allay concerns about results-dependent meta-analysis, researchers can either pre-register their analyses ahead of time or else be transparent about their choices after the fact. Sensitivity analyses can allay concerns that a specific choice of exclusion criteria is critically related to the reported results. 

<!-- It might be nice to show the reader an example of a risk of bias rating scale. Doing so would also help clarify what types of within-study biases meta-analysts are typically concerned about. https://drive.google.com/file/d/1Q4Fk3HCuBRwIDWTGZa5oH11OdR4Gbhdo/view-->

### Across-study biases

Across-study biases occur if, for example, researchers **selectively report** certain types of findings or selectively publishing certain types of findings (**publication bias**). Often, these across-study biases favor statistically significant positive results, which means the meta-analysis of the available results will be inflated. For example, if researchers publish only the studies that yield statistically significant positive results and hide the studies that don't, statistically combining the published studies via meta-analysis will obviously lead to exaggerated effect size estimates. [MM: Suggest removing some "garbage in, garbage out" statements to avoid excessively giving the impression that meta-analyses are useless when there are any biases.]

::: accident-report
üî¨ Accident Report: Quantifying publication bias in the social sciences

In 2014, @franco2014 and colleagues examined the population of 221 studies conducted through a funding initiative called TESS ("time-sharing experiments in the social sciences") that helps researchers run experiments on nationally-representative samples in the U.S. This sample of studies was unique in that everyone that used TESS had to register their study with the group. That meant that Franco et al. knew the whole universe of studies that had been conducted using TESS -- a key piece of information that meta-analysts almost never have available. 

Using this information, Franco and colleagues examined the records of these studies to determine whether the researchers found statistically significant results, a mixture of statistically significant and non-significant results, or only non-significant results. Then, they examined the likelihood that these results were published in the scientific literature.

Which results do you think were most likely to be published? If you guessed "significant results," then you have a good intuition about publication bias. Over 60% of studies with statistically significant results were published, compared to the mere 25% of studies that produced only statistically non-significant results. This finding was important because it quantified how strong that bias was, at least for one sample of studies. 
:::

Like within-study biases, meta-analysts often try to mitigate across-study biases by being careful about what studies make it into the meta-analysis. Meta-analysts don't only want to capture high-profile, published studies on their effect of interest, but also studies published in low-profile journals and the so-called "gray literature" [i.e., unpublished dissertations and theses; @lefebvre2019searching].[^5]

[^5]: Evidence is mixed regarding whether including gray literature actually reduces across-study biases in meta-analysis [@tsuji2020addressing; @sapbe], but it is still common practice to try to include this literature.

There are also statistical methods to help assess how robust the results may be to across-study biases. Among the most popular tools to assess and correct for publication bias is the **funnel plot** [@duval2000trim; @egger1997bias]. A funnel plot shows the relationship between studies' effect estimates and their precision (usually their standard error). They are called "funnel plots" because usually as the precision gets larger, the effects "funnel" towards the meta-analytic estimate. As the precision is smaller, they spread out more because of greater measurement error.

Here is an example of one type of funnel plot [@sapb] for a simulated meta-analysis of 100 studies with no publication bias:

```{r meta-funnel-unbiased, fig.cap = "Significance funnel plot for a meta-analysis simulated to have no publication bias. Orange points: studies with $p < 0.05$ and positive estimates. Grey points: studies with $p$ $\\ge$ $0.05$ or negative estimates. Black diamond: random-effects estimate of $\\widehat{\\mu}$ within all studies."}
# simulate meta with publication bias, but keep all studies
set.seed(451)
d.all = sim_data2( data.frame( k = 100,
                            per.cluster = 1,
                            mu = .5,
                            V = 1,
                            V.gam = 0,  # no clustering
                            sei.min = 0.05,
                            sei.max = 3,
                            true.dist = "norm",
                            SE.corr = FALSE,
                            select.SE = FALSE,
                            eta = 10  # set publication bias
),
keep.all.studies = TRUE )  

# published studies only
dp = d.all %>% filter(publish == TRUE)

meta.all = rma(yi = yi,
            vi = vi,
            data = d.all,
            method = "REML",
            knha = TRUE)

meta.NA = rma(yi = yi,
            vi = vi,
            data = d.all %>% filter(pval > 0.05 | yi < 0),
            method = "REML",
            knha = TRUE)

foo <- significance_funnel(yi = meta.all$yi,
                    vi = meta.all$vi,
                    favor.positive = TRUE,
                    plot.pooled = TRUE,
                    est.all = meta.all$b,
                    est.N = meta.NA$b,
                    xmin = -8, xmax = 8) 

# scale_x_continuous( breaks = seq(-2.5, 3.5, 0.5))
```

As implied by the "funnel" moniker, our plot looks a little funnel-like. Larger studies (those with smaller standard errors) cluster more closely around the mean of `r round(meta.all$b, digits)` than do smaller studies, but large and small studies alike have point estimates centered around the mean. That is, the funnel plot is symmetric.^[Classic funnel plots look more like Figure \@ref(fig:meta-classic-funnel). Our version is different in a couple of ways. Most prominently, we don't have the vertical axis reversed (which we think is confusing), and we don't have the left boundary highlighted (because we think folks don't typically select for negative studies).]

```{r meta-classic-funnel, fig.margin=TRUE, fig.cap = "Classic funnel plot."}
meta.p = rma(yi = yi,
            vi = vi,
            data = dp,
            method = "REML",
            knha = TRUE)

meta.p.NA = rma(yi = yi,
            vi = vi,
            data = dp %>% filter(pval > 0.05 | yi < 0),
            method = "REML",
            knha = TRUE)

funnel(meta.all, 
       xlab = "Standardized mean difference")
```


Not all funnel plots are symmetric! Figure \@ref(fig:meta-funnel-biased) is what happens to our hypothetical meta-analysis if all studies with $p<0.05$ and positive estimates are published, but only 10% of studies with $p \ge 0.05$ or with negative estimates are published. The introduction of publication bias dramatically inflates the pooled estimate from `r round(meta.all$b, digits)` to `r round(meta.p$b, digits)`.[^8] Also, there appears to be a correlation between studies' estimates and their standard errors, such that smaller studies tend to have larger estimates than do larger studies. This correlation is often called **funnel plot asymmetry** because the funnel plot starts to look like a right triangle rather than a funnel. Funnel plot asymmetry *can* be a diagnostic for publication bias, though it isn't always a perfect indicator, as we'll see in the next subsection.

 [MM: I think the significance funnel made more sense when there was some material about the sensitivity analyses. I'm entirely happy with the removal of that material, but now there's no antecedent for the worst-case estimate in the funnel plot. Seems maybe better to switch to usual contour-enhanced funnels given that? Or could briefly describe the worst-case estimate in the sidebar.]

```{r meta-funnel-biased, fig.cap = "Significance funnel plot for the same simulated meta-analysis after publication bias has occurred. Orange points: studies with $p < 0.05$ and positive estimates. Grey points: studies with $p$ $\\ge$ $0.05$ or negative estimates. Black diamond: random-effects estimate of $\\widehat{\\mu}$ within all studies. Grey diamond: random-effects estimate of $\\widehat{\\mu}$ within only studyes with $p$ $\\ge$ $0.05$ or negative estimates."}

foo <- significance_funnel(yi = meta.p$yi,
                    vi = meta.p$vi,
                    favor.positive = TRUE,
                    plot.pooled = TRUE,
                    est.all = meta.p$b,
                    est.N = meta.p.NA$b,
                    xmin = -8, xmax = 8) 

#p2 + scale_x_continuous( breaks = seq(-1, 3.5, 0.5) )
```
### Across-study bias correction

How do we identify and correct bias across studies? Starting with the funnel plot makes it look like the right thing to do is to quantify how asymmetric it is. Indeed, several popular statistical methods, such as Trim-and-Fill [@duval2000trim] and Egger's regression [@egger1997bias] are designed to quantify funnel plot asymmetry. But funnel plot asymmetry is not always representative of publication bias! 

Sometimes funnel plot asymmetry is driven by genuine differences in the effects being studied in small and large studies [@egger1997bias; @lau2006case]. For example, in a meta-analysis of intervention studies, if the most effective interventions are also the most expensive or difficult to implement, these highly effective interventions might be used primarily in the smallest studies ("small study effects"). Essentially, funnel plots and related methods can detect publication bias in which (1) small studies with large positive point estimates are more likely to be published than small studies with small or negative point estimates; and (2) the largest studies are published regardless of the magnitude of their point estimates. That model of publication bias is sometimes what is happening, but not always!

A more flexible approach for detecting publication bias uses **selection models**. These models can detect other forms of publication bias that funnel plots may not detect, such as publication bias that favors *significant* results. We won't cover these methods in detail here, but we think they are a better approach to the question, along with related sensitivity analyses.^[High-level overviews of selection models are given in @mcshane2016adjusting and @smt. For more methodological detail, see @hedges1984estimation, @iyengar1988, and @vevea1995. For a tutorial on fitting and interpreting selection models, see @smt. For sensitivity analyses, see @sapb.]

You may also have heard of "$p$-methods" to detect across-study biases such as $p$-curve and $p$-uniform [@simonsohn2014p; @simonsohn2014b; @van2015meta]. These methods essentially assess whether the significant $p$-values "bunch up" just under 0.05, which is taken to indicate publication bias. These methods are increasingly popular in psychology and have their merits. However, it is important to note that these methods are actually simplified versions of selection models [e.g., @hedges1984estimation] that work only under considerably more restrictive settings than do the original selection models [for example, when there is not heterogeneity across studies; @mcshane2016adjusting]. For this reason, it is usually (although not always) better to use selection models in place of the more restrictive $p$-methods.

In Paluck and colleagues' meta-analysis, they used a regression-based approach to assess and correct for publication bias. This approach provided significant evidence of a relationship between the standard error and effect size (i.e., an asymmetric funnel plot). This asymmetric funnel plot could be driven by small study effects, but it may be indicative of publication bias. Paluck and colleague subsequently used this same regression-based approach to try to correct for potential publication bias. Results from this model indicated that the bias-corrected effect size estimate was close to zero. In other words, even though all studies estimated that intergroup contact decreased prejudice, it is possible that there are unpublished studies that did not find this (or found that intergroup contact increased prejudice).

::: {.accident-report}
‚ö†Ô∏è Garbage in, garbage out? Meta-analyzing bad research 

Botox can help eliminate wrinkles. But some researchers have also suggested that it may help treat clinical depression when used to paralyze the muscles associated with frowning. As crazy as they may sound, a quick examination of the literature would lead many to conclude that this treatment works. Studies that randomly assign depressed patients to either receive Botox injections or saline injections do indeed find that Botox recipients exhibit decreases in depression. And when you combine all available evidence in a meta-analysis, you find that this difference is quite large: $\widehat{d}$ = 0.83, 95% CI [0.52, 1.14]. Unfortunately, this meta-analytic result has problems with both within- and between-study bias [@coles2019does]. 

First, participants are not supposed to know whether they have been randomly assigned to receive Botox or a control saline injections. However, only one of these treatments leads the upper half of your face to be paralyzed! After a couple of weeks you're pretty likely to figure out whether you received the Botox treatment or control saline injection. Thus, the apparent effect of Botox on depression could very well be a placebo effect, a clear within-study bias issue. 

Second,  51% of the outcomes measured were found not to have been reported by the study authors. This finding raises concerns about selective reporting: perhaps researchers examining the effects of Botox on depression only report the outcomes that demonstrate an effect, but not those that do not. In this scenario, any meta-analytic conclusions are potentially undermined by between-study bias.
:::

## Chapter summary: Meta-analysis

Taken together, Paluck and colleagues' use of meta-analysis provided several important insights that would have been easy to miss in a non-quantitative review. First, despite a preponderance of non-significant findings, real-world intergroup contact interventions were estimated to significant decrease prejudice by `r round(as.numeric(re.m$b), 2)` standard deviations. On the other hand, there was significant heterogeneity in intergroup contact effects, suggesting that there are important moderators of the effectiveness of these interventions. And finally, publication bias was a substantial concern, suggesting the need for follow-up research that will be published regardless of the outcome.

Overall, meta-analysis is a key technique for aggregating evidence across studies. Meta-analysis allows researchers to move beyond the bias of naive techniques like vote counting and towards a more quantitative summary of an experimental effect. Unfortunately, a meta-analysis is only as good as the literature it's based on, so the aspiring meta-analyst must be aware of both within- and between-study biases! 

::: exercise
Imagine that you read the following result in the abstract of a meta-analysis: "In 83 randomized studies of middle school children, replacing one hour of class time with mindfulness meditation significantly improved standardized test scores (standardized mean difference $\widehat{\mu} = 0.05$; 95% confidence interval: [$0.01, 0.09$]; $p<0.05$)." Why is this a problematic way to report on meta-analysis results? Suggest a better sentence to replace this one.

As you read the rest of the meta-analysis, you find that the authors conclude that "These findings demonstrate robust benefits of meditation for children, suggesting that test scores improve even when the meditation is introduced as a replacement for normal class time." You recall that the heterogeneity estimate was $\widehat{\tau} = 0.90$. Do you think that this result regarding the heterogeneity tends to support, or rather tends to undermine, the concluding sentence of the meta-analysis? Why?

What kinds of within-study biases would concern you in this meta-analysis? How might you assess the credibility of the meta-analyzed studies and of the meta-analysis as whole in light of these possible biases?
:::

::: exercise
Imagine you conduct a meta-analysis on a literature in which statistically significant results in either direction are much more likely to be published that non-significant results. Draw the funnel plot you would expect to see. Is the plot funnel symmetric or asymmetric?
:::

::: exercise
Why do you think small studies receive more weight in random-effects meta-analysis than in fixed-effects meta-analysis? Can you see why this is true mathematically based on the equations given above, and can you also explain the intuition in simple language?
:::


<!--chapter:end:019-meta.Rmd-->

```{r include=FALSE, cache=FALSE}
library(tidyverse)
library(glue)
library(here)
library(knitr)
library(shiny)
library(metafor)
library(ggrepel) # viz
library(GGally) # viz
library(BayesFactor) # inference
library(papaja)


opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  comment = "#>",
  collapse = TRUE,
  cache = TRUE,
  echo = FALSE,
  cache.lazy = FALSE,
  dev = "png",
  dpi = 300,
  out.width = "\\linewidth"
)
kable <- function(...) knitr::kable(..., booktabs = TRUE, linesep = "")

set.seed(42)

.font <- "Source Sans Pro"
if (!(.font %in% sysfonts::font_families()))
  sysfonts::font_add_google(.font, .font)
showtext::showtext_auto()

theme_set(theme_bw(base_size = 14, base_family = .font))
theme_update(panel.grid = ggplot2::element_blank(),
             strip.background = ggplot2::element_blank(),
             legend.key = ggplot2::element_blank(),
             panel.border = ggplot2::element_blank(),
             axis.line = ggplot2::element_line(),
             strip.text = ggplot2::element_text(face = "bold"))
.grey <- "grey70"
.refline <- "dotted"
.coef_line <- element_line(colour = .grey, size = 0.1)

.pal <- ggthemes::ptol_pal
.scale_colour_discrete <- ggthemes::scale_colour_ptol
.scale_color_discrete <- .scale_colour_discrete
.scale_fill_discrete <- ggthemes::scale_fill_ptol

.scale_colour_continuous <- viridis::scale_colour_viridis
.scale_color_continuous <- .scale_colour_continuous
.scale_fill_continuous <- viridis::scale_fill_viridis

.scale_colour_numerous <- scale_colour_discrete
.scale_color_numerous <- .scale_colour_numerous
.scale_fill_numerous <- scale_fill_discrete

# from https://bookdown.org/yihui/rmarkdown-cookbook/font-color.html
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
            x)
  } else x
}
```
# Conclusions {#conclusions}

::: {.learning-goals}
üçé Learning goals: Synthesize and extend lessons from previous chapters.
:::

::: {.case-study}
üî¨ Case study: How to run a study that doesn‚Äôt replicate, and how to design a study that doesn‚Äôt contribute to our knowledge. We report new empirical analysis of aggregate data from 10 years of Psych 251 (>100 replication projects), discussing empirical predictors of (non-)replicability
:::

Revisiting themes of precision, transparency, generalizability, bias-reduction

- Discuss the ‚Äúcrisis‚Äù narrative and reframe as one way into seeing the set of interlocking issues that keep behavioral research from contributing to cumulative theory.
- Collaboration as a way forward. Spotlight the Psychological Science Accelerator (Moshontz et al. 2018).

Concrete suggestions for building a cumulative research program

- This was a big book, and incorporating all of this advice into your research program may seem difficult or overwhelming.
- Start simple, and repeat: for building an empirical research program, you want to measure one thing well and then build on that measurement. 
- Internal replication as a critical way to check your own work. 
- Cumulativity as a key principle that you can model in your own work. 

<!--chapter:end:020-conclusions.Rmd-->


# (PART) Appendices {-}
# GitHub Tutorial {#git}

Placeholder


## Introduction {.unnumbered}
## 0. Review basic terminal commands {-}
## 1. Install git {.unnumbered}
### 1.1 Did you successfully install? {.unnumbered}
### 1.2 Other versions {.unnumbered}
### 1.3 Set your name and email address {.unnumbered}
## 2. Make a repo on GitHub, clone it to your computer {.unnumbered}
## 3. Make some commits {.unnumbered}
### 3.1 Update your README file {.unnumbered}
### 3.2 Add and commit changes to git {.unnumbered}
### 3.3 Add another file to the repo + commit it {.unnumbered}
## 4. Push your changes to GitHub {.unnumbered}
## 5. Make more changes to the repo {.unnumbered}
## 6. Rolling back to previous versions {.unnumbered}
## 7. What not to put on git {.unnumbered}
## Further Resources {.unnumbered}

<!--chapter:end:100-github.Rmd-->


# R Markdown Tutorial {#rmarkdown}

Placeholder


## Why write reproducible papers?
## Getting Started
## Structure of an R Markdown file
### Header
### Body text
### Code chunks
### Output formats
## Markdown syntax
## Headers, graphs, and tables
### Headers 
### Graphs
### Tables
### Statistics
## Writing APA-format papers
## Bibiographic management
## Collaboration 
## R Markdown: Chapter Summary

<!--chapter:end:101-rmarkdown.Rmd-->


# Tidyverse Tutorial {#tidyverse}

Placeholder


## Functions and Pipes
## Tidy data analysis
### Exploring and characterizing the dataset
### Filtering and mutating
### Standard descriptives using `summarise` and `group_by`
### More advanced combos
## Getting your data tidy
## Tidyverse: Chapter sumamry

<!--chapter:end:102-tidyverse.Rmd-->


# ggplot Tutorial {#ggplot}

Placeholder


## Exploring ggplot2 using `qplot`
## More complex with `ggplot`
## Facets
## Geoms
## Themes and plot cleanup
## Plot inheritance example
## Advanced plot exploration exercise
### Variable exploration
### Hypothesis-related exploration

<!--chapter:end:103-ggplot.Rmd-->


# Instructor's guide {#instructors}

Placeholder


## Introduction 
## Why Teach a Project-Based Course?
## Logistics
### Syllabus considerations
### Grading
### Course budget
### Course-related Institutional Review Board application
## Scenarios for different course layouts
### Student Level:
### Course Resources:
#### Teaching Assistants (or lack thereof)
#### Course Funding (or lack thereof)

<!--chapter:end:104-instructors.Rmd-->

