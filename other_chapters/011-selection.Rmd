# Experimental strategy {#strategy}

::: {.box .learning_goals}
- Consider what the best study is to do, and whether it should be a replication or an extension
- Evaluate the relevance and rigor of background literature
- Describe how to extract key information about the study design, measurement, and analysis
- Assess the likelihood of replication in a novel population
:::

::: {.box .case_study}
A classic social psychology paper revealed that holding a “pen-in-mouth” smiling manipulation made people feel happier (Strack, Martin, and Stepper 1988). Nearly two decades later, 17 teams attempted to replicate the study, but zero succeeded (Wagenmakers et al. 2016).
:::

Team up? 

When to replicate? @oberauer2019's analysis of discovery vs. theory testing. 

What studies are most informative to replicate?

- To date, systematic replication projects have generally focused on replicating studies sampled for scientific impact (e.g., using systematic sampling from certain high-impact journals). 
- But replications are expensive and time-consuming. Not every replication attempt will generate an equal amount of scientific value.

As a starting point, we might focus replication efforts on original studies that are not only scientifically impactful (the traditional criterion for selection into replication projects), but also that have a reasonably high prior probability of replicating (Hardwicke, Tessler, et al. 2018; Isager et al. 2020). Start with a solid literature

- Signs of a diverse and vibrant literature. 
- Statistical forensics. Selection models. Improbability analysis
- Reading a study to extract information about design, measurement, sampling, analysis.

::: {.box .accident_report}
(TITLE) video games and aggression (Hilgard 2021)
:::

Critically assessing whether your study will replicate. 

- The ‘logic’ of the overall argument (i.e. would this study/result address the theoretical question in principle or would even the strongest version of it be dead on arrival?)
- The ‘substance’ of the empirical step of the argument (i.e. do the details of the result support its role in the argument, or is there a flaw in the execution?)
- Construct, external, internal, and statistical validity? (e.g., shockingly small or large effects)

Connecting the literature to theories

- Cumulative effect sizes (Funder and Ozer 2019; VanderWeele, Mathur, and Chen 2019)
- Assessing the “expected information gain” from your study – how much will theory or practice shift based on different patterns of results. 
