
<!DOCTYPE html>
<html>
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 8 Measurement | Experimentology" />
<meta property="og:type" content="book" />




<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 8 Measurement | Experimentology">

<title>Chapter 8 Measurement | Experimentology</title>

<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<link rel="stylesheet" type="text/css" href="/assets/src/index.page.client.jsx.81210c8f.css"></head>
<body>



<div class="row">
<div class="col-sm-12">
<div id="island_0"><header class="_toc_1lnsy_1" id="toc"><a class="_book_title_1lnsy_24" href="/">Experimentology: An Open Science Approach to Experimental Psychology Methods</a><nav><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Preliminaries</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="1-experiments">Experiments</a><a class="_chapter_title_1lnsy_32" href="2-theories">Theories</a><a class="_chapter_title_1lnsy_32" href="3-replication">Replication</a><a class="_chapter_title_1lnsy_32" href="4-ethics">Ethics</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Statistics</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="5-estimation">Estimation</a><a class="_chapter_title_1lnsy_32" href="6-inference">Inference</a><a class="_chapter_title_1lnsy_32" href="7-models">Models</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Design</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="8-measurement">Measurement</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Reporting</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="9-writing">Writing</a><a class="_chapter_title_1lnsy_32" href="10-viz">Visualization</a><a class="_chapter_title_1lnsy_32" href="11-meta">Meta-analysis</a><a class="_chapter_title_1lnsy_32" href="12-conclusions">Conclusions</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Appendices</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="A-git">GitHub</a><a class="_chapter_title_1lnsy_32" href="B-rmarkdown">R Markdown</a><a class="_chapter_title_1lnsy_32" href="C-tidyverse">Tidyverse</a><a class="_chapter_title_1lnsy_32" href="D-ggplot">ggplot</a><a class="_chapter_title_1lnsy_32" href="E-instructors">Instructor’s guide</a></div></div></nav></header></div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="measurement" class="section level1" number="8">
<h1><span class="header-section-number">Chapter 8</span> Measurement</h1>
<div id="island_1"><div class="box learning_goals"><div class="Collapsible"><span id="collapsible-trigger-1664910881806" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1664910881806" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="apple-whole" class="svg-inline--fa fa-apple-whole " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M224 112c-8.8 0-16-7.2-16-16V80c0-44.2 35.8-80 80-80h16c8.8 0 16 7.2 16 16V32c0 44.2-35.8 80-80 80H224zM0 288c0-76.3 35.7-160 112-160c27.3 0 59.7 10.3 82.7 19.3c18.8 7.3 39.9 7.3 58.7 0c22.9-8.9 55.4-19.3 82.7-19.3c76.3 0 112 83.7 112 160c0 128-80 224-160 224c-16.5 0-38.1-6.6-51.5-11.3c-8.1-2.8-16.9-2.8-25 0c-13.4 4.7-35 11.3-51.5 11.3C80 512 0 416 0 288z"></path></svg>Learning goals<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M246.6 470.6c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 402.7 361.4 265.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3l-160 160zm160-352l-160 160c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 210.7 361.4 73.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3z"></path></svg></span><div id="collapsible-content-1664910881806" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1664910881806"><div class="Collapsible__contentInner">
<ul>
<li>Discuss the reliability and validity of psychological measures</li>
<li>Reason about tradeoffs between different measures and measure types</li>
<li>Identify the characteristics of well-constructed survey questions</li>
<li>Articulate risks of measurement flexibility and the costs and benefits of multiple measures</li>
</ul>
</div></div></div></div></div>

<p>Throughout the history of science, advances in measurement have gone hand in hand with advances in knowledge.<label for="tufte-sn-108" class="margin-toggle sidenote-number">108</label><input type="checkbox" id="tufte-sn-108" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">108</span> As such, measurement is a perennially controversial topic in philosophy of science. For an overview of competing frameworks, see <span class="citation">Tal (<a href="#ref-sep-measurement-science" role="doc-biblioref">2020</a>)</span> or <span class="citation">Maul et al. (<a href="#ref-maul2016philosophical" role="doc-biblioref">2016</a>)</span>, which focuses specifically on measurement in psychology.</span> Telescopes revolutionized astronomy, microscopes revolutionized biology, and patch clamping revolutionized physiology. But measurement isn’t easy. Even the humble thermometer, allowing reliable measurement of temperature, required centuries of painstaking effort to perfect <span class="citation">(<a href="#ref-chang2004inventing" role="doc-biblioref">Chang, 2004</a>)</span>. Psychology and the behavioral sciences are no different – we need reliable instruments to measure the things we care about. In this next section of the book, we’re going to discuss the challenges facing measurement in psychology, and the properties that distinguish good instruments from bad.</p>
<p>What does it mean to measure something? Intuitively, we know that a ruler measures the quantity of length, and a scale measures the quantity of weight <span class="citation">(<a href="#ref-kisch1965scales" role="doc-biblioref">Kisch, 1965</a>)</span>. But what does it mean to measure a psychological construct – a hypothesized theoretical quantity inside the head? According to <span class="citation">Stevens (<a href="#ref-stevens1946" role="doc-biblioref">1946</a>)</span>, measurement is simply the practice of assigning numbers to things. But, to paraphrase <span class="citation">Norman R. Campbell &amp; Jeffreys (<a href="#ref-campbell1938symposium" role="doc-biblioref">1938</a>)</span>, not every assignment of numbers is measurement! Roughly speaking, we want the numbers we assign to behave like the constructs we’re assigning them to (in the ways that matter). And not all measurement instruments are created equal.</p>
<p>This point is obvious when you think about physical measurement instruments: a caliper will give you a much more precise estimate of the thickness of a small object than a ruler. One way to see that the measurement is more precise is by repeating it a bunch of times. The measurements from the caliper will likely be more similar to one another, reflecting the fact that the amount of error in each individual measurement is smaller. We can do the same thing with a psychological measurement – repeat and assess variation – though as we’ll see below it’s a little trickier. Measurement instruments that have less error are called more <strong>reliable</strong> instruments.<label for="tufte-sn-109" class="margin-toggle sidenote-number">109</label><input type="checkbox" id="tufte-sn-109" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">109</span> Is <strong>reliability</strong> the same as <strong>precision</strong>? Yes, more or less. Confusingly, different fields call these concepts different things <span class="citation">(there’s a helpful table of these names in <a href="#ref-brandmaier2018" role="doc-biblioref">Brandmaier et al., 2018</a>)</span>. Here we’ll talk about reliability as a property of instruments specifically while using the term precision to talk about the measurements themselves.</span></p>
<p>When we have a physical quantity of interest, we can assess how well an instrument measures that quantity. But things are much trickier when the construct we are trying to measure can’t be assessed directly. We have to measure something observable – our operationalization of the construct – and then make an argument about how the measure relates to the construct of interest. This is an argument for the <strong>validity</strong> of measurements from the instrument.<label for="tufte-sn-110" class="margin-toggle sidenote-number">110</label><input type="checkbox" id="tufte-sn-110" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">110</span> We are also going to talk in Chapter <a href="#design"><strong>??</strong></a> about the validity of manipulations. The way you identify a causal effect on some measure is by operationalizing some construct as well. If this is done badly, the manipulation can be invalid – meaning the causal effect that’s measured doesn’t map onto the construct.</span></p>
<p>These two concepts, reliability and validity, provide a conceptual toolkit for assessing how good a psychological measurement instrument is.</p>
<div id="island_2"><div class="box case_study"><div class="Collapsible"><span id="collapsible-trigger-1664910881807" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1664910881807" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="microscope" class="svg-inline--fa fa-microscope " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M168 32c0-17.7 14.3-32 32-32h16c17.7 0 32 14.3 32 32h8c17.7 0 32 14.3 32 32V288c0 17.7-14.3 32-32 32h-8c0 17.7-14.3 32-32 32H200c-17.7 0-32-14.3-32-32h-8c-17.7 0-32-14.3-32-32V64c0-17.7 14.3-32 32-32l8 0zM32 448H320c70.7 0 128-57.3 128-128s-57.3-128-128-128V128c106 0 192 86 192 192c0 49.2-18.5 94-48.9 128H480c17.7 0 32 14.3 32 32s-14.3 32-32 32H320 32c-17.7 0-32-14.3-32-32s14.3-32 32-32zm80-64H304c8.8 0 16 7.2 16 16s-7.2 16-16 16H112c-8.8 0-16-7.2-16-16s7.2-16 16-16z"></path></svg>Case study<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M246.6 470.6c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 402.7 361.4 265.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3l-160 160zm160-352l-160 160c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 210.7 361.4 73.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3z"></path></svg></span><div id="collapsible-content-1664910881807" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1664910881807"><div class="Collapsible__contentInner"><p class="title">A reliable and valid measure of children’s vocabulary</p>

<p>Anyone who has worked with little children or had children of their own can attest to how variable their early language is. Some children speak clearly and produce long sentences from an early age, while others struggle; this variation appears to be linked to later school outcomes <span class="citation">(<a href="#ref-marchman2008" role="doc-biblioref">Marchman &amp; Fernald, 2008</a>)</span>. Thus, there are many reasons why you’d want to make precise measurements of children’s early language ability as a latent construct of interest.<label for="tufte-sn-111" class="margin-toggle sidenote-number">111</label><input type="checkbox" id="tufte-sn-111" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">111</span> Of course, you can also ask if early language is a single construct, or whether it is multi-dimensional! For example, does grammar develop separately from vocabulary? It turns out the two are very closely coupled <span class="citation">(<a href="#ref-frank2021" role="doc-biblioref">Frank et al., 2021</a>)</span>. This point illustrates the general idea that, especially in psychology, measurement and theory building are intimately related – you need data to inform your theory, but the measurement instruments you use to collect your data in turn presuppose some theory!</span></p>
<p>Because bringing children into a lab can be expensive, one popular option for measuring child language is the MacArthur Bates Communicative Development Inventory (CDI for short), a form which asks parents to mark words that their child says or understands. CDI forms are basically long checklists of words (the first page of an English form is shown in Figure <a href="8-measurement.html#fig:measurement-cdi">8.1</a>. But is parent report a reliable or valid measure of children’s early language?</p>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:measurement-cdi"></span>
<img src="images/measurement/cdi.jpg" alt="The first page of the MacArthur-Bates Communicative Development Inventory, a parent report instrument for measuring children's early language." width="\linewidth" />
Figure 8.1: The first page of the MacArthur-Bates Communicative Development Inventory, a parent report instrument for measuring children’s early language.
</span>
</p>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:measurement-psycho-cors"></span>
<img src="images/measurement/psycho-cors2.png" alt="Longitudinal correlations between a child's score on one administration of the CDI and another one several months later. From Frank et al. (2021). " width="\linewidth" />
Figure 8.2: Longitudinal correlations between a child’s score on one administration of the CDI and another one several months later. From Frank et al. (2021). 
</span>
</p>
<p>One test of the reliability of the CDI is a <strong>test-retest</strong> correlation, where we compute the correlation within children between two different administrations of the form. Unfortunately, this analysis has one issue: the longer you wait between observations the more the child has changed! Figure @ref(fig:measurement-psycho-cors longitudinal test-retest correlations for two CDIs, showing how correlations start off high and drop off as the gap between observations increases <span class="citation">(<a href="#ref-frank2021" role="doc-biblioref">Frank et al., 2021</a>)</span>.</p>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:measurement-cdi-validity"></span>
<img src="images/measurement/cdi-validity.png" alt="Relations between an early form of the CDI (the ELI) and several other measurements of children's early language from both transcripts and direct assessments. Arrows indicate loadings of different measures (boxes) onto a single construct (circle). Numbers give standardized weights [@bornstein1998]." width="\linewidth" />
Figure 8.3: Relations between an early form of the CDI (the ELI) and several other measurements of children’s early language from both transcripts and direct assessments. Arrows indicate loadings of different measures (boxes) onto a single construct (circle). Numbers give standardized weights <span class="citation">(<a href="#ref-bornstein1998" role="doc-biblioref">Bornstein &amp; Haynes, 1998</a>)</span>.
</span>
</p>
<p>Given that CDI forms are relatively reliable instruments, are they valid? That is, do they really measure the construct of interest, namely children’s early language ability? <span class="citation">Bornstein &amp; Haynes (<a href="#ref-bornstein1998" role="doc-biblioref">1998</a>)</span> collected many different measures of children’s language – including the ELI (an early CDI form) and other “gold standard” measures like transcribed samples of children’s speech. Figure <a href="8-measurement.html#fig:measurement-cdi-validity">8.3</a> shows the results of a structural equation model that measures the shared variance between these measures and a hypothesized central construct (“vocabulary competence”). The ELI (CDI) score correlated closely with the shared variance among all the different measures, suggesting that it was a valid measure of the construct.</p>
<p>The combination of reliability and validity evidence suggests that CDI are a useful (and relatively inexpensive source) of data about children’s early language, and indeed they have become one of the most common assessments for this age group!</p>

</div></div></div></div></div>
<div id="reliability" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Reliability</h2>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:measurement-brandmaier"></span>
<img src="images/measurement/reliability-validity.png" alt="Reliability and validity visualized. The reliability of an instrument is its expected precision. The bias of measurements from an instrument also provide a metaphor for its validity." width="\linewidth" />
Figure 8.4: Reliability and validity visualized. The reliability of an instrument is its expected precision. The bias of measurements from an instrument also provide a metaphor for its validity.
</span>
</p>
<p>Reliability is a way of describing the extent to which a measure yields signal relative to noise. Intuitively, if there’s less noise, then there will be more similarity between different measurements of the same quantity, illustrated in Figure <a href="8-measurement.html#fig:measurement-brandmaier">8.4</a> as a tighter grouping of points on the bulls-eye. But how do we measure signal and noise?</p>
<div id="island_3"><div class="box depth"><div class="Collapsible"><span id="collapsible-trigger-1664910881808" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1664910881808" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="landmark" class="svg-inline--fa fa-landmark " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M240.1 4.2c9.8-5.6 21.9-5.6 31.8 0l171.8 98.1L448 104l0 .9 47.9 27.4c12.6 7.2 18.8 22 15.1 36s-16.4 23.8-30.9 23.8H32c-14.5 0-27.2-9.8-30.9-23.8s2.5-28.8 15.1-36L64 104.9V104l4.4-1.6L240.1 4.2zM64 224h64V416h40V224h64V416h48V224h64V416h40V224h64V420.3c.6 .3 1.2 .7 1.8 1.1l48 32c11.7 7.8 17 22.4 12.9 35.9S494.1 512 480 512H32c-14.1 0-26.5-9.2-30.6-22.7s1.1-28.1 12.9-35.9l48-32c.6-.4 1.2-.7 1.8-1.1V224z"></path></svg>Depth<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M246.6 470.6c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 402.7 361.4 265.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3l-160 160zm160-352l-160 160c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 210.7 361.4 73.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3z"></path></svg></span><div id="collapsible-content-1664910881808" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1664910881808"><div class="Collapsible__contentInner"><p class="title">Early controversies over psychological measurement</p>

<blockquote>
<p>“Psychology cannot attain the certainty and exactness of the physical sciences, unless it rests on a foundation of […] measurement” <span class="citation">(<a href="#ref-cattel1890mental" role="doc-biblioref">Cattel, 1890</a>)</span>.</p>
</blockquote>
<p>It is no coincidence that the founders of experimental psychology were obsessed with measurement <span class="citation">(<a href="#ref-heidelberger2004nature" role="doc-biblioref">Heidelberger, 2004</a>)</span>.
It was viewed as the primary obstacle facing psychology on its road to becoming a legitimate quantitative science.
For example, one of the final pieces written by Hermann von Helmholtz (Wilhelm Wundt’s doctoral advisor), was a 1887 philosophical treatise entitled “Zahlen und Messen” (“Counting and Measuring”; see <span class="citation">Darrigol (<a href="#ref-darrigol2003number" role="doc-biblioref">2003</a>)</span>).
In the same year, <span class="citation">Fechner (<a href="#ref-fechner1987my" role="doc-biblioref">1987</a>)</span> explicitly grappled with the foundations of measurement in “Uber die psychischen Massprincipien” (“On Psychic Measurement Principles”).</p>
<p>Many of the early debates over measurement revolved around the emerging area of <em>psychophysics</em>, the problem of relating objective, physical stimuli (e.g. light or sound or pressure) to the subjective sensations they produce in the mind.
For example, <span class="citation">Fechner (<a href="#ref-fechner1860elemente" role="doc-biblioref">1860</a>)</span> was interested in a quantity called the “just noticeable difference” (JND), the smallest change in a stimulus that can be discriminated by our senses.
He argued for a lawful (logarithmic) relationship: a logarithmic change in the intensity of, say, brightness corresponded to a linear change in the reported intensity (up to some constant).
In other words, sensation was <em>measurable</em> via instruments like the JND.</p>
<p>It may be surprising to modern ears that the basic claim of measurability was controversial, even if the precise form of the psychophysical function would continue to be debated.
But this claim led to a deeply rancorous debate, culminating with the so-called Ferguson Committee, formed by the British Association for the Advancement of Science in 1932 to investigate whether such psychophysical procedures could count as quantitative ‘measurements’ of anything at all <span class="citation">(<a href="#ref-moscati2018measuring" role="doc-biblioref">Moscati, 2018</a>)</span>.
It was unable to reach a conclusion, with physicists and psychologists deadlocked:</p>
<blockquote>
<p>Having found that individual sensations have an order, they [some psychologists] assume that they are <em>measurable</em>. Having travestied physical measurement in order to justify that assumption, they assume that their sensation intensities will be related to stimuli by numerical laws […] which, if they mean anything, are certainly false. <span class="citation">(<a href="#ref-ferguson1940" role="doc-biblioref">Ferguson &amp; Tucker, 1940</a>)</span></p>
</blockquote>
<p>The heart of the disagreement was rooted in the classical definition of quantity requiring strictly <em>additive</em> structure.
An attribute was only considered measurable in light of a meaningful concatenation operation.
For example, weight was a measurable attribute because putting a bag of three rocks on a scale yields the same number as putting each of the three rock on separate scales and then summing up those numbers (in philosophy of science, attributes with this concatenation property are known as “extensive” attributes, as opposed to “intensive” ones.)
Norman Campbell, one of the most prominent members of the Ferguson Committee, had recently defined <em>fundamental</em> measurement in this way <span class="citation">(e.g. see <a href="#ref-campbell1928account" role="doc-biblioref">Norman Robert Campbell, 1928</a>)</span>, contrasting it with <em>derived measurement</em> which was some function of fundamental measures.
According to the physicists on the Ferguson Committee, measuring mental sensations was impossible because they could never be grounded in any <em>fundamental</em> scale with this kind of additive operation.
It just didn’t make sense to break up holistic sensations into parts the way we would weights or lengths: they didn’t come in “amounts” or “quantities” that could be combined <span class="citation">(<a href="#ref-cattell1962relational" role="doc-biblioref">Cattell, 1962</a>)</span>.
Even the intuitive additive logic of <span class="citation">Donders (<a href="#ref-donders1969speed" role="doc-biblioref">1868</a>)</span>’s “method of subtraction” for measuring the speed of mental processes was viewed skeptically on the same grounds by the time of the committee (e.g. in an early textbook, <span class="citation">Woodworth (<a href="#ref-woodworth1938" role="doc-biblioref">1938</a>)</span> claimed “we cannot break up the reaction into successive acts and obtain the time for each act.”)</p>
<p>The primary target of the Ferguson Committee’s investigation was the psychologist S. S. Stevens, who had claimed to measure the sensation of loudness using psychophysical instruments.
Exiled from classical frameworks of measurement, he went about developing an alternative “operational” framework <span class="citation">(<a href="#ref-stevens1946" role="doc-biblioref">Stevens, 1946</a>)</span>, where the classical ratio scale recognized by physicists was only one of several ways of assigning numbers to things (see <a href="8-measurement.html#tab:measurement-stevens-table">8.1</a> below).
Stevens’ framework quickly spread, leading to an explosion of proposed measures.
However, operationalism remains controversial outside psychology <span class="citation">(<a href="#ref-michell1999measurement" role="doc-biblioref">Michell, 1999</a>)</span>.
The most extreme version of his stance (“measurement is the assignment of numerals to objects or events according to rule”) permits researchers to <em>define</em> constructs operationally in terms of a measure <span class="citation">(<a href="#ref-hardcastle1995ss" role="doc-biblioref">Hardcastle, 1995</a>)</span>.
For example, one may say that the construct of intelligence is simply <em>whatever it is</em> that IQ measures.
It is then left up to the researcher to decide which scale type their proposed measure should belong to.</p>
<p>In Chapter <a href="2-theories.html#theories">2</a>, we outlined a somewhat different view, closer to a kind of constructive realism <span class="citation">Putnam (<a href="#ref-putnam1999threefold" role="doc-biblioref">2000</a>)</span>.
Psychological constructs like working memory or theory of mind are taken to exist independent of any given operationalization, putting us on firmer ground to debate the pros and cons associated with different ways of measuring the same construct.
In other words, we are not free to assign numerals however we like.
Whether a particular construct or quantity is measurable on a particular scale should be treated as an empirical question.</p>
<p>The next major breakthrough in measurement theory emerged with the birth of mathematical psychology in the 1960s, which aimed to put psychological measurement on more rigorous foundations.
This effort culminated in the three-volume Foundations of Measurement series <span class="citation">Robert Duncan Luce et al. (<a href="#ref-luce2007foundations" role="doc-biblioref">1990</a>)</span>, which has become the canonical text for every psychology student seeking to understand measurement in the non-physical sciences.<label for="tufte-sn-112" class="margin-toggle sidenote-number">112</label><input type="checkbox" id="tufte-sn-112" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">112</span> It is worth noting that 20th century physics has also seriously challenged the classical additive understanding of measurement. For example, velocities are revealed to be non-additive under general relativity, and properties of quantum particles are only measurable under a complex probabilistic framework.</span>
One of the key breakthroughs was to shift the burden from measuring (additive) constructs themselves to measuring (additive) <em>effects</em> of constructs in conjunction with one another:</p>
<blockquote>
<p>When no natural concatenation operation exists, one should try to discover a way to measure factors and responses such that the ‘effects’ of different factors are additive. <span class="citation">(<a href="#ref-luce1964simultaneous" role="doc-biblioref">R. Duncan Luce &amp; Tukey, 1964</a>)</span>.</p>
</blockquote>
<p>This modern viewpoint broadly informs the view we describe here.</p>
</div></div></div></div></div>
<div id="measurement-scales" class="section level3" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> Measurement scales</h3>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:measurement-cv"></span>
<img src="images/measurement/cv.png" alt="Computing the coefficient of variation (CV)." width="\linewidth" />
Figure 8.5: Computing the coefficient of variation (CV).
</span>
</p>
<p>In the physical sciences, it’s common to measure the precision of an instrument by quantifying its coefficient of variation <span class="citation">(<a href="#ref-brandmaier2018" role="doc-biblioref">Brandmaier et al., 2018</a>)</span>:</p>
<p><span class="math display">\[CV = \frac{\sigma_w}{\mu_w}\]</span>
where <span class="math inline">\(\sigma_w\)</span> is the standard deviation of the measurements within an individual and <span class="math inline">\(\mu_w\)</span> is the mean of those measurements (Figure <a href="8-measurement.html#fig:measurement-cv">8.5</a>).</p>
<p>Imagine we measure the height of a person five times, resulting in measurements of 171cm, 172cm, 171cm, 173cm, and 172cm. These are the combination of the person’s true height (we assume they have one!) and some <strong>measurement error</strong>. Now we can use these measurements to compute the coefficient of variation, which is 0.005. Why can’t we just do that with psychological measurements?</p>
<p><span class="marginnote shownote"><span id="tab:measurement-stevens-table">Table 8.1: </span>Stevens (1946) table of scale types and their associated operations and statistics.</span></p>
<table><thead><tr><th style="text-align: left;">
Scale
</th><th style="text-align: left;">
Definition
</th><th style="text-align: left;">
Operations
</th><th style="text-align: left;">
Statistics
</th></tr></thead><tbody><tr><td style="text-align: left;">
Nominal
</td><td style="text-align: left;">
Unordered list
</td><td style="text-align: left;">
Equality
</td><td style="text-align: left;">
Mode
</td></tr><tr><td style="text-align: left;">
Ordinal
</td><td style="text-align: left;">
Ordered list
</td><td style="text-align: left;">
Greater than or less than
</td><td style="text-align: left;">
Median
</td></tr><tr><td style="text-align: left;">
Interval
</td><td style="text-align: left;">
Numerical
</td><td style="text-align: left;">
Equality of intervals
</td><td style="text-align: left;">
Mean, SD
</td></tr><tr><td style="text-align: left;">
Ratio
</td><td style="text-align: left;">
Numerical with zero
</td><td style="text-align: left;">
Equality of ratios
</td><td style="text-align: left;">
Coefficient of variation
</td></tr></tbody></table>
<p>Thinking about this question takes us on a detour through the different kinds of measurement scales used in psychological research <span class="citation">(<a href="#ref-stevens1946" role="doc-biblioref">Stevens, 1946</a>)</span>. The height measurements in our example are on what is known as a <strong>ratio</strong> scale: a scale in which numerical measurements are equally spaced and on which there is a true zero point. These scales are common for physical quantities but actually quite infrequent in psychology. More common are <strong>interval</strong> scales, in which there is no true zero point. For example, IQ (and other standardized scores) are intended to capture interval variation on some dimension but 0 is meaningless – an IQ of 0 does not correspond to any particular interpretation.<label for="tufte-sn-113" class="margin-toggle sidenote-number">113</label><input type="checkbox" id="tufte-sn-113" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">113</span> It can actually be shown in a suitably rigorous sense that ratio and interval scales (and another lying in between) are the <em>only</em> scales possible for the real numbers <span class="citation">(<a href="#ref-narens1986measurement" role="doc-biblioref">Narens &amp; Luce, 1986</a>)</span></span></p>
<p><strong>Ordinal</strong> scales are also commonly used. These are scales that are ordered but are not necessarily spaced equally. For example, levels of educational achievement (“Elementary”,“High school”,“Some college”,“College”,“Graduate school”) are ordered, but there is no sense in which “High school” is as far from “Elementary” as “Graduate school” is from “College.” The last type in Stevens’ hierarchy is <strong>nominal</strong> scales, in which no ordering is possible either. For example, race is an unordered scale in which multiple categories are present but there is no inherent ordering of these categories. The full hierarchy is presented in Table <a href="8-measurement.html#tab:measurement-stevens-table">8.1</a>.</p>
<p>Critically, different summary measures work for each scale type. If you have an unordered list like a list of options for a question about race on a survey, you can present the modal response (the most likely one). It doesn’t even make sense to think about what the median was – there’s no ordering! For ordered levels of education, a median is possible but you can’t compute a mean. And for interval variables like “number of correct answers on a math test” you can compute a mean and a standard deviation.<label for="tufte-sn-114" class="margin-toggle sidenote-number">114</label><input type="checkbox" id="tufte-sn-114" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">114</span> You might be tempted to think that “number of correct answers” is a ratio variable – but is zero really meaningful? Does it truly correspond to “no math knowledge” or is it just a stand-in for “less math knowledge than this test requires”?</span></p>
<p>Now we’re ready to answer our initial question about why we can’t quantify reliability using the coefficient of variation. Unless you have a ratio scale with a true zero, you can’t compute a coefficient of variation. Think about it for IQ scores: currently, by convention, standardized IQ scores are set to have a mean of 100. If we tested someone multiple times and found the standard deviation of their test scores was 4 points, then we could estimate the precision of their measurements as “CV” of 4/100 = .04. But since IQ of 0 isn’t meaningful, we could just set the mean IQ for the population to 200. Our test would be the same, and so the CV would be 4/200 = .02. On that logic we just doubled the precision of our measurements by rescaling the test! That doesn’t make any sense.</p>

</div>
<div id="measuring-reliability" class="section level3" number="8.1.2">
<h3><span class="header-section-number">8.1.2</span> Measuring reliability</h3>
<p>So then how do we measure signal and noise when we don’t have a true zero? We can still look at the variation between repeated measurement, but rather than comparing that variation between measurements to the mean, we can compare it to some other kind of variation, for example, variation between people. In what follows, we’ll discuss reliability on interval scales, but many of the same tools have been developed for ordinal and nominal scales.
</p>
<p>Imagine that you are developing an instrument to measure some cognitive ability. We assume that every participant has a true ability, <span class="math inline">\(t\)</span>, just the same way that they have a true height in the example above. Every time we measure this true ability with our instrument, however, it gets messed up by some measurement error. Let’s specify that error is normally distributed with a mean of zero – so it doesn’t <strong>bias</strong> the measurements, it just adds noise. The result is our observed score, <span class="math inline">\(o\)</span>.<label for="tufte-sn-115" class="margin-toggle sidenote-number">115</label><input type="checkbox" id="tufte-sn-115" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">115</span> The approach we use to introduce this set of ideas is called <strong>classical test theory</strong>. There are other – more modern – alternative approaches, but CTT (as it’s called) is a good starting point for thinking through the concepts.</span></p>
<p>Taking this approach, we could define a relative version of the coefficient of variation. The idea is that the reliability of a measurement is the amount of variance attributable to the true score variance (signal), rather than the observed score variance (which includes noise). If <span class="math inline">\(\sigma^2_t\)</span> is the variance of the true scores and <span class="math inline">\(\sigma^2_o\)</span> is the variance of the observed scores, then this ratio is</p>
<p><span class="math display">\[
R = \frac{\sigma^2_t}{\sigma^2_o}.
\]</span>
When noise is high, then the denominator is going to be big and <span class="math inline">\(R\)</span> will go down to 0; when noise is low, the numerator and the denominator will be almost the same and <span class="math inline">\(R\)</span> will approach 1.</p>
<p>This all sounds great, except for one problem: we can’t compute reliability using this formula without knowing true ability scores and their variance. But if we knew those, we wouldn’t need to measure anything at all! To get around this fundamental issue, there are two main approaches to computing reliability from data.</p>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:measurement-trt"></span>
<img src="images/measurement/trt.png" alt="Computing test-retest reliability." width="\linewidth" />
Figure 8.6: Computing test-retest reliability.
</span>
</p>
<p><strong>Test-retest reliability</strong>. Imagine you have two parallel versions of your instrument that are the same difficulty and hence reflect the same true score for each participant you assess. In that case, you can use these two measurement to compute the reliability of the instrument by simply computing the correlation between the two scores. The logic is that, if both variants reflect the same true score, then the shared variance (<strong>covariance</strong>) between them is just <span class="math inline">\(\sigma^2_t\)</span>, the true score variance, which is the variable that we wanted but didn’t have. Test-retest reliability is thus a very convenient way to measure reliability (Figure <a href="8-measurement.html#fig:measurement-trt">8.6</a>).</p>
<p><strong>Internal reliability</strong>. If you don’t have two parallel versions of an instrument, or you can’t give the test twice for whatever reason, then you have another option. Assuming your instrument has multiple items – e.g., multiple survey questions or multiple math problems – then you can split the test in pieces and treat the scores from each of these sub-parts as parallel versions of the instrument. The simplest way to do this is to split the instrument in half and compute the correlation between participants’ scores on the two halves – this quantity is called <strong>split half reliability</strong>.<label for="tufte-sn-116" class="margin-toggle sidenote-number">116</label><input type="checkbox" id="tufte-sn-116" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">116</span> The problem is that each half is… half as long as the original instrument. To get around this, there is a correction called the Spearman-Brown correction that can be applied to estimate the expected correlation for the full-length instrument.</span></p>
<p>Another method for computing the internal reliability (the <strong>consistency</strong> of a test) is to treat each item as a sub-instrument and compute the average split-half correlation over all splits. This method yields the statistic <strong>Cronbach’s alpha</strong>. Alpha is a widely reported statistic, but it is also widely misinterpreted <span class="citation">(<a href="#ref-sijtsma2009" role="doc-biblioref">Sijtsma, 2009</a>)</span>. First, it is actually a lower bound on reliability rather than a good estimate of reliability itself. And second, it is often misinterpreted as evidence that an instrument yields scores that are “internally consistent,” which it does not; it’s not an accurate summary of dimensionality. Alpha is a standard statistic, but it should be used with caution.</p>





<div id="island_4"><div class="box depth"><div class="Collapsible"><span id="collapsible-trigger-1664910881809" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1664910881809" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="landmark" class="svg-inline--fa fa-landmark " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M240.1 4.2c9.8-5.6 21.9-5.6 31.8 0l171.8 98.1L448 104l0 .9 47.9 27.4c12.6 7.2 18.8 22 15.1 36s-16.4 23.8-30.9 23.8H32c-14.5 0-27.2-9.8-30.9-23.8s2.5-28.8 15.1-36L64 104.9V104l4.4-1.6L240.1 4.2zM64 224h64V416h40V224h64V416h48V224h64V416h40V224h64V420.3c.6 .3 1.2 .7 1.8 1.1l48 32c11.7 7.8 17 22.4 12.9 35.9S494.1 512 480 512H32c-14.1 0-26.5-9.2-30.6-22.7s1.1-28.1 12.9-35.9l48-32c.6-.4 1.2-.7 1.8-1.1V224z"></path></svg>Depth<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M246.6 470.6c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 402.7 361.4 265.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3l-160 160zm160-352l-160 160c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 210.7 361.4 73.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3z"></path></svg></span><div id="collapsible-content-1664910881809" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1664910881809"><div class="Collapsible__contentInner"><p class="title">Reliability paradoxes!</p>

<p>There’s a major issue with calculating reliabilities using the approaches we described here: reliability will always be relative to the variation in the sample. So if a sample has less variability, reliability will decrease!</p>
<p>Let’s think about the CDI data we were talking about earlier, which showed high test-retest reliability. Now imagine we restricted our sample to only 16 – 18 month-olds (our prior sample had 16 – 30-month-olds) with low maternal education. Within this more restricted subset, overall vocabularies would be lower and more similar to one another, and so the average amount of change <em>within</em> a child would be larger relative to the differences <em>between</em> children. That would make our test-retest reliability score go down, even though we would just be computing it on a subset of the same data.</p>
<p>We can construct a much more worrisome version of the same problem. Say we are very sloppy in our administration of the CDI and create lots of between-participants variability, perhaps by giving different instructions to different families. This practice will actually <em>increase</em> our estimate of split-half reliability – while the within-participant variability will remain the same, the between-participant variability will go up! You could call this a “reliability paradox” – sloppier data collection can actually lead to higher reliabilities.<label for="tufte-sn-117" class="margin-toggle sidenote-number">117</label><input type="checkbox" id="tufte-sn-117" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">117</span> If you get interested in this topic, take a look at <span class="citation">(<a href="#ref-luck2018" role="doc-biblioref"><strong>luck2018?</strong></a>)</span>. There’s also a fascinating article by <span class="citation">Hedge et al. (<a href="#ref-hedge2018" role="doc-biblioref">2018</a>)</span> that shows why many highly replicable cognitive tasks like the Stroop task nevertheless have low reliability: they don’t vary very much between individuals!</span></p>
<p>More generally, we need to be sensitive to the sources of variability we’re quantifying reliability over – both the numerator and the denominator. If we’re computing split-half reliabilities, typically we’re looking at variability across test questions (from some question bank) vs. across individuals (from some population). Both of these sampling decisions affect reliability – if the population is more variable <em>or</em> the questions are less variable, we’ll get higher reliability.</p>
</div></div></div></div></div>
</div>
<div id="practical-advice-for-computing-reliability" class="section level3" number="8.1.3">
<h3><span class="header-section-number">8.1.3</span> Practical advice for computing reliability</h3>
<p>Ignorance is not bliss. If you don’t know the reliability of your measures for an experiment, you risk wasting your and your participants’ time. A higher reliability measure will lead to more precise measurements of a causal effect of interest and hence smaller sample sizes.<label for="tufte-sn-118" class="margin-toggle sidenote-number">118</label><input type="checkbox" id="tufte-sn-118" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">118</span> Low-reliability measures also limit your ability to detect correlations between measurements. One of us spent several fruitless months in graduate school running dozens of participants through batteries of language processing tasks and correlating the results across tasks. This exercise was a waste of time because most of the tasks were of such low reliability that, even had they been highly correlated with another task, this relationship would have been almost impossible to detect without a huge sample size. One rule of thumb that’s helpful for individual difference designs of this sort is that the maximal correlation that can be observed between two variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is the square root of the product of their reliabilities: <span class="math inline">\(\sqrt{r_x r_y}\)</span>. So if you have two measures that are reliable at .25, the maximal measured correlation between them is .25 as well! This kind of method is now frequently used in cognitive neuroscience (and other fields as well) to compute the so-called <strong>noise ceiling</strong> for a measure: the maximum amount of signal that in principle <em>could</em> be predicted <span class="citation">(<a href="#ref-lage-castellanos2019" role="doc-biblioref">Lage-Castellanos et al., 2019</a>)</span>.</span></p>
<p>Test-retest reliability is generally the most conservative practical measure of reliability. Test-retest reliability estimates include not only measurement error but also participants’ state variation across different testing sessions and variance due to differences between versions of your instrument. These real-world quantities are absent from internal reliability estimates, which may make you erroneously think that there is more signal present in your instrument than there is.<label for="tufte-sn-119" class="margin-toggle sidenote-number">119</label><input type="checkbox" id="tufte-sn-119" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">119</span> Even though alpha is a theoretical lower bound on reliability, in [practice test-retest accuracy often ends up lower than alpha because it incorporates all these other sources of variation.] It’s hard work to measure test-retest reliability estimates, but if you plan on using an instrument more than once or twice, it will likely be worthwhile!</span></p>
</div>
<div id="file-names" class="section level3" number="8.1.4">
<h3><span class="header-section-number">8.1.4</span> File names</h3>
<p>As <a href="https://www.karlton.org/2017/12/naming-things-hard/">Phil Karlton reportedly said</a>, “There are only two hard things in Computer Science: cache invalidation and naming things.” What’s true for computer science is true for research in general.<label for="tufte-sn-120" class="margin-toggle sidenote-number">120</label><input type="checkbox" id="tufte-sn-120" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">120</span> We won’t talk about cache invalidation; that’s a more technical problem in computer science that is beyond the scope of this book.</span> Naming files is hard! Some very organized people survive on systems like <code>info-r1-draft-2020-07-13-js.docx</code> - meaning, “the info project revision 1 draft of July 13th, 2020, with edits by JS.” But this kind of system needs a lot of rules and discipline, and it requires everyone in a project to buy in completely.</p>
<p>On the other hand, if you are naming a file in a hierarchically organized version control repository, the naming problem gets dramatically easier. All of a sudden, you have a context in which names make sense. <code>data.csv</code> is a terrible name for a data file on its own. But the name is actually perfectly informative – in the context of a project repository with a README that states that there is only a single experiment, a repository structure such that the file lives in a folder called <code>raw_data</code>, and a commit history that indicates the file’s commit date and author.</p>
<p>As this example shows, naming is hard <em>out of context</em>. So here’s our rule: name a file with what it contains. Don’t use the name to convey the context of who edited it, when, or where it should go in a project.</p>
</div>
</div>
<div id="data-management" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Data Management</h2>
<p>We’ve just discussed how to manage projects in general; in this section we zoom in on datasets specifically. Data are often the most valuable research product because they represent the evidence generated by our research. We maximize the value of the evidence when other scientists can reuse it for independent verification or generation of novel discoveries. Yet lots of research data are not reusable, even when they are shared. In Chapter <a href="3-replication.html#replication">3</a>, we discussed <span class="citation">Hardwicke et al. (<a href="#ref-hardwicke2018b" role="doc-biblioref">2018</a>)</span>’s study of analytic reproducibility. But before we were able to even try and reproduce the analytic results we found that only 64% of shared datasets were both complete and understandable.</p>
<p>How can you make sure that your data are managed so as to enable effective sharing? We make four primary recommendations. First, save your raw data! Second, document your data collection process. Third, organize your raw data for later analysis – we provide guidance on organization for both spreadsheets and for data retrieved from software platforms, like Qualtrics. Fourth and finally, document your data using a codebook or other appropriate metadata.</p>
<div id="save-your-raw-data" class="section level3" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Save your raw data</h3>
<p>Raw data take many forms. For many of us, the raw data are those returned by the experimental software; for others, the raw data are videos of the experiment being carried out. Regardless of the form of these data, save them! They are often the only way to check issues in whatever processing pipeline brings these data from their initial state to the form you analyze. They also can be invaluable for addressing critiques or questions about your methods or results later in the process. If you need to correct something about your raw data, <em>do not alter the original files</em>. Make a copy, and make a note about how the copy differs from the original.<label for="tufte-sn-121" class="margin-toggle sidenote-number">121</label><input type="checkbox" id="tufte-sn-121" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">121</span> Future you will thank present you for explaining why there are two copies of subject 19’s data.</span></p>
<p>Raw data are often not anonymized or anonymizable. Anonymizing them sometimes means altering them (e.g., in the case of downloaded logs from a service that might include IDs or IP addresses). Or in some cases, anonymization is difficult or impossible without significant effort and loss of some value from the data, e.g. for video data or MRI data <span class="citation">(<a href="#ref-bischoff-grethe2007" role="doc-biblioref">Bischoff-Grethe et al., 2007</a>)</span>. Unless you have specific permission for broad distribution of these identifiable data, the raw data may then need to be stored in a different way. In these cases, we recommend saving your raw data in a separate repository with the appropriate permissions. For example, in the ManyBabies 1 study we described above, the public repository does not contain the raw data contributed by participating labs, which the team could not guarantee was anonymized; these data are instead stored in a private repository.<label for="tufte-sn-122" class="margin-toggle sidenote-number">122</label><input type="checkbox" id="tufte-sn-122" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">122</span> The precise repository you use for this task is likely to vary by the kind of data that you’re trying to store and the local regulatory environment. For example, in the United States, to store de-anonymized data with certain fields requires a server that is certified for HIPAA (the relevant medical privacy law). Many – but by no means all – universities provide HIPAA-compliant cloud storage.</span></p>
<p>You can use your repository’s README to describe what is and is not shared. For example, a README might state that “We provide anonymized versions of the files originally downloaded from Qualtrics” or “Participants did not provide permission for public distribution of raw video recordings, which are retained on a secure university server.” Critically, if you still share the derived tabular data, it should still be possible to reproduce the analytic results in your paper, even if checking the provenance of those numbers from the raw data is not possible for every reader.</p>

<div class="figure"><span style="display: block;" id="fig:management-mb-datafiles"></span>
<p class="caption marginnote shownote">
Figure 8.7: Example participant (top) and trial (bottom) level data from the ManyBabies (2020) case study.
</p>
<img src="images/management/mb-combined.png" alt="Example participant (top) and trial (bottom) level data from the ManyBabies (2020) case study." width="\linewidth" />
</div>
<p>One common practice is the use of participant identifiers to link specific experimental data – which, if they are responses on standardized measures, rarely pose a significant identifiability risk – to demographic data sheets that might include more sensitive and potentially identifiable data.<label for="tufte-sn-123" class="margin-toggle sidenote-number">123</label><input type="checkbox" id="tufte-sn-123" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">123</span> A word about subject identifiers. These should be anonymous identifiers, like randomly generated numbers, that cannot be linked to participant identities (like data of birth) and are unique. You laugh, but one of us was in a lab where all the subject IDs were the date of test and the initials of the participant. These were neither unique nor anonymous. One common convention is to give your study a code-name and to number participants sequentially, so your first participant in a sequence of experiments on information processing might be <code>INFO-1-01</code>.</span> Depending on the nature of the analyses being reported, the experimental data can then be shared with limited risk. Then a selected set of demographic variables – for example, those that do not increase privacy risks but are necessary for particular analyses – can be distributed as a separate file and joined back into the data later.</p>
</div>
<div id="document-your-data-collection-process" class="section level3" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Document your data collection process</h3>
<p>In order to understand the meaning of the raw data, its helpful to share as much as possible about the context in which it was collected. This also helps communicate the experience that participants had in your experiment. Documentation of this experience can take many forms.</p>
<p>If the experimental experience was a web-based questionnaire, archiving this experience can be as simple as downloading the questionnaire source.<label for="tufte-sn-124" class="margin-toggle sidenote-number">124</label><input type="checkbox" id="tufte-sn-124" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">124</span> If it’s in a proprietary format like a Qualtrics <code>.QSF</code> file, a good practice is to convert it to a simple plain text format as well so it can be opened and re-used by folks who do not have access to Qualtrics (which may include future you!)</span> On the other hand, for many more involved studies it can be more difficult to reconstruct what participants went through. This kind of situation is where video data can shine <span class="citation">(<a href="#ref-gilmore2017" role="doc-biblioref">Gilmore &amp; Adolph, 2017</a>)</span>. A video recording of a typical experimental session can provide a valuable tutorial for other experimenters – as well as good context for readers of your paper. This is doubly true if there is a substantial interactive element to your experimental experience, as is often the case for experiments with children. For example, the ManyBabies case study that we examined shared <a href="https://nyu.databrary.org/volume/896">“walk through” videos of experimental sessions</a> for many of the participating labs, creating a repository of standard experiences for infant development studies. If nothing else, a video of an experimental session can sometimes be a very nice archive of a particular context.<label for="tufte-sn-125" class="margin-toggle sidenote-number">125</label><input type="checkbox" id="tufte-sn-125" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">125</span> Videos of experimental sessions also are great to show in a talk, provided you have permission from the participant.</span></p>
<p>Regardless of what other documentation you keep, it’s critical to create some record linking your data to the particular documentation you have. For a questionnaire study, for example, this documentation might be as simple as a README that says that the data in the <code>raw_data</code> directory were collected on a particular date using the file named <code>experiment1.qsf</code>. This kind of “connective tissue” linking data to materials can be very important when you return to a project with questions. If you spot a potential error in your data, you will want to be able to examine the precise version of the materials that you used to gather those data in order to identify the source of the problem.</p>
</div>
<div id="organize-your-data-for-later-analysis-spreadsheet-version" class="section level3" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> Organize your data for later analysis (spreadsheet version)</h3>
<p>Data come in many forms, but chances are that at some point during your project you will end up with a spreadsheet full of information. Well-organized spreadsheets cam mean the difference between project success and failure! A wonderful article by <span class="citation">Broman &amp; Woo (<a href="#ref-broman2018" role="doc-biblioref">2018</a>)</span> gives a guide to spreadsheet organization that lays out the principles of good spreadsheet design. We highlight some of their principles here (with our own, opinionated ordering):</p>
<div class="figure"><span style="display: block;" id="fig:management-broman-nonrect"></span>
<p class="caption marginnote shownote">
Figure 8.8: Examples of non-rectangular spreadsheet formats that are likely to cause problems in analysis. From Broman and Woo (2018).
</p>
<img src="images/management/broman2018.png" alt="Examples of non-rectangular spreadsheet formats that are likely to cause problems in analysis. From Broman and Woo (2018)." width="\linewidth" />
</div>
<ol style="list-style-type: decimal;">
<li><p><em>Make it a rectangle</em><label for="tufte-sn-126" class="margin-toggle sidenote-number">126</label><input type="checkbox" id="tufte-sn-126" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">126</span> Think of your data like a well-ordered plate of sushi, neatly packed together without any gaps.</span>. Nearly all data analysis software, like SPSS, Stata, Jamovi and JASP (and many R packages), require data to be in a tabular format.<label for="tufte-sn-127" class="margin-toggle sidenote-number">127</label><input type="checkbox" id="tufte-sn-127" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">127</span> Tabular data is a precursor to “tidy” data, which we describe in more detail in Appendix <a href="C-tidyverse.html#tidyverse">C</a>.</span> If you are used to analyzing data exclusively in a spreadsheet, this kind of tabular data isn’t quite as readable, but readable formatting gets in the way of almost any analysis you want to do. Figure <a href="8-measurement.html#fig:management-broman-nonrect">8.8</a> gives some examples of non-rectangular spreadsheets. All of these will cause any analytic package to choke because of inconsistencies in how rows and columns are used!</p></li>
<li><p><em>Choose good names for your variables</em>. No one convention for name formatting is best, but it’s important to be consistent. We tend to follow the <a href="https://style.tidyverse.org">tidyverse style guide</a> and use lowercase words separated by underscores (<code>_</code>). It’s also helpful to give units where these are available, e.g., are reaction times in seconds or milliseconds. Table <a href="8-measurement.html#tab:management-broman-ex">8.2</a> gives some examples of good and bad variable names.</p></li>
</ol>
<p><span class="marginnote shownote"><span id="tab:management-broman-ex">Table 8.2: </span>Examples of good and bad variable names. Adapted from Broman and Woo (2018).</span></p>
<table><thead><tr><th style="text-align: left;">
Good name
</th><th style="text-align: left;">
Good alternative
</th><th style="text-align: left;">
Avoid
</th></tr></thead><tbody><tr><td style="text-align: left;">
subject_id
</td><td style="text-align: left;">
SubID
</td><td style="text-align: left;">
subject #
</td></tr><tr><td style="text-align: left;">
sex
</td><td style="text-align: left;">
female
</td><td style="text-align: left;">
M/F
</td></tr><tr><td style="text-align: left;">
rt_msec
</td><td style="text-align: left;">
reaction_time_ms
</td><td style="text-align: left;">
reaction time (millisec.)
</td></tr></tbody></table>
<ol start="3" style="list-style-type: decimal;">
<li><em>Be consistent with your cell formatting</em>. Each column should have one <em>kind</em> of thing in it. For example, if you have a column of numerical values, don’t all of a sudden introduce text data like “missing” into one of the cells. This kind of mixing of data types can cause havoc down the road. Mixed or multiple entries also don’t work, so don’t write “0 (missing)” as the value of a cell. Leaving cells blank is also risky because its ambiguous.</li>
</ol>
<p>Most software packages have a standard value for missing data (e.g. <code>NA</code> is what R uses). If you are writing dates, please be sure to use the “global standard” (ISO 8601), which is YYYY-MM-DD. Anything else can be misinterpreted easily. Dates in Excel deserve special mention as a source of terribleness. Excel has an unfortunate habit of interpreting information that has nothing to do with dates as dates, destroying the original content in the process.<label for="tufte-sn-128" class="margin-toggle sidenote-number">128</label><input type="checkbox" id="tufte-sn-128" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">128</span> Excel’s issue with dates has caused unending horror in the genetics literature, where gene names are automatically converted to dates, sometimes without the researchers noticing <span class="citation">(<a href="#ref-ziemann2016" role="doc-biblioref">Ziemann et al., 2016</a>)</span>. In fact, some gene names have had to be changed in order to avoid this issue!</span></p>
<ol start="4" style="list-style-type: decimal;">
<li><p><em>Decoration isn’t data</em>. Decorating your data with bold headings or highlighting may seem useful for humans, but it isn’t uniformly interpreted or even recognized by analysis software (e.g., reading an Excel spreadsheet into R will scrub all your beautiful highlighting and artistic fonts) so do not rely on it.</p></li>
<li><p><em>Save data in plain text files</em>. The CSV (comma-delimited) file format is a common standard for data that is uniformly understood by most analysis software (it is an “interoperable” file format).<label for="tufte-sn-129" class="margin-toggle sidenote-number">129</label><input type="checkbox" id="tufte-sn-129" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">129</span> Be aware of some interesting differences in how these files are output by European vs. American versions of Microsoft Excel! You might find semi-colons instead of commas in some datasets.</span> The advantage of CSVs is that they are not proprietary to Microsoft or another tech company, can be inspected in a text editor, but be careful: they do not preserve Excel formulas or formatting!</p></li>
</ol>
<p>Given the points above, we recommend that you avoid analyzing your data in Excel. If it is necessary to analyze your data in a spreadsheet program, we urge you to save the raw data as a separate CSV and then create distinct analysis spreadsheets so as to be sure to retain the raw data unaltered by your (or Excel’s) manipulations.</p>
</div>
<div id="organize-your-data-for-later-analysis-software-version" class="section level3" number="8.2.4">
<h3><span class="header-section-number">8.2.4</span> Organize your data for later analysis (software version)</h3>
<p>Many researchers do not create data by manually entering information into a spreadsheet. Instead they receive data as the output from a web platform, software package, or device. These tools typically provide researchers limited control over the format of the resulting tabular data export. Case in point is the survey platform Qualtrics, which provides data with not one but two header rows, complicating import into almost all analysis software!<label for="tufte-sn-130" class="margin-toggle sidenote-number">130</label><input type="checkbox" id="tufte-sn-130" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">130</span> The R package <code>qualtRics</code> can help with this.</span></p>
<p>That said, if your platform <em>does</em> allow you to control what comes out, you can try to use the principles of good tabular data design outlined above. For example, try to give your variables (e.g., questions in Qualtrics) sensible names!</p>
<div id="island_5"><div class="box accident_report"><div class="Collapsible"><span id="collapsible-trigger-1664910881810" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1664910881810" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="person-falling-burst" class="svg-inline--fa fa-person-falling-burst " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><path fill="currentColor" d="M256 32c0-17.7-14.3-32-32-32s-32 14.3-32 32l0 9.8c0 39-23.7 74-59.9 88.4C71.6 154.5 32 213 32 278.2V352c0 17.7 14.3 32 32 32s32-14.3 32-32l0-73.8c0-10 1.6-19.8 4.5-29L261.1 497.4c9.6 14.8 29.4 19.1 44.3 9.5s19.1-29.4 9.5-44.3L222.6 320H224l80 0 38.4 51.2c10.6 14.1 30.7 17 44.8 6.4s17-30.7 6.4-44.8l-43.2-57.6C341.3 263.1 327.1 256 312 256l-71.5 0-56.8-80.2-.2-.3c44.7-29 72.5-79 72.5-133.6l0-9.8zM96 80c0-26.5-21.5-48-48-48S0 53.5 0 80s21.5 48 48 48s48-21.5 48-48zM464 286.1l58.6 53.9c4.8 4.4 11.9 5.5 17.8 2.6s9.5-9 9-15.5l-5.6-79.4 78.7-12.2c6.5-1 11.7-5.9 13.1-12.2s-1.1-13-6.5-16.7l-65.6-45.1L603 92.2c3.3-5.7 2.7-12.8-1.4-17.9s-10.9-7.2-17.2-5.3L508.3 92.1l-29.4-74C476.4 12 470.6 8 464 8s-12.4 4-14.9 10.1l-29.4 74L343.6 68.9c-6.3-1.9-13.1 .2-17.2 5.3s-4.6 12.2-1.4 17.9l39.5 69.1-65.6 45.1c-5.4 3.7-8 10.3-6.5 16.7c.1 .3 .1 .6 .2 .8l19.4 0c20.1 0 39.2 7.5 53.8 20.8l18.4 2.9L383 265.3l36.2 48.3c2.1 2.8 3.9 5.7 5.5 8.6L464 286.1z"></path></svg>Accident report<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M246.6 470.6c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 402.7 361.4 265.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3l-160 160zm160-352l-160 160c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 210.7 361.4 73.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3z"></path></svg></span><div id="collapsible-content-1664910881810" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1664910881810"><div class="Collapsible__contentInner"><p class="title">Bad variable naming!</p>

<p>In our methods class, students often try to reproduce the original analyses from a published study before attempting to replicate the results in a new sample of participants. When Kengthsagn Louis looked at the code for the study she was interested in, she noticed that the variables in the analysis code were unnamed (presumably because they were output this way by the survey software). For example, one piece of Stata code looked like this!</p>
<pre verbatim="TRUE"><code>gen recall1=.
replace recall1=0 if Q21==1 
replace recall1=1 if Q21==3 | Q21==5 | Q21==6
replace recall1=2 if Q21==2 | Q21==4 | Q21==7 | Q21==8
replace recall1=0 if Q69==1 
replace recall1=1 if Q69==3 | Q69==5 | Q69==6
replace recall1=2 if Q69==2 | Q69==4 | Q69==7 | Q69==8
ta recall1</code></pre>
<p>In the process of translating this code into R in order to reproduce the analyses, Kengthsagn and a course teaching assistant, Andrew Lampinen, noticed that some participant responses had been assigned to the wrong variables. Because the variable names were not human-readable, this error was almost impossible to detect. After being made aware of the problem, the article’s author – to their credit – issued an immediate correction since the problem affected some of the inferential conclusions of the article <span class="citation">(<a href="#ref-petersen2019" role="doc-biblioref">Petersen, 2019</a>)</span>.</p>
<p>The moral of the story: obscure variable names can hide existing errors and create opportunities for further error! Sometimes you can adjust these within your experimental software, avoiding the issue. If not, make sure to create a “key” and translate the names immediately, double checking after you are done.</p>
</div></div></div></div></div>
</div>
<div id="document-the-format-of-your-data" class="section level3" number="8.2.5">
<h3><span class="header-section-number">8.2.5</span> Document the format of your data</h3>
<p>Even the best-organized tabular data are not always easy to understand by other researchers, or even yourself, especially after some time has passed. For that reason, best practices for data sharing include making a <strong>codebook</strong> (also known as a <strong>data dictionary</strong>) that explicitly documents what each variable is. Figure <a href="8-measurement.html#fig:management-mb-codebook">8.9</a> shows an example codebook for the trial-level data in the bottom of Figure <a href="8-measurement.html#fig:management-mb-datafiles">8.7</a>. Each row represents one variable in the associated dataset. Codebooks often describe what type of variable a column is (e.g., numeric, string), and what values can appear in that column. A human-readable explanation is often given as well, providing providing units (e.g., “seconds”) and a translation of numeric codes (e.g., “test condition is coded as 1”) where relevant.</p>
<div class="figure"><span style="display: block;" id="fig:management-mb-codebook"></span>
<p class="caption marginnote shownote">
Figure 8.9: Codebook for trial-level data (see above) from the ManyBabies (2020) case study.
</p>
<img src="images/management/mb1-codebook.png" alt="Codebook for trial-level data (see above) from the ManyBabies (2020) case study." width="\linewidth" />
</div>
<p>Creating a codebook need not require a lot of work. Almost any documentation is better than nothing! There are also several R packages that can automatically generate a codebook for you, for example <code>codebook</code>, <code>dataspice</code>, and <code>dataMaid</code> <span class="citation">(<a href="#ref-arslan2019" role="doc-biblioref">Arslan, 2019</a>)</span>. Adding a codebook can substantially increase the reuse value of the data and prevent hours of frustration as future-you and others try to decode your variable names and assumptions.</p>
</div>
</div>
<div id="sharing-research-products" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Sharing Research Products</h2>
<p>As we’ve been discussing throughout this chapter, if you’ve managed your research products effectively, sharing them with others is a far less daunting prospect, and usually just requires uploading them to an online repository like the Open Science Framework. This section discusses where and how to share research products and addresses some potential limitations on sharing that you should bear in mind.</p>
<div id="where-and-how-to-share" class="section level3" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> Where and how to share</h3>




<p>For shared research products<label for="tufte-sn-131" class="margin-toggle sidenote-number">131</label><input type="checkbox" id="tufte-sn-131" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">131</span> Most of this discussion is about data, because that’s where the community has focused its efforts. That said, almost everything here applies to other research products as well!</span> to be usable by others, they should meet a set of standards known as ‘FAIR’: Findable, Accessible, Interoperable, and Reusable <span class="citation">(<a href="#ref-wilkinson2016" role="doc-biblioref">Wilkinson et al., 2016</a>)</span>. Findable products are easily discoverable to both humans and machines. That means linking to them in research reports using unique persistent identifiers (e.g. a digital object identifier [DOI]).<label for="tufte-sn-132" class="margin-toggle sidenote-number">132</label><input type="checkbox" id="tufte-sn-132" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">132</span> DOIs are those long URL-like things that are often used to link to papers. Turns out they can also be associated with datasets and other research products. Critically, they are guaranteed to work to find stuff, whereas standard web URLs often go stale after several years when people refactor their website. Most online repositories, like the Open Science Framework, will issue DOIs for the research products you store there.</span> and attaching them with meta-data describing what they are so they can be indexed by search engines. Accessibility means that research products need to be preserved across the long-term and are retrievable via their standardized identifier. Interoperability means that the research products needs to be in a format that people and machines (e.g., search engines and analysis software) can understand. Reusable means that the research products need to be well organized, documented, and licensed so that others know how to use them.</p>
<p>If you’ve followed the guidance in the rest of this chapter, then you will already be well on your way to making your research products FAIR. There are a few final steps to consider. An important decision is where you are going to share the research products. We recommend uploading the files to a repository that’s designed according to support FAIR principles. Personal websites don’t cut it, since these sites tend to go out of date and disappear. There’s also no easy way to find research products on personal sites unless you know who created them. Github, though it’s a great platform for collaboration, isn’t a FAIR repository – for one thing, products there don’t have DOIs – and there are no archival guarantees on files that are shared there. And – perhaps surprisingly for some researchers – journal supplementary materials are also not a great place to put research products. Often they have no unique DOI or metadata, and they often change their URLS, leading data becoming unavailable <span class="citation">(<a href="#ref-evangelou2005" role="doc-biblioref">Evangelou et al., 2005</a>)</span>.</p>
<p>Fortunately, there are many repositories that help you conform to FAIR standards. Zenodo, Figshare, the Open Science Framework (OSF), and the various Dataverse sites are designed for this purpose, though there are many other domain-specific repositories that are particularly relevant for different research fields. We usually use the OSF as it makes it easy to share all research products connected to a project in one place. OSF is FAIR compatible and allows users to assign DOIs to their data and provide appropriate metadata.</p>
<p>We also recommend you attach a license to your research products. Academic culture is (usually) unburdened by discussion of intellectual property and legal rights and instead relies on scholarly norms about citation and attribution. The basic expectation is that if you rely on someone else’s research, you explicitly acknowledge the relevant journal article through a citation.</p>
<p>Although norms are still evolving, using research products created by others generally adheres to the same scholarly principle. However, research products can also be useful in non-academic contexts. Perhaps you created software that a company would like to use. Maybe a pediatrician would like to use a research instrument you’ve been working on to assess their patients. These applications (and many other reuses of the data) require a legal license. In practice, there are a number of simple, open source licenses that permit reuse. We tend to favor <a href="https://creativecommons.org">Creative Commons licenses</a>, which come in a variety of flavors such as
<a href="https://creativecommons.org/share-your-work/public-domain/cc0/">CC0</a> (which allows all reuse),
<a href="https://creativecommons.org/licenses/by/4.0/">CC-BY</a> (which allows reuse as long as there is attribution), and <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY-NC</a> (which only allows attributed, non-commercial reuse).<label for="tufte-sn-133" class="margin-toggle sidenote-number">133</label><input type="checkbox" id="tufte-sn-133" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">133</span> <span class="citation">O. Klein et al. (<a href="#ref-klein2018" role="doc-biblioref">2018</a>)</span> recommend the CC0 license, which puts no limits on what can be done with your data. At first blush it may seem like a license that requires attribution is useful. But it is academic norms and not the threat of litigation that enforces good citation practices, and more restrictive licenses can mean that some legitimate uses of your data can be blocked.</span> Regardless of what license you choose, having a license means that your products won’t be in a “not sure what I’m allowed to do with this” limbo for others who are interested in reusing them.</p>
<p>As we have discussed, you may want to consider working in the open from the outset. If you are using Github to manage your project, you can link the Git repository to the Open Science Framework so it automatically syncs. This provides a valuable incentive to organize your work properly throughout your project and makes sharing super easy, because you’ve already done it! On the other hand, this way of working can feel exposed for some researchers, and it does carry some risks, however small, of “scooping” or pre-emption by other groups working in the same space. Fortunately you can set up the same Git-OSF workflow and keep it private until your ready to make it public later on. The next stage at which you should consider sharing your research products is when you submit your study to a journal. If you’re still hesitant to make the project entirely public, many repositories (including OSF) will allow you to create special links that facilitate limited access to, for example, reviewers and editors. In general, the earlier you share your research products the better because there are more opportunities for others to learn from, build on, and verify your research.<label for="tufte-sn-134" class="margin-toggle sidenote-number">134</label><input type="checkbox" id="tufte-sn-134" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">134</span> If there are errors in our work, we’d certainly love to hear about it <em>before</em> the article is published in a journal rather than after!</span></p>
</div>
<div id="what-you-can-and-cant-share" class="section level3" number="8.3.2">
<h3><span class="header-section-number">8.3.2</span> What you can and can’t share</h3>
<p>We’ve been advocating that you share all of your research products, especially your data; however, in practice there can be some obstacles to sharing, especially if your research involves sensitive information. The most important of these is <strong>participant privacy</strong>. Unless they explicitly waive these rights, participants in psychology experiments have the expectation of privacy – that is, no one should be able to identify them from the data they have provided. Protecting participant privacy is an important part of researchers’ ethical responsibilities <span class="citation">(<a href="#ref-ross2018" role="doc-biblioref">M. W. Ross et al., 2018</a>)</span>, and needs to be balanced against the ethical imperatives to share (see Chapter <a href="4-ethics.html#ethics">4</a>).<label for="tufte-sn-135" class="margin-toggle sidenote-number">135</label><input type="checkbox" id="tufte-sn-135" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">135</span> <span class="citation">Meyer (<a href="#ref-meyer2018" role="doc-biblioref">2018</a>)</span> gives an excellent overview of how to navigate various legal and ethical issues around data sharing in the US context.</span></p>
<p>Furthermore, there are legal regulations that protect participants’ data, though these vary from country to country. In the US, the relevant regulation is <strong>HIPAA</strong>, the Health Insurance Portability and Accountability Act, which limits disclosures of private health information (<strong>PHI</strong>). In the European Union, the relevant regulation is the European GDPR (General Data Protection Regulation). It’s beyond the scope of this book to give a full treatment of these regulatory frameworks; you should consult with your local IRB regarding compliance, but here is the way we have navigated this situation while still sharing data.</p>
<p>Under both frameworks, <strong>anonymization</strong> (or equivalently <strong>de-identification</strong>) of data is a key concept, such that data sharing is generally just fine if the data meet the relevant standard. Under USguidelines, researchers can follow the “safe harbor” standard<label for="tufte-sn-136" class="margin-toggle sidenote-number">136</label><input type="checkbox" id="tufte-sn-136" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">136</span> <a href="https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html">As described on the relevant DHHS page</a>.</span> under which data are considered to be anonymized if they do not contain identifiers like names, telephone numbers, email addresses, social security numbers, dates of birth, faces, etc. Thus, data that only contain participant IDs and nothing from this list can typically be shared without participant consent without a problem.<label for="tufte-sn-137" class="margin-toggle sidenote-number">137</label><input type="checkbox" id="tufte-sn-137" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">137</span> US IRBs are a very de-centralized bunch and their interpretations often vary considerably. For reasons of liability or ethics, they may not allow data sharing even though it is permitted by US law. If you feel like arguing with an IRB that takes this kind of stand, you could mention that the DHHS rule actually doesn’t consider de-identified data to be “human subjects” data at all, and thus the IRB may not have regulatory authority over it. We’re not lawyers, and we’re not sure if you’ll succeed but it could be worth a try.</span></p>
<p>The EU’s GDPR also allows fully anonymized data sharing, with one big complication. Putting anonymous identifiers in a data file and removing identifiable fields does not itself suffice for GDPR anonymization if the data are still <strong>in-principle re-identifiable</strong> because you have maintained documentation linking IDs to identifiable data like names or email addresses. Only when the key linking identifiers to data has been destroyed are the data truly de-identified according to this standard.</p>
<p>De-identification is not always enough. As datasets get richer, <strong>statistical reidentification risks</strong> go up substantially such that, with a little bit of outside information, data can be matched with a unique individual. These risks are especially high with linguistic, physiological, and geospatial data, but they can be present even for simple behavioral experiments. In one influential demonstration, knowing a person’s location on two occasions was often enough to identify their data uniquely in a huge database of credit card transactions <span class="citation">(<a href="#ref-de-montjoye2015" role="doc-biblioref">De Montjoye et al., 2015</a>)</span>.<label for="tufte-sn-138" class="margin-toggle sidenote-number">138</label><input type="checkbox" id="tufte-sn-138" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">138</span> For an example closer to home, many of the contributing labs in the ManyBabies project logged the date of test for each participant. This useful and seemingly innocuous piece of information is unlikely to identify any particular participant – but alongside a social media post about a lab visit or a dataset about travel records, it could easily reveal a participant’s identity.</span> Thus, simply removing fields from the data is a good starting point, but if you are collecting richer data about participants’ behavior you may need to consult an expert.</p>
<div id="island_6"><div class="box ethical_considerations"><div class="Collapsible"><span id="collapsible-trigger-1664910881811" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1664910881811" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="leaf" class="svg-inline--fa fa-leaf " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M272 96c-78.6 0-145.1 51.5-167.7 122.5c33.6-17 71.5-26.5 111.7-26.5h88c8.8 0 16 7.2 16 16s-7.2 16-16 16H288 216s0 0 0 0c-16.6 0-32.7 1.9-48.3 5.4c-25.9 5.9-49.9 16.4-71.4 30.7c0 0 0 0 0 0C38.3 298.8 0 364.9 0 440v16c0 13.3 10.7 24 24 24s24-10.7 24-24V440c0-48.7 20.7-92.5 53.8-123.2C121.6 392.3 190.3 448 272 448l1 0c132.1-.7 239-130.9 239-291.4c0-42.6-7.5-83.1-21.1-119.6c-2.6-6.9-12.7-6.6-16.2-.1C455.9 72.1 418.7 96 376 96L272 96z"></path></svg>Ethical considerations<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M246.6 470.6c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 402.7 361.4 265.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3l-160 160zm160-352l-160 160c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 210.7 361.4 73.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3z"></path></svg></span><div id="collapsible-content-1664910881811" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1664910881811"><div class="Collapsible__contentInner"><p class="title">Really anonymous?</p>

<p>When we first began teaching Psych 251, our experimental methods course at Stanford, one of the biggest contributions of the course was simply showing students how to do experiments online. Amazon’s Mechanical Turk crowdsourcing service was relatively new, and our IRB did not have a good sense of what this service really was. We proposed that we would share data from the class and received approval for this practice. Our datasets were downloaded directly from Mechanical Turk and included participants’ MTurk IDs (long alphanumeric strings that seemed completely anonymous). Several experiences caused us to reconsider this practice!</p>
<p>First, we discovered that MTurk IDs were in some cases linked to study participants’ public Amazon “wish lists,” which could both inadvertently provide information about the participant and also even potentially provide a basis for reidentification (in rare cases). This discovery led us to consult with our IRB and provide more explicit consent language in our class experiments, linking to instructions for making Amazon profiles private.</p>
<p>Then, a little later we received an irate email from an MTurk participant who had discovered their data on github via a search for their MTurk ID. Although they were not identified in this dataset, it convinced us that at least some participants would not like this ID shared. After another consultation with the IRB, we apologized to this individual and removed their and others’ IDs from our github commit histories across that and other repositories. We now take care to anonymize IDs by creating a secret mapping between the IDs we post and the actual MTurk IDs prior to posting data.</p>
</div></div></div></div></div>
<div class="figure"><span style="display: block;" id="fig:management-sharing-chart"></span>
<p class="caption marginnote shownote">
Figure 8.10: A decision chart for thinking about sharing research products. Adapted from Klein et al. (2018).
</p>
<img src="images/management/kline2.png" alt="A decision chart for thinking about sharing research products. Adapted from Klein et al. (2018)." width="\linewidth" />
</div>
<p>Privacy issues are ubiquitous in data sharing, and almost every experimental research project will need to solve them before sharing data. For simple projects, often these are the only issues that preclude data sharing. However, in more complex projects, other concerns can arise. Funders may have specific mandates regarding where your data should be shared. Data use agreements or collaborator preferences may restrict where and when you can share. And certain data types require much more sensitivity since they are more consequential than, say, the reaction times on a Stroop task. We include here a set of questions to walk through to plan your sharing (Figure <a href="8-measurement.html#fig:management-sharing-chart">8.10</a>). When in doubt, it’s often a good idea to consult with the relevant local authority, e.g. your IRB for ethical issues or your research management office for regulatory issues.</p>

</div>
</div>
<div id="chapter-summary" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> Chapter summary</h2>
<p>All of the hard work you put into your experiments – not to mention the contributions of your participants – can be undermined by bad data and project management. As our accident reports and case study show, bad organizational practices can at a minimum cause huge headaches. Sometimes the consequences can be even worse. On the flip side, starting with a firm organizational foundation sets your experiment up for success. These practices also make it easier to share all of the products of your research, not just your findings. Such sharing is both useful for individual researchers and for the field as a whole.</p>




<div id="island_7"><div class="box discussion_questions"><div class="Collapsible"><span id="collapsible-trigger-1664910881812" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1664910881812" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="pen-ruler" class="svg-inline--fa fa-pen-ruler " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M469.3 19.3l23.4 23.4c25 25 25 65.5 0 90.5l-56.4 56.4L322.3 75.7l56.4-56.4c25-25 65.5-25 90.5 0zM44.9 353.2L299.7 98.3 413.7 212.3 158.8 467.1c-6.7 6.7-15.1 11.6-24.2 14.2l-104 29.7c-8.4 2.4-17.4 .1-23.6-6.1s-8.5-15.2-6.1-23.6l29.7-104c2.6-9.2 7.5-17.5 14.2-24.2zM249.4 103.4L103.4 249.4 16 161.9c-18.7-18.7-18.7-49.1 0-67.9L94.1 16c18.7-18.7 49.1-18.7 67.9 0l19.8 19.8c-.3 .3-.7 .6-1 .9l-64 64c-6.2 6.2-6.2 16.4 0 22.6s16.4 6.2 22.6 0l64-64c.3-.3 .6-.7 .9-1l45.1 45.1zM408.6 262.6l45.1 45.1c-.3 .3-.7 .6-1 .9l-64 64c-6.2 6.2-6.2 16.4 0 22.6s16.4 6.2 22.6 0l64-64c.3-.3 .6-.7 .9-1L496 350.1c18.7 18.7 18.7 49.1 0 67.9L417.9 496c-18.7 18.7-49.1 18.7-67.9 0l-87.4-87.4L408.6 262.6z"></path></svg>Discussion questions<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M246.6 470.6c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 402.7 361.4 265.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3l-160 160zm160-352l-160 160c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 210.7 361.4 73.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3z"></path></svg></span><div id="collapsible-content-1664910881812" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1664910881812"><div class="Collapsible__contentInner">
<ol style="list-style-type: decimal;">
<li><p>Find an Open Science Framework repository that corresponds to a published paper. What is their strategy for documenting what is shared? How easy is it to figure out where everything is and if the data and materials sharing is complete?</p></li>
<li><p>Open up the US Department of Health and Human Services “safe harbor” standards <a href="https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html">here</a> and navigate to the section called “The De-identification Standard.” Go through the list of identifiers that must be removed. Are there any on this list that you would need to conduct your own research? Can you think of any others that do not fall on this list?</p></li>
</ol>
</div></div></div></div></div>
<div id="island_8"><div class="box readings"><div class="Collapsible"><span id="collapsible-trigger-1664910881813" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1664910881813" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M96 0C43 0 0 43 0 96V416c0 53 43 96 96 96H384h32c17.7 0 32-14.3 32-32s-14.3-32-32-32V384c17.7 0 32-14.3 32-32V32c0-17.7-14.3-32-32-32H384 96zm0 384H352v64H96c-17.7 0-32-14.3-32-32s14.3-32 32-32zm32-240c0-8.8 7.2-16 16-16H336c8.8 0 16 7.2 16 16s-7.2 16-16 16H144c-8.8 0-16-7.2-16-16zm16 48H336c8.8 0 16 7.2 16 16s-7.2 16-16 16H144c-8.8 0-16-7.2-16-16s7.2-16 16-16z"></path></svg>Readings<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M246.6 470.6c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 402.7 361.4 265.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3l-160 160zm160-352l-160 160c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 210.7 361.4 73.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3z"></path></svg></span><div id="collapsible-content-1664910881813" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1664910881813"><div class="Collapsible__contentInner">
<ul>
<li>A more in-depth tutorial on various aspects of scientific openness: Klein, O., Hardwicke, T. E., Aust, F., Breuer, J., Danielsson, H., Hofelich Mohr, A., Ijzerman, H., Nilsonne, G., Vanpaemel, W., &amp; Frank, M. C. (2018). A practical guide for transparency in psychological science. <em>Collabra: Psychology</em>, <em>4</em>, 20. <a href>https://doi.org/10.1525/collabra.158</a></li>
</ul>
</div></div></div></div></div>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-arslan2019" class="csl-entry">
Arslan, R. C. (2019). How to automatically document data with the codebook package to facilitate data reuse. <em>Advances in Methods and Practices in Psychological Science</em>, <em>2</em>(2), 169–187.
</div>
<div id="ref-bischoff-grethe2007" class="csl-entry">
Bischoff-Grethe, A., Ozyurt, I. B., Busa, E., Quinn, B. T., Fennema-Notestine, C., Clark, C. P., Morris, S., Bondi, M. W., Jernigan, T. L., Dale, A. M.others. (2007). A technique for the deidentification of structural brain MR images. <em>Human Brain Mapping</em>, <em>28</em>(9), 892–903.
</div>
<div id="ref-bornstein1998" class="csl-entry">
Bornstein, M. H., &amp; Haynes, O. M. (1998). Vocabulary competence in early childhood: Measurement, latent construct, and predictive validity. <em>Child Development</em>, <em>69</em>(3), 654–671.
</div>
<div id="ref-brandmaier2018" class="csl-entry">
Brandmaier, A. M., Wenger, E., Bodammer, N. C., Kühn, S., Raz, N., &amp; Lindenberger, U. (2018). Assessing reliability in neuroimaging research through intra-class effect decomposition (ICED). <em>Elife</em>, <em>7</em>, e35718.
</div>
<div id="ref-broman2018" class="csl-entry">
Broman, K. W., &amp; Woo, K. H. (2018). Data organization in spreadsheets. <em>The American Statistician</em>, <em>72</em>(1), 2–10.
</div>
<div id="ref-campbell1928account" class="csl-entry">
Campbell, Norman Robert. (1928). <em>An account of the principles of measurement and calculation</em>. Longmans, Green; Company, Limited.
</div>
<div id="ref-campbell1938symposium" class="csl-entry">
Campbell, Norman R., &amp; Jeffreys, H. (1938). Symposium: Measurement and its importance for philosophy. <em>Proceedings of the Aristotelian Society, Supplementary Volumes</em>, <em>17</em>, 121–151.
</div>
<div id="ref-cattel1890mental" class="csl-entry">
Cattel, J. M. (1890). Mental tests and measurements. <em>Mind</em>, <em>15</em>, 373–380.
</div>
<div id="ref-cattell1962relational" class="csl-entry">
Cattell, R. B. (1962). The relational simplex theory of equal interval and absolute scaling. <em>Acta Psychologica</em>, <em>20</em>, 139–158.
</div>
<div id="ref-chang2004inventing" class="csl-entry">
Chang, H. (2004). <em>Inventing temperature: Measurement and scientific progress</em>. Oxford University Press.
</div>
<div id="ref-darrigol2003number" class="csl-entry">
Darrigol, O. (2003). Number and measure: Hermann von helmholtz at the crossroads of mathematics, physics, and psychology. <em>Studies in History and Philosophy of Science Part A</em>, <em>34</em>(3), 515–573.
</div>
<div id="ref-de-montjoye2015" class="csl-entry">
De Montjoye, Y.-A., Radaelli, L., Singh, V. K.others. (2015). Unique in the shopping mall: On the reidentifiability of credit card metadata. <em>Science</em>, <em>347</em>(6221), 536–539.
</div>
<div id="ref-donders1969speed" class="csl-entry">
Donders, F. C. (1868). On the speed of mental processes. <em>Acta Psychologica</em>, <em>30</em>, 412–431.
</div>
<div id="ref-evangelou2005" class="csl-entry">
Evangelou, E., Trikalinos, T. A., &amp; Ioannidis, J. P. (2005). Unavailability of online supplementary scientific information from articles published in major journals. <em>The FASEB Journal</em>, <em>19</em>(14), 1943–1944.
</div>
<div id="ref-fechner1860elemente" class="csl-entry">
Fechner, G. T. (1860). <em>Elemente der psychophysik</em> (Vol. 2). Breitkopf u. H<span>ä</span>rtel.
</div>
<div id="ref-fechner1987my" class="csl-entry">
Fechner, G. T. (1987). My own viewpoint on mental measurement (1887). <em>Psychological Research</em>, <em>49</em>(4), 213–219.
</div>
<div id="ref-ferguson1940" class="csl-entry">
Ferguson, M., A., &amp; Tucker, W. S. (1940). Quantitative estimates of sensory events, final report. <em>Report of the British Association for the Advancement of Science</em>, 331–349.
</div>
<div id="ref-frank2021" class="csl-entry">
Frank, M. C., Braginsky, M., Yurovsky, D., &amp; Marchman, V. A. (2021). <em>Variability and consistency in early language learning: The wordbank project</em>. MIT Press.
</div>
<div id="ref-gilmore2017" class="csl-entry">
Gilmore, R. O., &amp; Adolph, K. E. (2017). Video can make behavioural science more reproducible. In <em>Nature Human Behaviour</em> (No. 7; Vol. 1).
</div>
<div id="ref-hardcastle1995ss" class="csl-entry">
Hardcastle, G. L. (1995). SS stevens and the origins of operationism. <em>Philosophy of Science</em>, 404–424.
</div>
<div id="ref-hardwicke2018b" class="csl-entry">
Hardwicke, T. E., Mathur, M. B., MacDonald, K. E., Nilsonne, G., Banks, G. C., Kidwell, M., Mohr, A. H., Clayton, E., Yoon, E. J., Tessler, M. H., Lenne, R. L., Altman, S. K., Long, B., &amp; Frank, M. C. (2018). <em>Data availability, reusability, and analytic reproducibility: Evaluating the impact of a mandatory open data policy at the journal cognition</em>.
</div>
<div id="ref-hedge2018" class="csl-entry">
Hedge, C., Powell, G., &amp; Sumner, P. (2018). The reliability paradox: Why robust cognitive tasks do not produce reliable individual differences. <em>Behavior Research Methods</em>, <em>50</em>(3), 1166–1186.
</div>
<div id="ref-heidelberger2004nature" class="csl-entry">
Heidelberger, M. (2004). <em>Nature from within: Gustav theodor fechner and his psychophysical worldview</em>. University of Pittsburgh Pre.
</div>
<div id="ref-kisch1965scales" class="csl-entry">
Kisch, B. (1965). <em>Scales and weights: A historical outline</em>. Yale University Press.
</div>
<div id="ref-klein2018" class="csl-entry">
Klein, O., Hardwicke, T. E., Aust, F., Breuer, J., Danielsson, H., Mohr, A. H., IJzerman, H., Nilsonne, G., Vanpaemel, W., &amp; Frank, M. C. (2018). <em>A practical guide for transparency in psychological science</em>.
</div>
<div id="ref-lage-castellanos2019" class="csl-entry">
Lage-Castellanos, A., Valente, G., Formisano, E., &amp; De Martino, F. (2019). Methods for computing the maximum performance of computational models of fMRI responses. <em>PLoS Computational Biology</em>, <em>15</em>(3), e1006397.
</div>
<div id="ref-luce2007foundations" class="csl-entry">
Luce, Robert Duncan, Krantz, D. H., Suppes, P., &amp; Tversky, A. (1990). <em>Foundations of measurement III: Representation, axiomatization, and invariance</em>. Courier Corporation.
</div>
<div id="ref-luce1964simultaneous" class="csl-entry">
Luce, R. Duncan, &amp; Tukey, J. W. (1964). Simultaneous conjoint measurement: A new type of fundamental measurement. <em>Journal of Mathematical Psychology</em>, <em>1</em>(1), 1–27.
</div>
<div id="ref-marchman2008" class="csl-entry">
Marchman, V. A., &amp; Fernald, A. (2008). Speed of word recognition and vocabulary knowledge in infancy predict cognitive and language outcomes in later childhood. <em>Developmental Science</em>, <em>11</em>(3), F9–F16.
</div>
<div id="ref-maul2016philosophical" class="csl-entry">
Maul, A., Irribarra, D. T., &amp; Wilson, M. (2016). On the philosophical foundations of psychological measurement. <em>Measurement</em>, <em>79</em>, 311–320.
</div>
<div id="ref-meyer2018" class="csl-entry">
Meyer, M. N. (2018). Practical tips for ethical data sharing. <em>Advances in Methods and Practices in Psychological Science</em>, <em>1</em>(1), 131–144.
</div>
<div id="ref-michell1999measurement" class="csl-entry">
Michell, J. (1999). <em>Measurement in psychology: A critical history of a methodological concept</em> (Vol. 53). Cambridge University Press.
</div>
<div id="ref-moscati2018measuring" class="csl-entry">
Moscati, I. (2018). <em>Measuring utility: From the marginal revolution to behavioral economics</em>. Oxford University Press.
</div>
<div id="ref-narens1986measurement" class="csl-entry">
Narens, L., &amp; Luce, R. D. (1986). Measurement: The theory of numerical assignments. <em>Psychological Bulletin</em>, <em>99</em>(2), 166.
</div>
<div id="ref-petersen2019" class="csl-entry">
Petersen, M. B. (2019). <em>&quot; healthy out-group members are represented psychologically as infected in-group members&quot;: corrigendum.</em>
</div>
<div id="ref-putnam1999threefold" class="csl-entry">
Putnam, H. (2000). <em>The threefold cord: Mind, body, and world</em>. Columbia Univ. Press.
</div>
<div id="ref-ross2018" class="csl-entry">
Ross, M. W., Iguchi, M. Y., &amp; Panicker, S. (2018). Ethical aspects of data sharing and research participant protections. <em>American Psychologist</em>, <em>73</em>(2), 138.
</div>
<div id="ref-sijtsma2009" class="csl-entry">
Sijtsma, K. (2009). On the use, the misuse, and the very limited usefulness of cronbach’s alpha. <em>Psychometrika</em>, <em>74</em>(1), 107.
</div>
<div id="ref-stevens1946" class="csl-entry">
Stevens, S. S. (1946). On the theory of scales of measurement. <em>Science</em>, <em>103</em>(2684), 677–680.
</div>
<div id="ref-sep-measurement-science" class="csl-entry">
Tal, E. (2020). <span class="nocase">Measurement in Science</span>. In E. N. Zalta (Ed.), <em>The <span>Stanford</span> encyclopedia of philosophy</em> (<span>F</span>all 2020). <a href="https://plato.stanford.edu/archives/fall2020/entries/measurement-science/">https://plato.stanford.edu/archives/fall2020/entries/measurement-science/</a>; Metaphysics Research Lab, Stanford University.
</div>
<div id="ref-wilkinson2016" class="csl-entry">
Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., Silva Santos, L. B. da, Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The <span>FAIR</span> guiding principles for scientific data management and stewardship. <em>Sci Data</em>, <em>3</em>, 160018.
</div>
<div id="ref-woodworth1938" class="csl-entry">
Woodworth, R. S. (1938). <em>Experimental psychology.</em>
</div>
<div id="ref-ziemann2016" class="csl-entry">
Ziemann, M., Eren, Y., &amp; El-Osta, A. (2016). Gene name errors are widespread in the scientific literature. <em>Genome Biology</em>, <em>17</em>(1), 1–3.
</div>
</div>
<p style="text-align: center;">
<a href="7-models.html"><button class="btn btn-default">Previous</button></a>
<a href="9-writing.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



<script type="module" src="/assets/src/index.page.client.jsx.c78c6f50.js"></script><script id="vite-plugin-ssr_pageContext" type="application/json">{"pageContext":{"_pageId":"/src/index","islands":[{"id":"island_0","name":"TOC","props":{}},{"id":"island_1","name":"Box","props":{"title":"!undefined","type":"learning_goals","content":"\n\u003cul>\n\u003cli>Discuss the reliability and validity of psychological measures\u003c/li>\n\u003cli>Reason about tradeoffs between different measures and measure types\u003c/li>\n\u003cli>Identify the characteristics of well-constructed survey questions\u003c/li>\n\u003cli>Articulate risks of measurement flexibility and the costs and benefits of multiple measures\u003c/li>\n\u003c/ul>\n"}},{"id":"island_2","name":"Box","props":{"title":"A reliable and valid measure of children’s vocabulary","type":"case_study","content":"\n\n\u003cp>Anyone who has worked with little children or had children of their own can attest to how variable their early language is. Some children speak clearly and produce long sentences from an early age, while others struggle; this variation appears to be linked to later school outcomes \u003cspan class=\"citation\">(\u003ca href=\"#ref-marchman2008\" role=\"doc-biblioref\">Marchman &amp; Fernald, 2008\u003c/a>)\u003c/span>. Thus, there are many reasons why you’d want to make precise measurements of children’s early language ability as a latent construct of interest.\u003clabel for=\"tufte-sn-111\" class=\"margin-toggle sidenote-number\">111\u003c/label>\u003cinput type=\"checkbox\" id=\"tufte-sn-111\" class=\"margin-toggle\" />\u003cspan class=\"sidenote\">\u003cspan class=\"sidenote-number\">111\u003c/span> Of course, you can also ask if early language is a single construct, or whether it is multi-dimensional! For example, does grammar develop separately from vocabulary? It turns out the two are very closely coupled \u003cspan class=\"citation\">(\u003ca href=\"#ref-frank2021\" role=\"doc-biblioref\">Frank et al., 2021\u003c/a>)\u003c/span>. This point illustrates the general idea that, especially in psychology, measurement and theory building are intimately related – you need data to inform your theory, but the measurement instruments you use to collect your data in turn presuppose some theory!\u003c/span>\u003c/p>\n\u003cp>Because bringing children into a lab can be expensive, one popular option for measuring child language is the MacArthur Bates Communicative Development Inventory (CDI for short), a form which asks parents to mark words that their child says or understands. CDI forms are basically long checklists of words (the first page of an English form is shown in Figure \u003ca href=\"8-measurement.html#fig:measurement-cdi\">8.1\u003c/a>. But is parent report a reliable or valid measure of children’s early language?\u003c/p>\n\u003cp>\n\u003cspan class=\"marginnote shownote\">\n\u003cspan style=\"display: block;\" id=\"fig:measurement-cdi\">\u003c/span>\n\u003cimg src=\"images/measurement/cdi.jpg\" alt=\"The first page of the MacArthur-Bates Communicative Development Inventory, a parent report instrument for measuring children's early language.\" width=\"\\linewidth\" />\nFigure 8.1: The first page of the MacArthur-Bates Communicative Development Inventory, a parent report instrument for measuring children’s early language.\n\u003c/span>\n\u003c/p>\n\u003cp>\n\u003cspan class=\"marginnote shownote\">\n\u003cspan style=\"display: block;\" id=\"fig:measurement-psycho-cors\">\u003c/span>\n\u003cimg src=\"images/measurement/psycho-cors2.png\" alt=\"Longitudinal correlations between a child's score on one administration of the CDI and another one several months later. From Frank et al. (2021). \" width=\"\\linewidth\" />\nFigure 8.2: Longitudinal correlations between a child’s score on one administration of the CDI and another one several months later. From Frank et al. (2021). \n\u003c/span>\n\u003c/p>\n\u003cp>One test of the reliability of the CDI is a \u003cstrong>test-retest\u003c/strong> correlation, where we compute the correlation within children between two different administrations of the form. Unfortunately, this analysis has one issue: the longer you wait between observations the more the child has changed! Figure @ref(fig:measurement-psycho-cors longitudinal test-retest correlations for two CDIs, showing how correlations start off high and drop off as the gap between observations increases \u003cspan class=\"citation\">(\u003ca href=\"#ref-frank2021\" role=\"doc-biblioref\">Frank et al., 2021\u003c/a>)\u003c/span>.\u003c/p>\n\u003cp>\n\u003cspan class=\"marginnote shownote\">\n\u003cspan style=\"display: block;\" id=\"fig:measurement-cdi-validity\">\u003c/span>\n\u003cimg src=\"images/measurement/cdi-validity.png\" alt=\"Relations between an early form of the CDI (the ELI) and several other measurements of children's early language from both transcripts and direct assessments. Arrows indicate loadings of different measures (boxes) onto a single construct (circle). Numbers give standardized weights [@bornstein1998].\" width=\"\\linewidth\" />\nFigure 8.3: Relations between an early form of the CDI (the ELI) and several other measurements of children’s early language from both transcripts and direct assessments. Arrows indicate loadings of different measures (boxes) onto a single construct (circle). Numbers give standardized weights \u003cspan class=\"citation\">(\u003ca href=\"#ref-bornstein1998\" role=\"doc-biblioref\">Bornstein &amp; Haynes, 1998\u003c/a>)\u003c/span>.\n\u003c/span>\n\u003c/p>\n\u003cp>Given that CDI forms are relatively reliable instruments, are they valid? That is, do they really measure the construct of interest, namely children’s early language ability? \u003cspan class=\"citation\">Bornstein &amp; Haynes (\u003ca href=\"#ref-bornstein1998\" role=\"doc-biblioref\">1998\u003c/a>)\u003c/span> collected many different measures of children’s language – including the ELI (an early CDI form) and other “gold standard” measures like transcribed samples of children’s speech. Figure \u003ca href=\"8-measurement.html#fig:measurement-cdi-validity\">8.3\u003c/a> shows the results of a structural equation model that measures the shared variance between these measures and a hypothesized central construct (“vocabulary competence”). The ELI (CDI) score correlated closely with the shared variance among all the different measures, suggesting that it was a valid measure of the construct.\u003c/p>\n\u003cp>The combination of reliability and validity evidence suggests that CDI are a useful (and relatively inexpensive source) of data about children’s early language, and indeed they have become one of the most common assessments for this age group!\u003c/p>\n\n"}},{"id":"island_3","name":"Box","props":{"title":"Early controversies over psychological measurement","type":"depth","content":"\n\n\u003cblockquote>\n\u003cp>“Psychology cannot attain the certainty and exactness of the physical sciences, unless it rests on a foundation of […] measurement” \u003cspan class=\"citation\">(\u003ca href=\"#ref-cattel1890mental\" role=\"doc-biblioref\">Cattel, 1890\u003c/a>)\u003c/span>.\u003c/p>\n\u003c/blockquote>\n\u003cp>It is no coincidence that the founders of experimental psychology were obsessed with measurement \u003cspan class=\"citation\">(\u003ca href=\"#ref-heidelberger2004nature\" role=\"doc-biblioref\">Heidelberger, 2004\u003c/a>)\u003c/span>.\nIt was viewed as the primary obstacle facing psychology on its road to becoming a legitimate quantitative science.\nFor example, one of the final pieces written by Hermann von Helmholtz (Wilhelm Wundt’s doctoral advisor), was a 1887 philosophical treatise entitled “Zahlen und Messen” (“Counting and Measuring”; see \u003cspan class=\"citation\">Darrigol (\u003ca href=\"#ref-darrigol2003number\" role=\"doc-biblioref\">2003\u003c/a>)\u003c/span>).\nIn the same year, \u003cspan class=\"citation\">Fechner (\u003ca href=\"#ref-fechner1987my\" role=\"doc-biblioref\">1987\u003c/a>)\u003c/span> explicitly grappled with the foundations of measurement in “Uber die psychischen Massprincipien” (“On Psychic Measurement Principles”).\u003c/p>\n\u003cp>Many of the early debates over measurement revolved around the emerging area of \u003cem>psychophysics\u003c/em>, the problem of relating objective, physical stimuli (e.g. light or sound or pressure) to the subjective sensations they produce in the mind.\nFor example, \u003cspan class=\"citation\">Fechner (\u003ca href=\"#ref-fechner1860elemente\" role=\"doc-biblioref\">1860\u003c/a>)\u003c/span> was interested in a quantity called the “just noticeable difference” (JND), the smallest change in a stimulus that can be discriminated by our senses.\nHe argued for a lawful (logarithmic) relationship: a logarithmic change in the intensity of, say, brightness corresponded to a linear change in the reported intensity (up to some constant).\nIn other words, sensation was \u003cem>measurable\u003c/em> via instruments like the JND.\u003c/p>\n\u003cp>It may be surprising to modern ears that the basic claim of measurability was controversial, even if the precise form of the psychophysical function would continue to be debated.\nBut this claim led to a deeply rancorous debate, culminating with the so-called Ferguson Committee, formed by the British Association for the Advancement of Science in 1932 to investigate whether such psychophysical procedures could count as quantitative ‘measurements’ of anything at all \u003cspan class=\"citation\">(\u003ca href=\"#ref-moscati2018measuring\" role=\"doc-biblioref\">Moscati, 2018\u003c/a>)\u003c/span>.\nIt was unable to reach a conclusion, with physicists and psychologists deadlocked:\u003c/p>\n\u003cblockquote>\n\u003cp>Having found that individual sensations have an order, they [some psychologists] assume that they are \u003cem>measurable\u003c/em>. Having travestied physical measurement in order to justify that assumption, they assume that their sensation intensities will be related to stimuli by numerical laws […] which, if they mean anything, are certainly false. \u003cspan class=\"citation\">(\u003ca href=\"#ref-ferguson1940\" role=\"doc-biblioref\">Ferguson &amp; Tucker, 1940\u003c/a>)\u003c/span>\u003c/p>\n\u003c/blockquote>\n\u003cp>The heart of the disagreement was rooted in the classical definition of quantity requiring strictly \u003cem>additive\u003c/em> structure.\nAn attribute was only considered measurable in light of a meaningful concatenation operation.\nFor example, weight was a measurable attribute because putting a bag of three rocks on a scale yields the same number as putting each of the three rock on separate scales and then summing up those numbers (in philosophy of science, attributes with this concatenation property are known as “extensive” attributes, as opposed to “intensive” ones.)\nNorman Campbell, one of the most prominent members of the Ferguson Committee, had recently defined \u003cem>fundamental\u003c/em> measurement in this way \u003cspan class=\"citation\">(e.g. see \u003ca href=\"#ref-campbell1928account\" role=\"doc-biblioref\">Norman Robert Campbell, 1928\u003c/a>)\u003c/span>, contrasting it with \u003cem>derived measurement\u003c/em> which was some function of fundamental measures.\nAccording to the physicists on the Ferguson Committee, measuring mental sensations was impossible because they could never be grounded in any \u003cem>fundamental\u003c/em> scale with this kind of additive operation.\nIt just didn’t make sense to break up holistic sensations into parts the way we would weights or lengths: they didn’t come in “amounts” or “quantities” that could be combined \u003cspan class=\"citation\">(\u003ca href=\"#ref-cattell1962relational\" role=\"doc-biblioref\">Cattell, 1962\u003c/a>)\u003c/span>.\nEven the intuitive additive logic of \u003cspan class=\"citation\">Donders (\u003ca href=\"#ref-donders1969speed\" role=\"doc-biblioref\">1868\u003c/a>)\u003c/span>’s “method of subtraction” for measuring the speed of mental processes was viewed skeptically on the same grounds by the time of the committee (e.g. in an early textbook, \u003cspan class=\"citation\">Woodworth (\u003ca href=\"#ref-woodworth1938\" role=\"doc-biblioref\">1938\u003c/a>)\u003c/span> claimed “we cannot break up the reaction into successive acts and obtain the time for each act.”)\u003c/p>\n\u003cp>The primary target of the Ferguson Committee’s investigation was the psychologist S. S. Stevens, who had claimed to measure the sensation of loudness using psychophysical instruments.\nExiled from classical frameworks of measurement, he went about developing an alternative “operational” framework \u003cspan class=\"citation\">(\u003ca href=\"#ref-stevens1946\" role=\"doc-biblioref\">Stevens, 1946\u003c/a>)\u003c/span>, where the classical ratio scale recognized by physicists was only one of several ways of assigning numbers to things (see \u003ca href=\"8-measurement.html#tab:measurement-stevens-table\">8.1\u003c/a> below).\nStevens’ framework quickly spread, leading to an explosion of proposed measures.\nHowever, operationalism remains controversial outside psychology \u003cspan class=\"citation\">(\u003ca href=\"#ref-michell1999measurement\" role=\"doc-biblioref\">Michell, 1999\u003c/a>)\u003c/span>.\nThe most extreme version of his stance (“measurement is the assignment of numerals to objects or events according to rule”) permits researchers to \u003cem>define\u003c/em> constructs operationally in terms of a measure \u003cspan class=\"citation\">(\u003ca href=\"#ref-hardcastle1995ss\" role=\"doc-biblioref\">Hardcastle, 1995\u003c/a>)\u003c/span>.\nFor example, one may say that the construct of intelligence is simply \u003cem>whatever it is\u003c/em> that IQ measures.\nIt is then left up to the researcher to decide which scale type their proposed measure should belong to.\u003c/p>\n\u003cp>In Chapter \u003ca href=\"2-theories.html#theories\">2\u003c/a>, we outlined a somewhat different view, closer to a kind of constructive realism \u003cspan class=\"citation\">Putnam (\u003ca href=\"#ref-putnam1999threefold\" role=\"doc-biblioref\">2000\u003c/a>)\u003c/span>.\nPsychological constructs like working memory or theory of mind are taken to exist independent of any given operationalization, putting us on firmer ground to debate the pros and cons associated with different ways of measuring the same construct.\nIn other words, we are not free to assign numerals however we like.\nWhether a particular construct or quantity is measurable on a particular scale should be treated as an empirical question.\u003c/p>\n\u003cp>The next major breakthrough in measurement theory emerged with the birth of mathematical psychology in the 1960s, which aimed to put psychological measurement on more rigorous foundations.\nThis effort culminated in the three-volume Foundations of Measurement series \u003cspan class=\"citation\">Robert Duncan Luce et al. (\u003ca href=\"#ref-luce2007foundations\" role=\"doc-biblioref\">1990\u003c/a>)\u003c/span>, which has become the canonical text for every psychology student seeking to understand measurement in the non-physical sciences.\u003clabel for=\"tufte-sn-112\" class=\"margin-toggle sidenote-number\">112\u003c/label>\u003cinput type=\"checkbox\" id=\"tufte-sn-112\" class=\"margin-toggle\" />\u003cspan class=\"sidenote\">\u003cspan class=\"sidenote-number\">112\u003c/span> It is worth noting that 20th century physics has also seriously challenged the classical additive understanding of measurement. For example, velocities are revealed to be non-additive under general relativity, and properties of quantum particles are only measurable under a complex probabilistic framework.\u003c/span>\nOne of the key breakthroughs was to shift the burden from measuring (additive) constructs themselves to measuring (additive) \u003cem>effects\u003c/em> of constructs in conjunction with one another:\u003c/p>\n\u003cblockquote>\n\u003cp>When no natural concatenation operation exists, one should try to discover a way to measure factors and responses such that the ‘effects’ of different factors are additive. \u003cspan class=\"citation\">(\u003ca href=\"#ref-luce1964simultaneous\" role=\"doc-biblioref\">R. Duncan Luce &amp; Tukey, 1964\u003c/a>)\u003c/span>.\u003c/p>\n\u003c/blockquote>\n\u003cp>This modern viewpoint broadly informs the view we describe here.\u003c/p>\n"}},{"id":"island_4","name":"Box","props":{"title":"Reliability paradoxes!","type":"depth","content":"\n\n\u003cp>There’s a major issue with calculating reliabilities using the approaches we described here: reliability will always be relative to the variation in the sample. So if a sample has less variability, reliability will decrease!\u003c/p>\n\u003cp>Let’s think about the CDI data we were talking about earlier, which showed high test-retest reliability. Now imagine we restricted our sample to only 16 – 18 month-olds (our prior sample had 16 – 30-month-olds) with low maternal education. Within this more restricted subset, overall vocabularies would be lower and more similar to one another, and so the average amount of change \u003cem>within\u003c/em> a child would be larger relative to the differences \u003cem>between\u003c/em> children. That would make our test-retest reliability score go down, even though we would just be computing it on a subset of the same data.\u003c/p>\n\u003cp>We can construct a much more worrisome version of the same problem. Say we are very sloppy in our administration of the CDI and create lots of between-participants variability, perhaps by giving different instructions to different families. This practice will actually \u003cem>increase\u003c/em> our estimate of split-half reliability – while the within-participant variability will remain the same, the between-participant variability will go up! You could call this a “reliability paradox” – sloppier data collection can actually lead to higher reliabilities.\u003clabel for=\"tufte-sn-117\" class=\"margin-toggle sidenote-number\">117\u003c/label>\u003cinput type=\"checkbox\" id=\"tufte-sn-117\" class=\"margin-toggle\" />\u003cspan class=\"sidenote\">\u003cspan class=\"sidenote-number\">117\u003c/span> If you get interested in this topic, take a look at \u003cspan class=\"citation\">(\u003ca href=\"#ref-luck2018\" role=\"doc-biblioref\">\u003cstrong>luck2018?\u003c/strong>\u003c/a>)\u003c/span>. There’s also a fascinating article by \u003cspan class=\"citation\">Hedge et al. (\u003ca href=\"#ref-hedge2018\" role=\"doc-biblioref\">2018\u003c/a>)\u003c/span> that shows why many highly replicable cognitive tasks like the Stroop task nevertheless have low reliability: they don’t vary very much between individuals!\u003c/span>\u003c/p>\n\u003cp>More generally, we need to be sensitive to the sources of variability we’re quantifying reliability over – both the numerator and the denominator. If we’re computing split-half reliabilities, typically we’re looking at variability across test questions (from some question bank) vs. across individuals (from some population). Both of these sampling decisions affect reliability – if the population is more variable \u003cem>or\u003c/em> the questions are less variable, we’ll get higher reliability.\u003c/p>\n"}},{"id":"island_5","name":"Box","props":{"title":"Bad variable naming!","type":"accident_report","content":"\n\n\u003cp>In our methods class, students often try to reproduce the original analyses from a published study before attempting to replicate the results in a new sample of participants. When Kengthsagn Louis looked at the code for the study she was interested in, she noticed that the variables in the analysis code were unnamed (presumably because they were output this way by the survey software). For example, one piece of Stata code looked like this!\u003c/p>\n\u003cpre verbatim=\"TRUE\">\u003ccode>gen recall1=.\nreplace recall1=0 if Q21==1 \nreplace recall1=1 if Q21==3 | Q21==5 | Q21==6\nreplace recall1=2 if Q21==2 | Q21==4 | Q21==7 | Q21==8\nreplace recall1=0 if Q69==1 \nreplace recall1=1 if Q69==3 | Q69==5 | Q69==6\nreplace recall1=2 if Q69==2 | Q69==4 | Q69==7 | Q69==8\nta recall1\u003c/code>\u003c/pre>\n\u003cp>In the process of translating this code into R in order to reproduce the analyses, Kengthsagn and a course teaching assistant, Andrew Lampinen, noticed that some participant responses had been assigned to the wrong variables. Because the variable names were not human-readable, this error was almost impossible to detect. After being made aware of the problem, the article’s author – to their credit – issued an immediate correction since the problem affected some of the inferential conclusions of the article \u003cspan class=\"citation\">(\u003ca href=\"#ref-petersen2019\" role=\"doc-biblioref\">Petersen, 2019\u003c/a>)\u003c/span>.\u003c/p>\n\u003cp>The moral of the story: obscure variable names can hide existing errors and create opportunities for further error! Sometimes you can adjust these within your experimental software, avoiding the issue. If not, make sure to create a “key” and translate the names immediately, double checking after you are done.\u003c/p>\n"}},{"id":"island_6","name":"Box","props":{"title":"Really anonymous?","type":"ethical_considerations","content":"\n\n\u003cp>When we first began teaching Psych 251, our experimental methods course at Stanford, one of the biggest contributions of the course was simply showing students how to do experiments online. Amazon’s Mechanical Turk crowdsourcing service was relatively new, and our IRB did not have a good sense of what this service really was. We proposed that we would share data from the class and received approval for this practice. Our datasets were downloaded directly from Mechanical Turk and included participants’ MTurk IDs (long alphanumeric strings that seemed completely anonymous). Several experiences caused us to reconsider this practice!\u003c/p>\n\u003cp>First, we discovered that MTurk IDs were in some cases linked to study participants’ public Amazon “wish lists,” which could both inadvertently provide information about the participant and also even potentially provide a basis for reidentification (in rare cases). This discovery led us to consult with our IRB and provide more explicit consent language in our class experiments, linking to instructions for making Amazon profiles private.\u003c/p>\n\u003cp>Then, a little later we received an irate email from an MTurk participant who had discovered their data on github via a search for their MTurk ID. Although they were not identified in this dataset, it convinced us that at least some participants would not like this ID shared. After another consultation with the IRB, we apologized to this individual and removed their and others’ IDs from our github commit histories across that and other repositories. We now take care to anonymize IDs by creating a secret mapping between the IDs we post and the actual MTurk IDs prior to posting data.\u003c/p>\n"}},{"id":"island_7","name":"Box","props":{"title":"!undefined","type":"discussion_questions","content":"\n\u003col style=\"list-style-type: decimal;\">\n\u003cli>\u003cp>Find an Open Science Framework repository that corresponds to a published paper. What is their strategy for documenting what is shared? How easy is it to figure out where everything is and if the data and materials sharing is complete?\u003c/p>\u003c/li>\n\u003cli>\u003cp>Open up the US Department of Health and Human Services “safe harbor” standards \u003ca href=\"https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html\">here\u003c/a> and navigate to the section called “The De-identification Standard.” Go through the list of identifiers that must be removed. Are there any on this list that you would need to conduct your own research? Can you think of any others that do not fall on this list?\u003c/p>\u003c/li>\n\u003c/ol>\n"}},{"id":"island_8","name":"Box","props":{"title":"!undefined","type":"readings","content":"\n\u003cul>\n\u003cli>A more in-depth tutorial on various aspects of scientific openness: Klein, O., Hardwicke, T. E., Aust, F., Breuer, J., Danielsson, H., Hofelich Mohr, A., Ijzerman, H., Nilsonne, G., Vanpaemel, W., &amp; Frank, M. C. (2018). A practical guide for transparency in psychological science. \u003cem>Collabra: Psychology\u003c/em>, \u003cem>4\u003c/em>, 20. \u003ca href>https://doi.org/10.1525/collabra.158\u003c/a>\u003c/li>\n\u003c/ul>\n"}}]}}</script></body>
</html>
